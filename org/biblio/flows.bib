@article{papamakarios19_normal_flows_probab_model_infer,
  author =       {Papamakarios, George and Nalisnick, Eric and
                  Rezende, Danilo Jimenez and Mohamed, Shakir and
                  Lakshminarayanan, Balaji},
  title =        {Normalizing Flows for Probabilistic Modeling and
                  Inference},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1912.02762v1},
  abstract =     {Normalizing flows provide a general mechanism for
                  defining expressive probability distributions, only
                  requiring the specification of a (usually simple)
                  base distribution and a series of bijective
                  transformations. There has been much recent work on
                  normalizing flows, ranging from improving their
                  expressive power to expanding their application. We
                  believe the field has now matured and is in need of
                  a unified perspective. In this review, we attempt to
                  provide such a perspective by describing flows
                  through the lens of probabilistic modeling and
                  inference. We place special emphasis on the
                  fundamental principles of flow design, and discuss
                  foundational topics such as expressive power and
                  computational trade-offs. We also broaden the
                  conceptual framing of flows by relating them to more
                  general probability transformations. Lastly, we
                  summarize the use of flows for tasks such as
                  generative modeling, approximate inference, and
                  supervised learning.},
  archivePrefix ={arXiv},
  eprint =       {1912.02762},
  primaryClass = {stat.ML},
}
