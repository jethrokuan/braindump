#+SETUPFILE:./hugo_setup.org
#+HUGO_SLUG: multimodal_representation
#+TITLE: Multi-modal Representation

- tags :: [[file:multimodal_machine_learning.org][Multi-modal Machine Learning]]

A multi-modal representation is a representation of data using
information from multiple modalities. The ability to represent data in
a meaningful way is crucial to multimodal problems.

* Key Challenges
- how to combine data from heterogeneous sources
- how to deal with differing levels of noise, and noise otpology
- how to deal with missing data

* Categories of multi-modal representations
cite:baltrusaitis17:_multim_machin_learn proposes two categories of
multimodal representation:

- joint :: combining unimodal signals into the same representation space.
- coordinated :: process unimodal signals separately, but enforcing
  certain similarity constraints on them to bring them into a
  coordinated space.

* Joint Representations
Joint representations are mostly used in tasks where multimodal data
is present during both training and inference.

** Neural Networks

A common approach is to use deep neural networks to create abstract
representations of unimodal data, beore combining them in the last
layer (for example, via concatenation). The hidden layers project the
modalities into a joint space. There is a close relationship between
multimodal representation learning and [[file:multi_modal_fusion.org][Multi-modal Fusion]] when using
neural networks. 

It is common to pretrain these representations using
an [[file:autoencoder.org][Autoencoder]] on unsupervised data, because neural networks require a
lot of labelled data. The neural network approach often yields
superior performance, but is /unable to handle missing data naturally/.

** Probabilistic Graphical Models

[[file:pgm.org][Probabilistic Graph Models]] construct representations through the use
of latent random variables.

A popular approach is [[file:deep_boltzmann_machines.org][Deep Boltzmann Machines]], which uses [[file:restricted_boltzmann_machines.org][Restricted
Boltzmann machines]] as building blocks. Each successive layer of a DBM
is expected to represent the data at a higher level of abstraction.
DBMs do not need supervised data for training. It is also possible to
convert them into a deterministic neural network, but this loses the
generative aspect of the model.

One advantage of using multimodal DBMs is that they are [[file:generative_models.org][generative
models]], allowing them to:
 
1. Naturally deal with missing data.
2. Generate samples of one modality in the presence of at least 1
   modality

* Coordinated Representations

Separate representations are learnt from each modality, but later
coordinated through a constraint.

*Similarity models* minimize the distance between modalities in the
coordinated space. Neural network approaches such as DeViSE use inner
product and a ranking loss function.

Some models use a *structured coordinated space*, enforcing additional
constraints between the modality representations. These structural
constraints differ across applications, which include [[file:crossmodal_hashing.org][Cross-modal Hashing]],
cross-modal retrieval, and image captioning.

Other examples of structured coordinated spaces use enforce a partial
order in the multimodal space. Another special case of this is
[[file:canonical_correlation_analysis.org][Canonical Correlation Analysis]]. 

bibliography:biblio.bib
