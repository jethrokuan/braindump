@article{pfeiffer18_deep_learn_with_spikin_neuron,
  author =       {Michael Pfeiffer and Thomas Pfeil},
  title =        {Deep Learning With Spiking Neurons: Opportunities
                  and Challenges},
  journal =      {Frontiers in Neuroscience},
  volume =       12,
  number =       {nil},
  pages =        {nil},
  year =         2018,
  doi =          {10.3389/fnins.2018.00774},
  url =          {https://doi.org/10.3389/fnins.2018.00774},
  DATE_ADDED =   {Mon Aug 12 20:01:23 2019},
}


@article{jang18_introd_to_spikin_neural_networ,
  author =       {Jang, Hyeryung and Simeone, Osvaldo and Gardner,
                  Brian and Gr{\"u}ning, Andr{\'e}},
  title =        {An Introduction To Spiking Neural Networks:
                  Probabilistic Models, Learning Rules, and
                  Applications},
  journal =      {CoRR},
  year =         2018,
  url =          {http://arxiv.org/abs/1812.03929v4},
  abstract =     {Spiking Neural Networks (SNNs) are distributed
                  trainable systems whose computing elements, or
                  neurons, are characterized by internal analog
                  dynamics and by digital and sparse synaptic
                  communications. The sparsity of the synaptic spiking
                  inputs and the corresponding event-driven nature of
                  neural processing can be leveraged by hardware
                  implementations that have demonstrated significant
                  energy reductions as compared to conventional
                  Artificial Neural Networks (ANNs). Most existing
                  training algorithms for SNNs have been designed
                  either for biological plausibility or through
                  conversion from pre-trained ANNs via rate encoding.
                  This paper aims at providing an introduction to SNNs
                  by focusing on a probabilistic signal processing
                  methodology that enables the direct derivation of
                  learning rules leveraging the unique time encoding
                  capabilities of SNNs. To this end, the paper adopts
                  discrete-time probabilistic models for networked
                  spiking neurons, and it derives supervised and
                  unsupervised learning rules from first principles by
                  using variational inference. Examples and open
                  research problems are also provided.},
  archivePrefix ={arXiv},
  eprint =       {1812.03929v4},
  primaryClass = {eess.SP},
}
@article{bellec18_long_short_term_memor_learn,
  author =       {Bellec, Guillaume and Salaj, Darjan and Subramoney,
                  Anand and Legenstein, Robert and Maass, Wolfgang},
  title =        {Long Short-Term Memory and Learning-To-Learn in
                  Networks of Spiking Neurons},
  journal =      {CoRR},
  year =         2018,
  url =          {http://arxiv.org/abs/1803.09574v4},
  abstract =     {Recurrent networks of spiking neurons (RSNNs)
                  underlie the astounding computing and learning
                  capabilities of the brain. But computing and
                  learning capabilities of RSNN models have remained
                  poor, at least in comparison with artificial neural
                  networks (ANNs). We address two possible reasons for
                  that. One is that RSNNs in the brain are not
                  randomly connected or designed according to simple
                  rules, and they do not start learning as a tabula
                  rasa network. Rather, RSNNs in the brain were
                  optimized for their tasks through evolution,
                  development, and prior experience. Details of these
                  optimization processes are largely unknown. But
                  their functional contribution can be approximated
                  through powerful optimization methods, such as
                  backpropagation through time (BPTT). A second major
                  mismatch between RSNNs in the brain and models is
                  that the latter only show a small fraction of the
                  dynamics of neurons and synapses in the brain. We
                  include neurons in our RSNN model that reproduce one
                  prominent dynamical process of biological neurons
                  that takes place at the behaviourally relevant time
                  scale of seconds: neuronal adaptation. We denote
                  these networks as LSNNs because of their Long
                  short-term memory. The inclusion of adapting neurons
                  drastically increases the computing and learning
                  capability of RSNNs if they are trained and
                  configured by deep learning (BPTT combined with a
                  rewiring algorithm that optimizes the network
                  architecture). In fact, the computational
                  performance of these RSNNs approaches for the first
                  time that of LSTM networks. In addition RSNNs with
                  adapting neurons can acquire abstract knowledge from
                  prior learning in a Learning-to-Learn (L2L) scheme,
                  and transfer that knowledge in order to learn new
                  but related tasks from very few examples. We
                  demonstrate this for supervised learning and
                  reinforcement learning.},
  archivePrefix ={arXiv},
  eprint =       {1803.09574},
  primaryClass = {cs.NE},
}
@article{huh17_gradien_descen_spikin_neural_networ,
  author =       {Huh, Dongsung and Sejnowski, Terrence J.},
  title =        {Gradient Descent for Spiking Neural Networks},
  journal =      {CoRR},
  year =         2017,
  url =          {http://arxiv.org/abs/1706.04698v2},
  abstract =     {Much of studies on neural computation are based on
                  network models of static neurons that produce analog
                  output, despite the fact that information processing
                  in the brain is predominantly carried out by dynamic
                  neurons that produce discrete pulses called spikes.
                  Research in spike-based computation has been impeded
                  by the lack of efficient supervised learning
                  algorithm for spiking networks. Here, we present a
                  gradient descent method for optimizing spiking
                  network models by introducing a differentiable
                  formulation of spiking networks and deriving the
                  exact gradient calculation. For demonstration, we
                  trained recurrent spiking networks on two dynamic
                  tasks: one that requires optimizing fast
                  (~millisecond) spike-based interactions for
                  efficient encoding of information, and a delayed
                  memory XOR task over extended duration (~second).
                  The results show that our method indeed optimizes
                  the spiking network dynamics on the time scale of
                  individual spikes as well as behavioral time scales.
                  In conclusion, our result offers a general purpose
                  supervised learning algorithm for spiking neural
                  networks, thus advancing further investigations on
                  spike-based computation.},
  archivePrefix ={arXiv},
  eprint =       {1706.04698},
  primaryClass = {q-bio.NC},
}
@article{neftci19_surrog_gradien_learn_spikin_neural_networ,
  author =       {Neftci, Emre O. and Mostafa, Hesham and Zenke,
                  Friedemann},
  title =        {Surrogate Gradient Learning in Spiking Neural
                  Networks},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1901.09948v2},
  abstract =     {Spiking neural networks are nature's versatile
                  solution to fault-tolerant and energy efficient
                  signal processing. To translate these benefits into
                  hardware, a growing number of neuromorphic spiking
                  neural network processors attempt to emulate
                  biological neural networks. These developments have
                  created an imminent need for methods and tools to
                  enable such systems to solve real-world signal
                  processing problems. Like conventional neural
                  networks, spiking neural networks can be trained on
                  real, domain specific data. However, their training
                  requires overcoming a number of challenges linked to
                  their binary and dynamical nature. This article
                  elucidates step-by-step the problems typically
                  encountered when training spiking neural networks,
                  and guides the reader through the key concepts of
                  synaptic plasticity and data-driven learning in the
                  spiking setting. To that end, it gives an overview
                  of existing approaches and provides an introduction
                  to surrogate gradient methods, specifically, as a
                  particularly flexible and efficient method to
                  overcome the aforementioned challenges.},
  archivePrefix ={arXiv},
  eprint =       {1901.09948},
  primaryClass = {cs.NE},
}


@article{whittington19_theor_error_back_propag_brain,
  author =       {James C.R. Whittington and Rafal Bogacz},
  title =        {Theories of Error Back-Propagation in the Brain},
  journal =      {Trends in Cognitive Sciences},
  volume =       23,
  number =       3,
  pages =        {235-250},
  year =         2019,
  doi =          {10.1016/j.tics.2018.12.005},
  url =          {https://doi.org/10.1016/j.tics.2018.12.005},
  DATE_ADDED =   {Tue Aug 20 10:09:27 2019},
}

@article{comsa19_tempor_codin_spikin_neural_networ,
  author =       {Comsa, Iulia M. and Potempa, Krzysztof and Versari,
                  Luca and Fischbacher, Thomas and Gesmundo, Andrea
                  and Alakuijala, Jyrki},
  title =        {Temporal Coding in Spiking Neural Networks With
                  Alpha Synaptic Function},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1907.13223v1},
  abstract =     {The timing of individual neuronal spikes is
                  essential for biological brains to make fast
                  responses to sensory stimuli. However, conventional
                  artificial neural networks lack the intrinsic
                  temporal coding ability present in biological
                  networks. We propose a spiking neural network model
                  that encodes information in the relative timing of
                  individual neuron spikes. In classification tasks,
                  the output of the network is indicated by the first
                  neuron to spike in the output layer. This temporal
                  coding scheme allows the supervised training of the
                  network with backpropagation, using locally exact
                  derivatives of the postsynaptic spike times with
                  respect to presynaptic spike times. The network
                  operates using a biologically-plausible alpha
                  synaptic transfer function. Additionally, we use
                  trainable synchronisation pulses that provide bias,
                  add flexibility during training and exploit the
                  decay part of the alpha function. We show that such
                  networks can be trained successfully on noisy
                  Boolean logic tasks and on the MNIST dataset encoded
                  in time. The results show that the spiking neural
                  network outperforms comparable spiking models on
                  MNIST and achieves similar quality to fully
                  connected conventional networks with the same
                  architecture. We also find that the spiking network
                  spontaneously discovers two operating regimes,
                  mirroring the accuracy-speed trade-off observed in
                  human decision-making: a slow regime, where a
                  decision is taken after all hidden neurons have
                  spiked and the accuracy is very high, and a fast
                  regime, where a decision is taken very fast but the
                  accuracy is lower. These results demonstrate the
                  computational power of spiking networks with
                  biological characteristics that encode information
                  in the timing of individual neurons. By studying
                  temporal coding in spiking networks, we aim to
                  create building blocks towards energy-efficient and
                  more complex biologically-inspired neural
                  architectures.},
  archivePrefix ={arXiv},
  eprint =       {1907.13223},
  primaryClass = {cs.NE},
}

@book{gerstner2002spiking,
  title={Spiking neuron models: Single neurons, populations, plasticity},
  author={Gerstner, Wulfram and Kistler, Werner M},
  year={2002},
  publisher={Cambridge university press}
}

@article{queiroz06_reinf_learn_simpl_contr_task,
  author =       {Murilo Saraiva de Queiroz and Roberto Coelho de
                  Berrêdo and Antônio de P{\'a}dua Braga},
  title =        {Reinforcement Learning of a Simple Control Task
                  Using the Spike Response Model},
  journal =      {Neurocomputing},
  volume =       70,
  number =       {1-3},
  pages =        {14-20},
  year =         2006,
  doi =          {10.1016/j.neucom.2006.07.002},
  url =          {https://doi.org/10.1016/j.neucom.2006.07.002},
  DATE_ADDED =   {Thu Aug 29 12:45:08 2019},
}

@article{sboev18_spikin_neural_networ_reinf_learn,
  author =       {Alexander Sboev and Danila Vlasov and Roman Rybka
                  and Alexey Serenko},
  title =        {Spiking Neural Network Reinforcement Learning Method
                  Based on Temporal Coding and Stdp},
  journal =      {Procedia Computer Science},
  volume =       145,
  number =       {nil},
  pages =        {458-463},
  year =         2018,
  doi =          {10.1016/j.procs.2018.11.107},
  url =          {https://doi.org/10.1016/j.procs.2018.11.107},
  DATE_ADDED =   {Mon Sep 30 11:08:34 2019},
}

@article{heeger2000poisson,
  title={Poisson model of spike generation},
  author={Heeger, David},
  journal={Handout, University of Standford},
  volume={5},
  pages={1--13},
  year={2000}
}

@article{MAASS19971659,
  title =        "Networks of spiking neurons: The third generation of
                  neural network models",
  journal =      "Neural Networks",
  volume =       10,
  number =       9,
  pages =        "1659 - 1671",
  year =         1997,
  issn =         "0893-6080",
  doi =          "https://doi.org/10.1016/S0893-6080(97)00011-7",
  url =
                  "http://www.sciencedirect.com/science/article/pii/S0893608097000117",
  author =       "Wolfgang Maass",
  keywords =     "Spiking neuron, Integrate-and-fire neutron,
                  Computational complexity, Sigmoidal neural nets,
                  Lower bounds",
  abstract =     "The computational power of formal models for
                  networks of spiking neurons is compared with that of
                  other neural network models based on McCulloch Pitts
                  neurons (i.e., threshold gates), respectively,
                  sigmoidal gates. In particular it is shown that
                  networks of spiking neurons are, with regard to the
                  number of neurons that are needed, computationally
                  more powerful than these other neural network
                  models. A concrete biologically relevant function is
                  exhibited which can be computed by a single spiking
                  neuron (for biologically reasonable values of its
                  parameters), but which requires hundreds of hidden
                  units on a sigmoidal neural net. On the other hand,
                  it is known that any function that can be computed
                  by a small sigmoidal neural net can also be computed
                  by a small network of spiking neurons. This article
                  does not assume prior knowledge about spiking
                  neurons, and it contains an extensive list of
                  references to the currently available literature on
                  computations in networks of spiking neurons and
                  relevant results from neurobiology."
}

@Article{Cybenko1989,
  author =       "Cybenko, G.",
  title =        "Approximation by superpositions of a sigmoidal
                  function",
  journal =      "Mathematics of Control, Signals and Systems",
  year =         1989,
  month =        "Dec",
  day =          01,
  volume =       2,
  number =       4,
  pages =        "303--314",
  abstract =     "In this paper we demonstrate that finite linear
                  combinations of compositions of a fixed, univariate
                  function and a set of affine functionals can
                  uniformly approximate any continuous function ofn
                  real variables with support in the unit hypercube;
                  only mild conditions are imposed on the univariate
                  function. Our results settle an open question about
                  representability in the class of single hidden layer
                  neural networks. In particular, we show that
                  arbitrary decision regions can be arbitrarily well
                  approximated by continuous feedforward neural
                  networks with only a single internal, hidden layer
                  and any continuous sigmoidal nonlinearity. The paper
                  discusses approximation properties of other possible
                  types of nonlinearities that might be implemented by
                  artificial neural networks.",
  issn =         "1435-568X",
  doi =          "10.1007/BF02551274",
  url =          "https://doi.org/10.1007/BF02551274"
}

@article{TAVANAEI201947,
  title =        "Deep learning in spiking neural networks",
  journal =      "Neural Networks",
  volume =       111,
  pages =        "47 - 63",
  year =         2019,
  issn =         "0893-6080",
  doi =          "https://doi.org/10.1016/j.neunet.2018.12.002",
  url =
                  "http://www.sciencedirect.com/science/article/pii/S0893608018303332",
  author =       "Amirhossein Tavanaei and Masoud Ghodrati and Saeed
                  Reza Kheradpisheh and Timothée Masquelier and
                  Anthony Maida",
  keywords =     "Deep learning, Spiking neural network, Biological
                  plausibility, Machine learning, Power-efficient
                  architecture",
  abstract =     "In recent years, deep learning has revolutionized
                  the field of machine learning, for computer vision
                  in particular. In this approach, a deep (multilayer)
                  artificial neural network (ANN) is trained, most
                  often in a supervised manner using backpropagation.
                  Vast amounts of labeled training examples are
                  required, but the resulting classification accuracy
                  is truly impressive, sometimes outperforming humans.
                  Neurons in an ANN are characterized by a single,
                  static, continuous-valued activation. Yet biological
                  neurons use discrete spikes to compute and transmit
                  information, and the spike times, in addition to the
                  spike rates, matter. Spiking neural networks (SNNs)
                  are thus more biologically realistic than ANNs, and
                  are arguably the only viable option if one wants to
                  understand how the brain computes at the neuronal
                  description level. The spikes of biological neurons
                  are sparse in time and space, and event-driven.
                  Combined with bio-plausible local learning rules,
                  this makes it easier to build low-power,
                  neuromorphic hardware for SNNs. However, training
                  deep SNNs remains a challenge. Spiking neurons’
                  transfer function is usually non-differentiable,
                  which prevents using backpropagation. Here we review
                  recent supervised and unsupervised methods to train
                  deep SNNs, and compare them in terms of accuracy and
                  computational cost. The emerging picture is that
                  SNNs still lag behind ANNs in terms of accuracy, but
                  the gap is decreasing, and can even vanish on some
                  tasks, while SNNs typically require many fewer
                  operations and are the better candidates to process
                  spatio-temporal data."
}

@article {Merolla668,
  author =       {Merolla, Paul A. and Arthur, John V. and
                  Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and
                  Sawada, Jun and Akopyan, Filipp and Jackson, Bryan
                  L. and Imam, Nabil and Guo, Chen and Nakamura,
                  Yutaka and Brezzo, Bernard and Vo, Ivan and Esser,
                  Steven K. and Appuswamy, Rathinakumar and Taba,
                  Brian and Amir, Arnon and Flickner, Myron D. and
                  Risk, William P. and Manohar, Rajit and Modha,
                  Dharmendra S.},
  title =        {A million spiking-neuron integrated circuit with a
                  scalable communication network and interface},
  volume =       345,
  number =       6197,
  pages =        {668--673},
  year =         2014,
  doi =          {10.1126/science.1254642},
  publisher =    {American Association for the Advancement of Science},
  abstract =     {Computers are nowhere near as versatile as our own
                  brains. Merolla et al. applied our present knowledge
                  of the structure and function of the brain to design
                  a new computer chip that uses the same wiring rules
                  and architecture. The flexible, scalable chip
                  operated efficiently in real time, while using very
                  little power.Science, this issue p. 668 Inspired by
                  the brain{\textquoteright}s structure, we have
                  developed an efficient, scalable, and flexible
                  non{\textendash}von Neumann architecture that
                  leverages contemporary silicon technology. To
                  demonstrate, we built a 5.4-billion-transistor chip
                  with 4096 neurosynaptic cores interconnected via an
                  intrachip network that integrates 1 million
                  programmable spiking neurons and 256 million
                  configurable synapses. Chips can be tiled in two
                  dimensions via an interchip communication interface,
                  seamlessly scaling the architecture to a cortexlike
                  sheet of arbitrary size. The architecture is well
                  suited to many applications that use complex neural
                  networks in real time, for example, multiobject
                  detection and classification. With
                  400-pixel-by-240-pixel video input at 30 frames per
                  second, the chip consumes 63 milliwatts.},
  issn =         {0036-8075},
  URL =          {https://science.sciencemag.org/content/345/6197/668},
  eprint =
                  {https://science.sciencemag.org/content/345/6197/668.full.pdf},
  journal =      {Science}
}

@article{davies2018loihi,
  title =        {Loihi: A neuromorphic manycore processor with
                  on-chip learning},
  author =       {Davies, Mike and Srinivasa, Narayan and Lin,
                  Tsung-Han and Chinya, Gautham and Cao, Yongqiang and
                  Choday, Sri Harsha and Dimou, Georgios and Joshi,
                  Prasad and Imam, Nabil and Jain, Shweta and others},
  journal =      {IEEE Micro},
  volume =       38,
  number =       1,
  pages =        {82--99},
  year =         2018,
  publisher =    {IEEE}
}

@article{SnnSlam,
  author =       "Tang, Guangzhi and Shah, Arpit and Michmizos,
                  Konstantinos P.",
  title =        "Spiking Neural Network on Neuromorphic Hardware for
                  Energy-Efficient Unidimensional Slam",
  journal =      "CoRR",
  year =         2019,
  url =          "http://arxiv.org/abs/1903.02504v2",
  abstract =     "Energy-efficient simultaneous localization and
                  mapping (SLAM) is crucial for mobile robots
                  exploring unknown environments. The mammalian brain
                  solves SLAM via a network of specialized neurons,
                  exhibiting asynchronous computations and event-based
                  communications, with very low energy consumption. We
                  propose a brain-inspired spiking neural network
                  (SNN) architecture that solves the unidimensional
                  SLAM by introducing spike-based reference frame
                  transformation, visual likelihood computation, and
                  Bayesian inference. We integrated our neuromorphic
                  algorithm to Intel's Loihi neuromorphic processor, a
                  non-Von Neumann hardware that mimics the brain's
                  computing paradigms. We performed comparative
                  analyses for accuracy and energy-efficiency between
                  our neuromorphic approach and the GMapping
                  algorithm, which is widely used in small
                  environments. Our Loihi-based SNN architecture
                  consumes 100 times less energy than GMapping run on
                  a CPU while having comparable accuracy in head
                  direction localization and map-generation. These
                  results pave the way for scaling our approach
                  towards active-SLAM alternative solutions for
                  Loihi-controlled autonomous robots.",
  archivePrefix ="arXiv",
  eprint =       "1903.02504",
  primaryClass = "cs.RO",
}

@article{Severa2016SpikingNA,
  title =        "Spiking network algorithms for scientific computing",
  author =       "William Severa and Ojas Parekh and Kristofor D.
                  Carlson and Conrad D. James and James B. Aimone",
  journal =      "2016 IEEE International Conference on Rebooting
                  Computing (ICRC)",
  year =         2016,
  pages =        "1-8"
}

@article{aiskinLee,
  author =       {Lee, Wang Wei and Tan, Yu Jun and Yao, Haicheng and
                  Li, Si and See, Hian Hian and Hon, Matthew and Ng,
                  Kian Ann and Xiong, Betty and Ho, John S. and Tee,
                  Benjamin C. K.},
  title =        {A neuro-inspired artificial peripheral nervous
                  system for scalable electronic skins},
  volume =       4,
  number =       32,
  elocation-id = {eaax2198},
  year =         2019,
  doi =          {10.1126/scirobotics.aax2198},
  publisher =    {Science Robotics},
  abstract =     {The human sense of touch is essential for dexterous
                  tool usage, spatial awareness, and social
                  communication. Equipping intelligent human-like
                  androids and prosthetics with electronic
                  skins{\textemdash}a large array of sensors spatially
                  distributed and capable of rapid somatosensory
                  perception{\textemdash}will enable them to work
                  collaboratively and naturally with humans to
                  manipulate objects in unstructured living
                  environments. Previously reported tactile-sensitive
                  electronic skins largely transmit the tactile
                  information from sensors serially, resulting in
                  readout latency bottlenecks and complex wiring as
                  the number of sensors increases. Here, we introduce
                  the Asynchronously Coded Electronic Skin
                  (ACES){\textemdash}a neuromimetic architecture that
                  enables simultaneous transmission of thermotactile
                  information while maintaining exceptionally low
                  readout latencies, even with array sizes beyond
                  10,000 sensors. We demonstrate prototype arrays of
                  up to 240 artificial mechanoreceptors that
                  transmitted events asynchronously at a constant
                  latency of 1 ms while maintaining an ultra-high
                  temporal precision of \&lt;60 ns, thus resolving
                  fine spatiotemporal features necessary for rapid
                  tactile perception. Our platform requires only a
                  single electrical conductor for signal propagation,
                  realizing sensor arrays that are dynamically
                  reconfigurable and robust to damage. We anticipate
                  that the ACES platform can be integrated with a wide
                  range of skin-like sensors for artificial
                  intelligence (AI){\textendash}enhanced autonomous
                  robots, neuroprosthetics, and neuromorphic computing
                  hardware for dexterous object manipulation and
                  somatosensory perception.},
  URL =
                  {https://robotics.sciencemag.org/content/4/32/eaax2198},
  eprint =
                  {https://robotics.sciencemag.org/content/4/32/eaax2198.full.pdf},
  journal =      {Science Robotics}
}

@incollection{NIPS2018_7415,
  title =        {SLAYER: Spike Layer Error Reassignment in Time},
  author =       {Shrestha, Sumit Bam and Orchard, Garrick},
  booktitle =    {Advances in Neural Information Processing Systems
                  31},
  editor =       {S. Bengio and H. Wallach and H. Larochelle and K.
                  Grauman and N. Cesa-Bianchi and R. Garnett},
  pages =        {1412--1421},
  year =         2018,
  publisher =    {Curran Associates, Inc.},
  url =
                  {http://papers.nips.cc/paper/7415-slayer-spike-layer-error-reassignment-in-time.pdf}
}

@article{SHRESTHA201733,
  title =        "Robust spike-train learning in spike-event based
                  weight update",
  journal =      "Neural Networks",
  volume =       96,
  pages =        "33 - 46",
  year =         2017,
  issn =         "0893-6080",
  doi =          "https://doi.org/10.1016/j.neunet.2017.08.010",
  url =
                  "http://www.sciencedirect.com/science/article/pii/S0893608017302009",
  author =       "Sumit Bam Shrestha and Qing Song",
  keywords =     "Spiking neural network, Multilayer spike-train
                  learning, Supervised learning, Weight convergence,
                  Robust stability, Adaptive learning rate",
  abstract =     "Supervised learning algorithms in a spiking neural
                  network either learn a spike-train pattern for a
                  single neuron receiving input spike-train from
                  multiple input synapses or learn to output the first
                  spike time in a feedforward network setting. In this
                  paper, we build upon spike-event based weight update
                  strategy to learn continuous spike-train in a
                  spiking neural network with a hidden layer using a
                  dead zone on–off based adaptive learning rate rule
                  which ensures convergence of the learning process in
                  the sense of weight convergence and robustness of
                  the learning process to external disturbances. Based
                  on different benchmark problems, we compare this new
                  method with other relevant spike-train learning
                  algorithms. The results show that the speed of
                  learning is much improved and the rate of successful
                  learning is also greatly improved."
}

@ARTICLE{10.3389/fnins.2016.00508,
  AUTHOR =       {Lee, Jun Haeng and Delbruck, Tobi and Pfeiffer,
                  Michael},
  TITLE =        {Training Deep Spiking Neural Networks Using
                  Backpropagation},
  JOURNAL =      {Frontiers in Neuroscience},
  VOLUME =       10,
  PAGES =        508,
  YEAR =         2016,
  URL =
                  {https://www.frontiersin.org/article/10.3389/fnins.2016.00508},
  DOI =          {10.3389/fnins.2016.00508},
  ISSN =         {1662-453X},
  ABSTRACT =     {Deep spiking neural networks (SNNs) hold the
                  potential for improving the latency and energy
                  efficiency of deep neural networks through
                  data-driven event-based computation. However,
                  training such networks is difficult due to the
                  non-differentiable nature of spike events. In this
                  paper, we introduce a novel technique, which treats
                  the membrane potentials of spiking neurons as
                  differentiable signals, where discontinuities at
                  spike times are considered as noise. This enables an
                  error backpropagation mechanism for deep SNNs that
                  follows the same principles as in conventional deep
                  networks, but works directly on spike signals and
                  membrane potentials. Compared with previous methods
                  relying on indirect training and conversion, our
                  technique has the potential to capture the
                  statistics of spikes more precisely. We evaluate the
                  proposed framework on artificially generated events
                  from the original MNIST handwritten digit benchmark,
                  and also on the N-MNIST benchmark recorded with an
                  event-based dynamic vision sensor, in which the
                  proposed method reduces the error rate by a factor
                  of more than three compared to the best previous
                  SNN, and also achieves a higher accuracy than a
                  conventional convolutional neural network (CNN)
                  trained and tested on the same data. We demonstrate
                  in the context of the MNIST task that thanks to
                  their event-driven operation, deep SNNs (both fully
                  connected and convolutional) trained with our method
                  achieve accuracy equivalent with conventional neural
                  networks. In the N-MNIST example, equivalent
                  accuracy is achieved with about five times fewer
                  computational operations.}
}

@article{ivanov19_moder_deep_reinf_learn_algor,
  author =       {Ivanov, Sergey and D'yakonov, Alexander},
  title =        {Modern Deep Reinforcement Learning Algorithms},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1906.10025v2},
  abstract =     {Recent advances in Reinforcement Learning, grounded
                  on combining classical theoretical results with Deep
                  Learning paradigm, led to breakthroughs in many
                  artificial intelligence tasks and gave birth to Deep
                  Reinforcement Learning (DRL) as a field of research.
                  In this work latest DRL algorithms are reviewed with
                  a focus on their theoretical justification,
                  practical limitations and observed empirical
                  properties.},
  archivePrefix ={arXiv},
  eprint =       {1906.10025},
  primaryClass = {cs.LG},
}

@article{li18_deep_reinf_learn,
  author =       {Li, Yuxi},
  title =        {Deep Reinforcement Learning},
  journal =      {CoRR},
  year =         2018,
  url =          {http://arxiv.org/abs/1810.06339v1},
  abstract =     {We discuss deep reinforcement learning in an
                  overview style. We draw a big picture, filled with
                  details. We discuss six core elements, six important
                  mechanisms, and twelve applications, focusing on
                  contemporary work, and in historical contexts. We
                  start with background of artificial intelligence,
                  machine learning, deep learning, and reinforcement
                  learning (RL), with resources. Next we discuss RL
                  core elements, including value function, policy,
                  reward, model, exploration vs. exploitation, and
                  representation. Then we discuss important mechanisms
                  for RL, including attention and memory, unsupervised
                  learning, hierarchical RL, multi-agent RL,
                  relational RL, and learning to learn. After that, we
                  discuss RL applications, including games, robotics,
                  natural language processing (NLP), computer vision,
                  finance, business management, healthcare, education,
                  energy, transportation, computer systems, and,
                  science, engineering, and art. Finally we summarize
                  briefly, discuss challenges and opportunities, and
                  close with an epilogue.},
  archivePrefix ={arXiv},
  eprint =       {1810.06339},
  primaryClass = {cs.LG},
}

@article{sboev18_spikin_neural_networ_reinf_learn,
  author =       {Alexander Sboev and Danila Vlasov and Roman Rybka
                  and Alexey Serenko},
  title =        {Spiking Neural Network Reinforcement Learning Method
                  Based on Temporal Coding and Stdp},
  journal =      {Procedia Computer Science},
  volume =       145,
  number =       {nil},
  pages =        {458-463},
  year =         2018,
  doi =          {10.1016/j.procs.2018.11.107},
  url =          {https://doi.org/10.1016/j.procs.2018.11.107},
  DATE_ADDED =   {Mon Sep 30 11:08:34 2019},
}

@ARTICLE{10.3389/fninf.2018.00089,
  AUTHOR =       {Hazan, Hananel and Saunders, Daniel J. and Khan,
                  Hassaan and Patel, Devdhar and Sanghavi, Darpan T.
                  and Siegelmann, Hava T. and Kozma, Robert},
  TITLE =        {BindsNET: A Machine Learning-Oriented Spiking Neural
                  Networks Library in Python},
  JOURNAL =      {Frontiers in Neuroinformatics},
  VOLUME =       12,
  PAGES =        89,
  YEAR =         2018,
  URL =
                  {https://www.frontiersin.org/article/10.3389/fninf.2018.00089},
  DOI =          {10.3389/fninf.2018.00089},
  ISSN =         {1662-5196},
}

@ARTICLE{10.3389/fnbot.2019.00018,
  AUTHOR =       {Bing, Zhenshan and Baumann, Ivan and Jiang, Zhuangyi
                  and Huang, Kai and Cai, Caixia and Knoll, Alois},
  TITLE =        {Supervised Learning in SNN via Reward-Modulated
                  Spike-Timing-Dependent Plasticity for a Target
                  Reaching Vehicle},
  JOURNAL =      {Frontiers in Neurorobotics},
  VOLUME =       13,
  PAGES =        18,
  YEAR =         2019,
  URL =
                  {https://www.frontiersin.org/article/10.3389/fnbot.2019.00018},
  DOI =          {10.3389/fnbot.2019.00018},
  ISSN =         {1662-5218},
  ABSTRACT =     {Spiking neural networks (SNNs) offer many advantages
                  over traditional artificial neural networks (ANNs)
                  such as biological plausibility, fast information
                  processing, and energy efficiency. Although SNNs
                  have been used to solve a variety of control tasks
                  using the Spike-Timing-Dependent Plasticity (STDP)
                  learning rule, existing solutions usually involve
                  hard-coded network architectures solving specific
                  tasks rather than solving different kinds of tasks
                  generally. This results in neglecting one of the
                  biggest advantages of ANNs, i.e., being
                  general-purpose and easy-to-use due to their simple
                  network architecture, which usually consists of an
                  input layer, one or multiple hidden layers and an
                  output layer. This paper addresses the problem by
                  introducing an end-to-end learning approach of
                  spiking neural networks constructed with one hidden
                  layer and reward-modulated Spike-Timing-Dependent
                  Plasticity (R-STDP) synapses in an all-to-all
                  fashion. We use the supervised reward-modulated
                  Spike-Timing-Dependent-Plasticity learning rule to
                  train two different SNN-based sub-controllers to
                  replicate a desired obstacle avoiding and goal
                  approaching behavior, provided by pre-generated
                  datasets. Together they make up a target-reaching
                  controller, which is used to control a simulated
                  mobile robot to reach a target area while avoiding
                  obstacles in its path. We demonstrate the
                  performance and effectiveness of our trained SNNs to
                  achieve target reaching tasks in different unknown
                  scenarios.}
}

@ARTICLE{10.3389/fnins.2018.00435,
  AUTHOR =       {Lee, Chankyu and Panda, Priyadarshini and
                  Srinivasan, Gopalakrishnan and Roy, Kaushik},
  TITLE =        {Training Deep Spiking Convolutional Neural Networks
                  With STDP-Based Unsupervised Pre-training Followed
                  by Supervised Fine-Tuning},
  JOURNAL =      {Frontiers in Neuroscience},
  VOLUME =       12,
  PAGES =        435,
  YEAR =         2018,
  URL =
                  {https://www.frontiersin.org/article/10.3389/fnins.2018.00435},
  DOI =          {10.3389/fnins.2018.00435},
  ISSN =         {1662-453X},
  ABSTRACT =     {Spiking Neural Networks (SNNs) are fast becoming a
                  promising candidate for brain-inspired neuromorphic
                  computing because of their inherent power efficiency
                  and impressive inference accuracy across several
                  cognitive tasks such as image classification and
                  speech recognition. The recent efforts in SNNs have
                  been focused on implementing deeper networks with
                  multiple hidden layers to incorporate exponentially
                  more difficult functional representations. In this
                  paper, we propose a pre-training scheme using
                  biologically plausible unsupervised learning, namely
                  Spike-Timing-Dependent-Plasticity (STDP), in order
                  to better initialize the parameters in multi-layer
                  systems prior to supervised optimization. The
                  multi-layer SNN is comprised of alternating
                  convolutional and pooling layers followed by
                  fully-connected layers, which are populated with
                  leaky integrate-and-fire spiking neurons. We train
                  the deep SNNs in two phases wherein, first,
                  convolutional kernels are pre-trained in a
                  layer-wise manner with unsupervised learning
                  followed by fine-tuning the synaptic weights with
                  spike-based supervised gradient descent
                  backpropagation. Our experiments on digit
                  recognition demonstrate that the STDP-based
                  pre-training with gradient-based optimization
                  provides improved robustness, faster (~2.5 ×)
                  training time and better generalization compared
                  with purely gradient-based training without
                  pre-training.}
}

@incollection{NIPS2018_7417,
  title =        {Gradient Descent for Spiking Neural Networks},
  author =       {Huh, Dongsung and Sejnowski, Terrence J},
  booktitle =    {Advances in Neural Information Processing Systems
                  31},
  editor =       {S. Bengio and H. Wallach and H. Larochelle and K.
                  Grauman and N. Cesa-Bianchi and R. Garnett},
  pages =        {1433--1443},
  year =         2018,
  publisher =    {Curran Associates, Inc.},
  url =
                  {http://papers.nips.cc/paper/7417-gradient-descent-for-spiking-neural-networks.pdf}
}

@ARTICLE{10.3389/fncom.2017.00024,
  author =       {Scellier, Benjamin and Bengio, Yoshua},
  title =        {Equilibrium Propagation: Bridging the Gap between
                  Energy-Based Models and Backpropagation},
  journal =      {Frontiers in Computational Neuroscience},
  volume =       11,
  pages =        24,
  year =         2017,
  url =
                  {https://www.frontiersin.org/article/10.3389/fncom.2017.00024},
  doi =          {10.3389/fncom.2017.00024},
  issn =         {1662-5188},
  abstract =     {We introduce Equilibrium Propagation, a learning
                  framework for energy-based models. It involves only
                  one kind of neural computation, performed in both
                  the first phase (when the prediction is made) and
                  the second phase of training (after the target or
                  prediction error is revealed). Although this
                  algorithm computes the gradient of an objective
                  function just like Backpropagation, it does not need
                  a special computation or circuit for the second
                  phase, where errors are implicitly propagated.
                  Equilibrium Propagation shares similarities with
                  Contrastive Hebbian Learning and Contrastive
                  Divergence while solving the theoretical issues of
                  both algorithms: our algorithm computes the gradient
                  of a well-defined objective function. Because the
                  objective function is defined in terms of local
                  perturbations, the second phase of Equilibrium
                  Propagation corresponds to only nudging the
                  prediction (fixed point or stationary distribution)
                  toward a configuration that reduces prediction
                  error. In the case of a recurrent multi-layer
                  supervised network, the output units are slightly
                  nudged toward their target in the second phase, and
                  the perturbation introduced at the output layer
                  propagates backward in the hidden layers. We show
                  that the signal “back-propagated” during this second
                  phase corresponds to the propagation of error
                  derivatives and encodes the gradient of the
                  objective function, when the synaptic update
                  corresponds to a standard form of spike-timing
                  dependent plasticity. This work makes it more
                  plausible that a mechanism similar to
                  Backpropagation could be implemented by brains,
                  since leaky integrator neural computation performs
                  both inference and error back-propagation in our
                  model. The only local difference between the two
                  phases is whether synaptic changes are allowed or
                  not. We also show experimentally that multi-layer
                  recurrently connected networks with 1, 2, and 3
                  hidden layers can be trained by Equilibrium
                  Propagation on the permutation-invariant MNIST
                  task.}
}

@InProceedings{pmlr-v89-o-connor19a,
  title = 	 {Training a Spiking Neural Network with Equilibrium Propagation},
  author = 	 {O'Connor, Peter and Gavves, Efstratios and Welling, Max},
  booktitle = 	 {Proceedings of Machine Learning Research},
  pages = 	 {1516--1523},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {16--18 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/o-connor19a/o-connor19a.pdf},
  url = 	 {http://proceedings.mlr.press/v89/o-connor19a.html},
  abstract = 	 {Backpropagation is almost universally used to train artificial neural networks.  However, there are several reasons that backpropagation could not be plausibly implemented by biological neurons.  Among these are the facts that (1) biological neurons appear to lack any mechanism for sending gradients backwards across synapses, and (2) biological “spiking” neurons emit binary signals, whereas back-propagation requires that neurons communicate continuous values between one another.   Recently, Scellier and Bengio [2017], demonstrated an alternative to backpropagation, called Equilibrium Propagation, wherein gradients are implicitly computed by the dynamics of the neural network, so that neurons do not need an internal mechanism for backpropagation of gradients.  This provides an interesting solution to problem (1).  In this paper, we address problem (2) by proposing a way in which Equilibrium Propagation can be implemented with neurons which are constrained to just communicate binary values at each time step.  We show that with appropriate step-size annealing, we can converge to the same fixed-point as a real-valued neural network, and that with predictive coding, we can make this convergence much faster. We demonstrate that the resulting model can be used to train a spiking neural network using the update scheme from Equilibrium propagation.}
}


@article{urbanczik09_gradien_learn_rule_tempot,
  author =       {Robert Urbanczik and Walter Senn},
  title =        {A Gradient Learning Rule for the Tempotron},
  journal =      {Neural Computation},
  volume =       21,
  number =       2,
  pages =        {340-352},
  year =         2009,
  doi =          {10.1162/neco.2008.09-07-605},
  url =          {https://doi.org/10.1162/neco.2008.09-07-605},
  DATE_ADDED =   {Fri Nov 1 16:00:33 2019},
}

@article{training_deep_snn_bpp_lee,
author = {Lee, Jun and Delbruck, Tobi and Pfeiffer, Michael},
year = {2016},
month = {08},
pages = {},
title = {Training Deep Spiking Neural Networks Using Backpropagation},
volume = {10},
journal = {Frontiers in Neuroscience},
doi = {10.3389/fnins.2016.00508}
}

@inproceedings{spikeprop,
author = {Bohte, Sander and Kok, Joost and Poutré, Johannes},
year = {2000},
month = {01},
pages = {419-424},
title = {SpikeProp: backpropagation for networks of spiking neurons.},
volume = {48},
journal = {ESANN}
}
@article{comsa19_tempor_codin_spikin_neural_networ,
  author =       {Comsa, Iulia M. and Potempa, Krzysztof and Versari,
                  Luca and Fischbacher, Thomas and Gesmundo, Andrea
                  and Alakuijala, Jyrki},
  title =        {Temporal Coding in Spiking Neural Networks With
                  Alpha Synaptic Function},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1907.13223v2},
  abstract =     {The timing of individual neuronal spikes is
                  essential for biological brains to make fast
                  responses to sensory stimuli. However, conventional
                  artificial neural networks lack the intrinsic
                  temporal coding ability present in biological
                  networks. We propose a spiking neural network model
                  that encodes information in the relative timing of
                  individual neuron spikes. In classification tasks,
                  the output of the network is indicated by the first
                  neuron to spike in the output layer. This temporal
                  coding scheme allows the supervised training of the
                  network with backpropagation, using locally exact
                  derivatives of the postsynaptic spike times with
                  respect to presynaptic spike times. The network
                  operates using a biologically-plausible alpha
                  synaptic transfer function. Additionally, we use
                  trainable synchronisation pulses that provide bias,
                  add flexibility during training and exploit the
                  decay part of the alpha function. We show that such
                  networks can be trained successfully on noisy
                  Boolean logic tasks and on the MNIST dataset encoded
                  in time. The results show that the spiking neural
                  network outperforms comparable spiking models on
                  MNIST and achieves similar quality to fully
                  connected conventional networks with the same
                  architecture. We also find that the spiking network
                  spontaneously discovers two operating regimes,
                  mirroring the accuracy-speed trade-off observed in
                  human decision-making: a slow regime, where a
                  decision is taken after all hidden neurons have
                  spiked and the accuracy is very high, and a fast
                  regime, where a decision is taken very fast but the
                  accuracy is lower. These results demonstrate the
                  computational power of spiking networks with
                  biological characteristics that encode information
                  in the timing of individual neurons. By studying
                  temporal coding in spiking networks, we aim to
                  create building blocks towards energy-efficient and
                  more complex biologically-inspired neural
                  architectures.},
  archivePrefix ={arXiv},
  eprint =       {1907.13223},
  primaryClass = {cs.NE},
}
@article{rueckauer16_theor_tools_conver_analog_to,
  author =       {Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu,
                  Yuhuang and Pfeiffer, Michael},
  title =        {Theory and Tools for the Conversion of Analog To
                  Spiking Convolutional Neural Networks},
  journal =      {CoRR},
  year =         2016,
  url =          {http://arxiv.org/abs/1612.04052v1},
  abstract =     {Deep convolutional neural networks (CNNs) have shown
                  great potential for numerous real-world machine
                  learning applications, but performing inference in
                  large CNNs in real-time remains a challenge. We have
                  previously demonstrated that traditional CNNs can be
                  converted into deep spiking neural networks (SNNs),
                  which exhibit similar accuracy while reducing both
                  latency and computational load as a consequence of
                  their data-driven, event-based style of computing.
                  Here we provide a novel theory that explains why
                  this conversion is successful, and derive from it
                  several new tools to convert a larger and more
                  powerful class of deep networks into SNNs. We
                  identify the main sources of approximation errors in
                  previous conversion methods, and propose simple
                  mechanisms to fix these issues. Furthermore, we
                  develop spiking implementations of common CNN
                  operations such as max-pooling, softmax, and
                  batch-normalization, which allow almost loss-less
                  conversion of arbitrary CNN architectures into the
                  spiking domain. Empirical evaluation of different
                  network architectures on the MNIST and CIFAR10
                  benchmarks leads to the best SNN results reported to
                  date.},
  archivePrefix ={arXiv},
  eprint =       {1612.04052},
  primaryClass = {stat.ML},
}

@article{pfeiffer2018deep,
  title={Deep learning with spiking neurons: opportunities and challenges},
  author={Pfeiffer, Michael and Pfeil, Thomas},
  journal={Frontiers in neuroscience},
  volume={12},
  year={2018},
  publisher={Frontiers Media SA}
}


@article{guetig14_to_spike_or_when_to_spike,
  author =       {Robert G{\"u}tig},
  title =        {To Spike, Or When To Spike?},
  journal =      {Current Opinion in Neurobiology},
  volume =       25,
  number =       {nil},
  pages =        {134-139},
  year =         2014,
  doi =          {10.1016/j.conb.2014.01.004},
  url =          {https://doi.org/10.1016/j.conb.2014.01.004},
  DATE_ADDED =   {Fri Nov 1 16:23:04 2019},
}

@article{stemmler1996single,
  title={A single spike suffices: the simplest form of stochastic resonance in model neurons},
  author={Stemmler, Martin},
  journal={Network: Computation in Neural Systems},
  volume={7},
  number={4},
  pages={687--716},
  year={1996},
  publisher={Taylor \& Francis}
}


@ARTICLE{10.3389/fnins.2015.00481,
  
AUTHOR={Serrano-Gotarredona, Teresa and Linares-Barranco, Bernabé},   
	 
TITLE={Poker-DVS and MNIST-DVS. Their History, How They Were Made, and Other Details},      
	
JOURNAL={Frontiers in Neuroscience},      
	
VOLUME={9},      

PAGES={481},     
	
YEAR={2015},      
	  
URL={https://www.frontiersin.org/article/10.3389/fnins.2015.00481},       
	
DOI={10.3389/fnins.2015.00481},      
	
ISSN={1662-453X},   
   
ABSTRACT={This article reports on two databases for event-driven object recognition using a Dynamic Vision Sensor (DVS). The first, which we call Poker-DVS and is being released together with this article, was obtained by browsing specially made poker card decks in front of a DVS camera for 2–4 s. Each card appeared on the screen for about 20–30 ms. The poker pips were tracked and isolated off-line to constitute the 131-recording Poker-DVS database. The second database, which we call MNIST-DVS and which was released in December 2013, consists of a set of 30,000 DVS camera recordings obtained by displaying 10,000 moving symbols from the standard MNIST 70,000-picture database on an LCD monitor for about 2–3 s each. Each of the 10,000 symbols was displayed at three different scales, so that event-driven object recognition algorithms could easily be tested for different object sizes. This article tells the story behind both databases, covering, among other aspects, details of how they work and the reasons for their creation. We provide not only the databases with corresponding scripts, but also the scripts and data used to generate the figures shown in this article (as Supplementary Material).}
}

@misc{openai_gym,
  author =       {OpenAI},
  howpublished = {https://gym.openai.com/envs/CartPole-v0/},
  note =         {Online; accessed 02 November 2019},
  title =        {OpenAI Gym},
  year =         {2019},
}

@InProceedings{ klaus_greff-proc-scipy-2017,
  author    = { {K}laus {G}reff and {A}aron {K}lein and {M}artin {C}hovanec and {F}rank {H}utter and {J}\"urgen {S}chmidhuber },
  title     = { {T}he {S}acred {I}nfrastructure for {C}omputational {R}esearch },
  booktitle = { {P}roceedings of the 16th {P}ython in {S}cience {C}onference },
  pages     = { 49 - 56 },
  year      = { 2017 },
  editor    = { {K}aty {H}uff and {D}avid {L}ippa and {D}illon {N}iederhut and {M} {P}acer },
  doi       = { 10.25080/shinma-7f4c6e7-008 }
}

@article{heeger2000poisson,
  title={Poisson model of spike generation},
  author={Heeger, David},
  journal={Handout, University of Standford},
  volume={5},
  pages={1--13},
  year={2000}
}


@article{stemmler96_singl_spike_suffic,
  author =       {Martin Stemmler},
  title =        {A Single Spike Suffices: the Simplest Form of
                  Stochastic Resonance in Model Neurons},
  journal =      {Network: Computation in Neural Systems},
  volume =       7,
  number =       4,
  pages =        {687-716},
  year =         1996,
  doi =          {10.1088/0954-898x_7_4_005},
  url =          {https://doi.org/10.1088/0954-898x_7_4_005},
  DATE_ADDED =   {Sat Nov 2 19:32:20 2019},
}

@article{florian07_reinf_learn_throug_modul_spike,
  author =       {Răzvan V. Florian},
  title =        {Reinforcement Learning Through Modulation of
                  Spike-Timing-Dependent Synaptic Plasticity},
  journal =      {Neural Computation},
  volume =       19,
  number =       6,
  pages =        {1468-1502},
  year =         2007,
  doi =          {10.1162/neco.2007.19.6.1468},
  url =          {https://doi.org/10.1162/neco.2007.19.6.1468},
  DATE_ADDED =   {Mon Nov 4 15:30:29 2019},
}

@inproceedings{florian2005,
author = {Florian, Răzvan},
year = {2005},
month = {10},
pages = {8 pp.-},
title = {A reinforcement learning algorithm for spiking neural networks},
volume = {2005},
isbn = {0-7695-2453-2},
journal = {Proceedings - Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, SYNASC 2005},
doi = {10.1109/SYNASC.2005.13}
}

@article{VITANZA20153122,
title = "Spiking neural controllers in multi-agent competitive systems for adaptive targeted motor learning",
journal = "Journal of the Franklin Institute",
volume = "352",
number = "8",
pages = "3122 - 3143",
year = "2015",
note = "Special Issue on Advances in Nonlinear Dynamics and Control",
issn = "0016-0032",
doi = "https://doi.org/10.1016/j.jfranklin.2015.04.014",
url = "http://www.sciencedirect.com/science/article/pii/S001600321500174X",
author = "Alessandra Vitanza and Luca Patané and Paolo Arena",
abstract = "The proposed work introduces a neural control strategy for guiding adaptation in spiking neural structures acting as nonlinear controllers in a group of bio-inspired robots which compete in reaching targets in a virtual environment. The neural structures embedded into each agent are inspired by a specific part of the insect brain, namely Central Complex, devoted to detect, learn and memorize visual features for targeted motor control. A reduced-order model of a spiking neuron is used as the basic building block for the neural controller. The control methodology employs bio-inspired, correlation based learning mechanisms like Spike timing dependent plasticity with the addition of a reward/punishment-based method experimentally found in insects. The reference signal for the overall multi-agent control system is imposed by a global reward, which guides motor learning to direct each agent towards specific visual targets. The neural controllers within the agents start from identical conditions: the learning strategy induces each robot to show anticipated targeting actions upon specific visual stimuli. The whole control structure also contributes to make the robots refractory or more sensitive to specific visual stimuli, showing distinct preferences in future choices. This leads to an environmentally induced, targeted motor control, even without a direct communication among the agents, giving robots, while running, the ability to perform adaptation in real-time. Experiments, carried out in a dynamic simulation environment, show the suitability of the proposed approach. Specific performance indexes, like Shannon׳s Entropy, are adopted to quantitatively analyze diversity and specialization within the group."
}

@article{aenugu19_reinf_learn_with_spikin_coagen,
  author =       {Aenugu, Sneha and Sharma, Abhishek and Yelamarthi,
                  Sasikiran and Hazan, Hananel and Thomas, Philip S.
                  and Kozma, Robert},
  title =        {Reinforcement Learning With Spiking Coagents},
  journal =      {CoRR},
  year =         2019,
  url =          {http://arxiv.org/abs/1910.06489v2},
  abstract =     {Neuroscientific theory suggests that dopaminergic
                  neurons broadcast global reward prediction errors to
                  large areas of the brain influencing the synaptic
                  plasticity of the neurons in those regions. We build
                  on this theory to propose a multi-agent learning
                  framework with spiking neurons in the generalized
                  linear model (GLM) formulation as agents, to solve
                  reinforcement learning (RL) tasks. We show that a
                  network of GLM spiking agents connected in a
                  hierarchical fashion, where each spiking agent
                  modulates its firing policy based on local
                  information and a global prediction error, can learn
                  complex action representations to solve RL tasks. We
                  further show how leveraging principles of modularity
                  and population coding inspired from the brain can
                  help reduce variance in the learning updates making
                  it a viable optimization technique.},
  archivePrefix ={arXiv},
  eprint =       {1910.06489},
  primaryClass = {cs.LG},
}