#+SETUPFILE: ./export_template.org
#+TITLE: Tensorflow
* What is Tensorflow
TensorFlow is open-source software library for numerical computation.
A graph of computations is defined, and TensorFlow builds an optimized
graph from that.

TensorFlow breaks up the graph into several chunks and run them in
parallel across multiple CPUs or GPUs where possible, and also
supports distributed computing.

#+DOWNLOADED: https://3.bp.blogspot.com/-l2UT45WGdyw/Wbe7au1nfwI/AAAAAAAAD1I/GeQcQUUWezIiaFFRCiMILlX2EYdG49C0wCLcBGAs/s1600/image6.png @ 2018-09-15 14:26:09
[[file:images/tensorflow/TF Estimator API/image6_2018-09-15_14-26-09.png]]

** Glossary
- tensor :: A tf.Tensor object represents a partially defined
            computation that will eventually produce a value.
            TensorFlow programs work by first building a graph of
            tf.Tensor objects, detailing how each tensor is computed
            based on the other available tensors and then by running
            parts of this graph to achieve the desired results.
* TF Slim
** ~arg_scope~
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/arg_scope.py

Example of how to use tf.contrib.framework.arg_scope:

#+BEGIN_SRC python
  from third_party.tensorflow.contrib.layers.python import layers

  arg_scope = tf.contrib.framework.arg_scope

  with arg_scope([layers.conv2d], padding='SAME',
                 initializer=layers.variance_scaling_initializer(),
                 regularizer=layers.l2_regularizer(0.05)):
    net = layers.conv2d(inputs, 64, [11, 11], 4, padding='VALID', scope='conv1')
    net = layers.conv2d(net, 256, [5, 5], scope='conv2')
#+END_SRC

The first call to conv2d will behave as follows:

#+BEGIN_SRC python
  layers.conv2d(inputs, 64, [11, 11], 4, padding='VALID',
                initializer=layers.variance_scaling_initializer(),
                regularizer=layers.l2_regularizer(0.05), scope='conv1')
#+END_SRC
The second call to conv2d will also use the arg_scope's default for
padding:

#+BEGIN_SRC python
  layers.conv2d(inputs, 256, [5, 5], padding='SAME',
                initializer=layers.variance_scaling_initializer(),
                regularizer=layers.l2_regularizer(0.05), scope='conv2')
#+END_SRC

  Example of how to reuse an arg_scope:

#+BEGIN_SRC python
  with arg_scope([layers.conv2d], padding='SAME',
                 initializer=layers.variance_scaling_initializer(),
                 regularizer=layers.l2_regularizer(0.05)) as sc:
      net = layers.conv2d(net, 256, [5, 5], scope='conv1')
      # ...

  with arg_scope(sc):
      net = layers.conv2d(net, 256, [5, 5], scope='conv2')
#+END_SRC

  Example of how to use tf.contrib.framework.add_arg_scope to enable your
  function to be called within an arg_scope later:

#+BEGIN_SRC python
  @tf.contrib.framework.add_arg_scope
  def conv2d(*args, **kwargs)
#+END_SRC
* TF Serve
https://towardsdatascience.com/introducing-tfserve-simple-and-easy-http-server-for-tensorflow-model-inference-582ea1b07da8?source=rss----7f60cf5620c9---4
* TF Estimator
#+DOWNLOADED: https://1.bp.blogspot.com/-njTtnjOq_cE/Wbe772URrgI/AAAAAAAAD1Y/h1mWj6MGSzYg_KDuVXWBYeNqA4z5WRSpACLcBGAs/s1600/image2.jpg @ 2018-09-15 14:42:49
[[file:images/tensorflow/TF Estimator API/image2_2018-09-15_14-42-49.jpg]]

References:
- http://ruder.io/text-classification-tensorflow-estimators/
- https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html
- https://arxiv.org/abs/1708.02637

At the heart of our framework is Estimator, a class that both provides
an interface for downstream infrastructure, as well as a convenient
harness for developers. Te interface for users of Estimator is loosely
modeled afer Scikit-learn and consists of only four methods: ~train~
trains the model, given training data. ~evaluate~ computes evaluation
metrics over test data, ~predict~ performs inference on new data given a
trained model, and fnally, ~export_savedmodel~ exports a SavedModel, a
serialization format which allows the model to be used in TensorFlow
Serving, a prebuilt production server for TensorFlow models.

| Estimator method | Mode parameter set |
|------------------+--------------------|
| train()          | ModeKeys.TRAIN     |
| evaluate()       | ModeKeys.EVAL      |
| predict()        | ModeKeys.PREDICT   |

Estimators receive a configuration object called ~RunConfig~ which
communicates everything that the Estimator needs to know about the
environment in which the model will be run: how many workers are
available,, how to save intermediate checkpoints etc.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-09-16 22:17:13
[[file:images/tensorflow/TF Estimator/screenshot_2018-09-16_22-17-13.png]]

~train~, ~evaluate~ and ~predict~ take an input function, which is expected
to produce two dictionaries: one containing Tensors with inputs
(features), and one containing Tensors with labels.

** Predict
 When ~model_fn~ is called with ~mode == ModeKeys.PREDICT~, the model
function must return a ~tf.estimator.EstimatorSpec~ containing the
following information:

1. the mode, which is ~tf.estimator.ModeKeys.PREDICT~
2.  the prediction

#+BEGIN_SRC python
  # class_ids will be the model prediction for the class (Iris flower type)
  # The output node with the highest value is our prediction
  predictions = { 'class_ids': tf.argmax(input=logits, axis=1) }

  # Return our prediction
  if mode == tf.estimator.ModeKeys.PREDICT:
     return tf.estimator.EstimatorSpec(mode, predictions=predictions)
#+END_SRC

** Eval
When ~model_fn~ is called with ~mode == ModeKeys.EVAL~, the model function must evaluate the model, returning loss. 

#+BEGIN_SRC python
  # To calculate the loss, we need to convert our labels
  # Our input labels have shape: [batch_size, 1]
  labels = tf.squeeze(labels, 1)          # Convert to shape [batch_size]
  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
#+END_SRC

We can also compute and return additional metrics.
#+BEGIN_SRC python
  # Calculate the accuracy between the true labels, and our predictions
  accuracy = tf.metrics.accuracy(labels, predictions['class_ids'])
#+END_SRC

#+BEGIN_SRC python
  if mode == tf.estimator.ModeKeys.EVAL:
     return tf.estimator.EstimatorSpec(
         mode,
         loss=loss,
         eval_metric_ops={'my_accuracy': accuracy})
#+END_SRC

** Train
When ~model_fn~ is called with ~mode == ModeKeys.TRAIN~, the model
function must train the model. 

#+BEGIN_SRC python
  optimizer = tf.train.AdagradOptimizer(0.05)
  train_op = optimizer.minimize(
     loss,
     global_step=tf.train.get_global_step())

  # Set the TensorBoard scalar my_accuracy to the accuracy
  tf.summary.scalar('my_accuracy', accuracy[1])

  return tf.estimator.EstimatorSpec(
     mode,
     loss=loss,
     train_op=train_op)
#+END_SRC
* TF Feature Columns
Reference:
https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html

#+DOWNLOADED: https://3.bp.blogspot.com/-3Wf_6BEn7GE/Wg4GiQ9TXDI/AAAAAAAAEGo/yoLiIyJW1c4Vh-VfP4vVjuaD92rcnVphACLcBGAs/s1600/2.jpg @ 2018-09-15 15:03:18
[[file:images/tensorflow/TF Feature Columns/2_2018-09-15_15-03-18.jpg]]

Feature columns bridge raw data with the data your model needs. 

There are nine functions in the ~tf.feature_column~ api.
** Numeric column

#+BEGIN_SRC python
  numeric_feature_column = tf.feature_column.numeric_column(key="SepalLength",
                                                            dtype=tf.float64)

  vector_feature_column = tf.feature_column.numeric_column(key="Bowling",
                                                           shape=10)

  matrix_feature_column = tf.feature_column.numeric_column(key="MyMatrix",
                                                           shape=[10,5]) 
#+END_SRC

** Bucketized column

Often, you don't want to feed a number directly into the model, but
instead split its value into different categories based on numerical
ranges. Consider the following bucketing scheme:


#+DOWNLOADED: https://2.bp.blogspot.com/-qrTI2ZUBr7w/Wg4G9lWHk5I/AAAAAAAAEG0/v17Zqcix1Wou5ZRpTGxAQ8jMSBjCKmCAACLcBGAs/s1600/4.jpg @ 2018-09-15 15:09:59
[[file:images/tensorflow/TF Feature Columns/4_2018-09-15_15-09-59.jpg]]

We create the bucketized column from a numeric column:

#+BEGIN_SRC python
  # A numeric column for the raw input.
  numeric_feature_column = tf.feature_column.numeric_column("Year")

  # Bucketize the numeric column on the years 1960, 1980, and 2000
  bucketized_feature_column = tf.feature_column.bucketized_column(
      source_column = numeric_feature_column,
      boundaries = [1960, 1980, 2000])
#+END_SRC

** Categorical Identity Column

Categorical identity columns are a special case of bucketized columns.
In a categorical identity column, each bucket represents a single,
unique integer.

#+DOWNLOADED: https://4.bp.blogspot.com/-cG-gEXRkohM/Wg4HGyFNWxI/AAAAAAAAEG4/qPzVWnap6PkxDDyZUWuYP_WYoRca3z7yQCLcBGAs/s1600/5.jpg @ 2018-09-15 15:20:10
[[file:images/tensorflow/TF Feature Columns/5_2018-09-15_15-20-10.jpg]]

This is a one-hot encoding, not a binary numerical encoding.

#+BEGIN_SRC python
  # Create a categorical output for input "feature_name_from_input_fn",
  # which must be of integer type. Value is expected to be >= 0 and < num_buckets
  identity_feature_column = tf.feature_column.categorical_column_with_identity(
      key='feature_name_from_input_fn', 
      num_buckets=4) # Values [0, 4)

  # The 'feature_name_from_input_fn' above needs to match an integer key that is 
  # returned from input_fn (see below). So for this case, 'Integer_1' or
  # 'Integer_2' would be valid strings instead of 'feature_name_from_input_fn'.
  # For more information, please check out Part 1 of this blog series.
  def input_fn():
      ...<code>...
      return ({ 'Integer_1':[values], ..<etc>.., 'Integer_2':[values] },
              [Label_values])
#+END_SRC

** Categorical vocabulary column


#+DOWNLOADED: https://1.bp.blogspot.com/-tATYn91S0Mw/Wg4HVJgTy6I/AAAAAAAAEG8/I0GiWJH0aBYSwfuyBFGwRiS0SHVVGrNngCLcBGAs/s1600/6.jpg @ 2018-09-15 15:20:30
[[file:images/tensorflow/TF Feature Columns/6_2018-09-15_15-20-30.jpg]]

We cannot input strings directly to a model. Instead, we must first
map strings to numeric or categorical values. Categorical vocabulary
columns provide a good way to represent strings as a one-hot vector.

#+BEGIN_SRC python
  # Given input "feature_name_from_input_fn" which is a string,
  # create a categorical feature to our model by mapping the input to one of
  # the elements in the vocabulary list.
  vocabulary_feature_column =
      tf.feature_column.categorical_column_with_vocabulary_list(
          key="feature_name_from_input_fn",
          vocabulary_list=["kitchenware", "electronics", "sports"])

  # Given input "feature_name_from_input_fn" which is a string,
  # create a categorical feature to our model by mapping the input to one of 
  # the elements in the vocabulary list.
  vocabulary_feature_column =
      tf.feature_column.categorical_column_with_vocabulary_list(
          key="feature_name_from_input_fn",
          vocabulary_list=["kitchenware", "electronics", "sports"]) 
#+END_SRC

In many cases, the number of categories is large, and we can limit it
via hashing:


#+DOWNLOADED: https://3.bp.blogspot.com/--IhLgHs0JYE/Wg4INL8zoYI/AAAAAAAAEHI/Q_wl_7jrTYMgoxH__m1GAG5ilYAU_M4TgCLcBGAs/s1600/7.jpg @ 2018-09-15 15:20:51
[[file:images/tensorflow/TF Feature Columns/7_2018-09-15_15-20-51.jpg]]

#+BEGIN_SRC python
  # Create categorical output for input "feature_name_from_input_fn".
  # Category becomes: hash_value("feature_name_from_input_fn") % hash_bucket_size
  hashed_feature_column =
      tf.feature_column.categorical_column_with_hash_bucket(
          key = "feature_name_from_input_fn",
          hash_buckets_size = 100) # The number of categories
#+END_SRC

** Feature Crosses
Combining features allows the model to learn separate weights
specifically for whatever that feature combination means.

#+BEGIN_SRC python
  # In our input_fn, we convert input longitude and latitude to integer values
  # in the range [0, 100)
  def input_fn():
      # Using Datasets, read the input values for longitude and latitude
      latitude = ...   # A tf.float32 value
      longitude = ...  # A tf.float32 value

      # In our example we just return our lat_int, long_int features.
      # The dictionary of a complete program would probably have more keys.
      return { "latitude": latitude, "longitude": longitude, ...}, labels

  # As can be see from the map, we want to split the latitude range
  # [33.641336, 33.887157] into 100 buckets. To do this we use np.linspace
  # to get a list of 99 numbers between min and max of this range.
  # Using this list we can bucketize latitude into 100 buckets.
  latitude_buckets = list(np.linspace(33.641336, 33.887157, 99))
  latitude_fc = tf.feature_column.bucketized_column(
      tf.feature_column.numeric_column('latitude'),
      latitude_buckets)

  # Do the same bucketization for longitude as done for latitude.
  longitude_buckets = list(np.linspace(-84.558798, -84.287259, 99))
  longitude_fc = tf.feature_column.bucketized_column(
      tf.feature_column.numeric_column('longitude'), longitude_buckets)

  # Create a feature cross of fc_longitude x fc_latitude.
  fc_san_francisco_boxed = tf.feature_column.crossed_column(
      keys=[latitude_fc, longitude_fc],
      hash_bucket_size=1000) # No precise rule, maybe 1000 buckets will be good?
#+END_SRC

** Indicator and Embedding columns
Indicator columns treat each category as an element in a one-hot
vector, where the matching category has value 1 and the rest have 0s.


#+DOWNLOADED: https://4.bp.blogspot.com/-w6qEZq65F1o/Wg4J6-F6O3I/AAAAAAAAEHY/hU8xIK8P854Ehed8HUMCKe5m0nI7UiNSACLcBGAs/s1600/6.jpg @ 2018-09-15 15:21:01
[[file:images/tensorflow/TF Feature Columns/6_2018-09-15_15-21-01.jpg]]

#+BEGIN_SRC python
  indicator_column = tf.feature_column.indicator_column(categorical_column)
#+END_SRC

An embedding column represents data as a lower-dimensional, ordinary
vector in which each cell can contain any number.


#+DOWNLOADED: https://2.bp.blogspot.com/-q7GLL9Z95uY/Wg4KIyRryYI/AAAAAAAAEHc/BckVSXOmT1M0qs79D60t2XMv1RFNSd89gCLcBGAs/s1600/image9.jpg @ 2018-09-15 15:22:02
[[file:images/tensorflow/TF Feature
Columns/image9_2018-09-15_15-22-02.jpg]]

As a guideline, the embedding vector dimension should be the 4th root
of the number of categories.

#+BEGIN_SRC python
  categorical_column = ... # Create any categorical column

  # Represent the categorical column as an embedding column.
  # This means creating a one-hot vector with one element for each category.
  embedding_column = tf.feature_column.embedding_column(
      categorical_column=categorical_column,
      dimension=dimension_of_embedding_vector)
#+END_SRC
* Hooks
Hooks are useful for custom processing that has to happen alongside
the main loop. For example, we can use hooks for recordkeeping,
debugging, monitoring or reporting. Hooks are activated by passing
them to the ~train~ call. Estimators use hooks internally to implement
checkpointing, summaries and more.

#+BEGIN_SRC python
  class TimeBasedStopHook(tf.train.SessionRunHook):
      def begin(self):
          self.started_at = time.time()

      def after_run(self, run_context, run_values):
          if time.time() - self.started_at >= TRAIN_TIME:
              run_context.request_stop()
#+END_SRC
* Experiment
The core of the distributed execution support is provided with the
~Experiment~ class.


#+DOWNLOADED: /tmp/screenshot.png @ 2018-09-16 22:26:07
[[file:images/tensorflow/Experiment/screenshot_2018-09-16_22-26-07.png]]

In each TensorFlow cluster, there are several parameter servers, and
several worker tasks. Most workers are handling the training process,
which basically calls the Estimator ~train~ method with the training
~input_fn~.

The primary mode of replica training in Estimators is between-graph
replication and asynchronous training.


