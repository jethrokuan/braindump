#+SETUPFILE: ./export_template.org
#+TITLE: Theory Of Computation
* Introduction
Finite automata are a useful model for many kinds of important
software and hardware. Automatas are found in compilers, software for
designing digital circuits, and many more systems.

/Grammars/ provide useful models when designing software that processes
data with a recursive structure. The compiler's parser deals with
recursively nested features. Regular Expressions also denote the
structure of data, especially in text strings.

The study of automata addresses the following questions:

1. What can a computer do?
2. What can a computer do efficiently?

Automata theory also provides a tool for making formal proofs, of both
the inductive and deductive type.

** Formal Proofs
Here the more common proof requirements are here:

1. Proving set equivalence: we can approach this by rephrasing it into
   an iff statement.
   1. Prove that if ~x~ is in ~E~, then ~x~  is in ~F~.
   2. Prove that if ~x~ is in ~F~, then ~x~ is in ~E~.

** Inductive Proofs
Suppose we are given a statement $S(n)$ to prove. The inductive
approach involves:
1. The /basis/: where we show $S(i)$ for a particular $i$.
2. The /inductive step/: where we show if $S(k)$ (or $S(i), S(i+1),
   \dots, S(k)$) then $S(k+1)$.

** Structural Inductions
We can sometimes prove statements by construction. This is often the
case with recursively defined structures, such as with trees and
expressions. This works because we the recursive definition is invoked
at each step, so we are guaranteed that at each step of the
construction, the construction $X_i$ is valid.

* Automata Theory

** Definitions
- alphabet :: An /alphabet/ is a finite, nonempty set of symbols. E.g.
              $\Sigma = \{0, 1\}$ represents the binary alphabet
- string :: A /string/ is a finite sequence of symbols chosen from some
            alphabet. For example, $01101$ is a string from the binary
            alphabet. The empty string is represented by \epsilon, and
            sometimes $\Lambda$. The
            /length/ of a string is denoted as such: $|001| = 3$
- Powers :: $\Sigma^k$ represents strings of length $k$.
- Concatenation :: $xy$ denotes the concatenation of strings $x$ and
                   $y$.
- Problem :: a /problem/ is the question of deciding whether a given
             string is a member of some particular language.

* Finite Automata
An automata has:
- a set of states
- its "control" moves from state to state in response to external inputs

A finite automata is one where the automaton can only be in a single
state at once (it is deterministic). Non-determinism allows us to
program solutions to problems using a higher-level language.

Determinism refers to the fact that on each input there is one and
only one state to which the automaton can transition from its current
state. Deterministic Finite Automata is often abbrieviated with /DFA/.

** Deterministic Finite Automata
A /dfa/ consists of:

1. A finite set of /states/, often denoted $Q$.
2. A finite set of /input symbols/, often denoted $\Sigma$.
3. A /transition function/ that takes as arguments a state and an input
   symbol and returns a state, often denoted $\delta$. If $q$ is a state,
   and $a$ is an input symbol, then $\delta(q,a)$ is that state $p$ such
   that there is an arc labeled $a$ from $q$ to $p$.
4. A /start state/, one of the states in $Q$.
5. A set of /final/ or /accepting/ states $F$. The set $F$ is a subset of
   $Q$.

In proofs, we often talk about a DFA in "five-tuple" notation:

\begin{equation}
  A = \left(Q, \Sigma, \delta, q_0, F \right)
\end{equation}

*** Simpler Notations
The two preferred notation for describing automata are:
- transition diagrams :: a graph

#+DOWNLOADED: https://quickgrid.files.wordpress.com/2015/10/subset-construction-nfa-from-transition-table.jpg @ 2018-08-14 12:47:06
[[file:images/theory_of_computation/Finite
Automata/subset-construction-nfa-from-transition-table_2018-08-14_12-47-06.jpg]]

- transition table :: a tubular listing of the $\delta$ function, which by
     implication tells us the states and the input alphabet.

#+DOWNLOADED: https://i.stack.imgur.com/jTETt.png @ 2018-08-14 12:49:00
[[file:images/theory_of_computation/Finite Automata/jTETt_2018-08-14_12-49-00.png]]

** Language of a DFA
We can define the /language/ of a DFA $A = \left(Q, \Sigma, q_0, F\right)$.
This language is denoted $L(A)$, and is defined by:

\begin{equation}
L(A) = \{ w | \delta(q_0, w) \text{ is in } F\}
\end{equation}

The language of $A$ is the set of strings $w$ that take the start
state $q_0$ to one of the accepting states.

** Extending Transition Function to Strings
Basis:
\begin{equation}
\hat{\delta}\left(q, \epsilon\right) = q
\end{equation}
Induction:
\begin{equation}
\hat{\delta}\left(q, xa\right) = \delta \left(\hat{\delta}\left(q, x\right), a \right)
\end{equation}

* Nondeterministic Finite Automata
A NFA has can be in several states at once, and this ability is
expressed as an ability to "guess" something about its input. It can
be shown that NFAs accept exactly the regular languages, just as DFAs
do. We can always convert an NFA to a DFA, although the latter may
have exponentially more states than the NFA.

** Definition
An NFA has:

1. A finite set of states $Q$.
2. A finite set of input symbols $\Sigma$.
3. A starting state $q_0 \in Q$,
4. A set of final states $F \subset Q$.
5. A transition function that takes a state in $Q$ and an input symbol
   in $\Sigma$ as arguments and returns a *subset* of $Q$.

** The Language of an NFA
if $A = (Q, \Sigma, \delta, q_0, F)$ is an NFA, then

\begin{equation}
L(A) = \{w | \hat{\delta}(q_0, w) \cap F \neq \emptyset\}
\end{equation}

That is, $L(A)$ is the set of strings $w$ in $\Sigma^*$ such that
$\hat{\delta}(q_0, w)$.

** The Equivalence of DFA and NFA

#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-14 13:44:15
[[file:images/theory_of_computation/Nondeterministic Finite Automata/screenshot_2018-08-14_13-44-15.png]]

** Finite Automata with Epsilon-Transitions
Transitions on \epsilon, the empty string, allow NFAs to make a transition
spontaneously. This is sometimes referred to as \epsilon-NFAs, and are
closely related to regular expressions.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-15 11:45:07
[[file:images/theory_of_computation/Nondeterministic Finite
Automata/screenshot_2018-08-15_11-45-07.png]]

Of particular interest is the transition from $q_0$ to $q_1$, where the
$+$ and $-$ sign is optional.

** Epsilon-Closures
We \epsilon-close a state $q$ by following all transitions out of $q$ that
are labelled \epsilon, eventually finding all states that can be reached
from $q$ along any path whose arcs are all labelled \epsilon.

\epsilon-closure allows us to explain easily what the transitions of an
\epsilon-NFA look like when given a sequence of (non-\epsilon) inputs. Suppose
that $E = (Q, \Sigma, \delta, q_0, F)$ is an \epsilon-NFA. We first define $\hat{\delta}$,
the extended transition function, to reflect what happens on a
sequence of inputs.

*BASIS*: $\hat{\delta}(q, \epsilon) = ECLOSE(q)$. If the label of the path is \epsilon,
then we can follow only \epsilon-labeled arcs extending from state $q$.

*INDUCTION*: Suppose $w$ is of the form $xa$, where $a$ is the last
symbol of $w$. Note that $a$ is a member of $\Sigma$; it cannot be \epsilon.
Then:

\begin{align}
  \text{Let } & \hat{\delta}(q, x) = \{p_1, p_2, \dots, p_k\} \\
   & \bigcup\limits_{i=1}^k \delta(p_i, a) = \{r_1, r_2, \dots, r_m\} \\
  \text{Then } & \hat{\delta}(q,w) = \bigcup\limits_{j=1}^m ECLOSE(r_j)
\end{align}

** Eliminating \epsilon-Transitions
Given any \epsilon-NFA $E$, we can find a DFA $D$ that accepts the same
language as $E$.

Let $E = (Q_E, \Sigma, \delta_E, q_0, F_E)$, then the equivalent DFA $D = (Q_D, \Sigma,
\delta_D, q_D, F_D)$ is defined as follows:

1. $Q_D$ is the set of subsets of $Q_E$. All accessible states of $D$
   are \epsilon-closed subsets of $Q_E$, i.e. $S \subseteq Q_K s.t. S =
   ECLOSE(S)$. Any \epsilon-transition out of one of the states in $S$
   leads to another state in $S$.
2. $q_D = ECLOSE(q_0)$, we get the start state of $D$ by closing the set
   consisting of only the start state of $E$.
3. $F_D$ is those set of states that contain at least one accepting
   state of $E$. $F_D = \{S | S \text{ is in } Q_D \text{ and } S \cap F_E
   \neq \emptyset \}$
4. $\delta_D(S,a)$ is computed for all $a$ in $\Sigma$ and sets $S$ in $Q_D$ by:
   1. Let $S = \{p_1, p_2, \dots, p_k\}$
   2. Compute $\bigcup\limits_{i=1}^{k}\delta_E(p_i, a) = \{r_1, r_2, \dots, r_m\}$
   3. Then $\delta_D(S, a) = \bigcup\limits_{j=1}^{m}ECLOSE(r_j)$


* Regular Expressions
Regular expressions may be thought of as a "programming language", in
which many important applications like text search applications or
compiler components can be expressed in.

Regular expressions can define the exact same languages that various
forms of automata describe: the regular languages. Regular expressions
denote languages. We define 3 operations on languages that the
operators of regular expressions represent.

1. The /union/ of two languages $L \bigcup M$, is the set of strings
   that are either in $L$ or $M$.
2. The /concatenation/ of languages $L$ and $M$ is the set of strings
   that can be formed by taking any string in $L$ and concatenating it
   with any string in $M$.
3. The closure (or star, or /Kleene closure/) is denoted $L^*$ and
   represents the set of those strings that can be formed by taking
   any number of strings from $L$, possibly with repetitions, and
   concatenating all of them.

We can describe regular expressions recursively. For each expression
$E$, we denote the language it represents with $L(E)$.

*BASIS*:

1. The constants $\epsilon$ and $\emptyset$ are regular expressions, denoting the
   languages $\{\epsilon\}$ and $\emptyset$ respectively.
2. If $a$ is a symbol, then $\mathbb{a}$ is a regular expression. This
   expression denotes the language $\{a\}$.

*INDUCTION*:

1. $L(E) + L(F) = L(E) \bigcup L(F)$
2. $L(EF) = L(E)L(F)$
3. $L(E^*) = (L(E))^*$
4. $L((E)) = L(E)$

** Precedence of regular expression operators

The precedence in order of highest to lowest, is:
1. star
2. dot (note that this operation is associative)
3. union ($\plus$ operator)

** Equivalence of DFA and Regular Expressions

We show this by showing that:
1. Every language defined by a DFA is also defined by a regular
  expression.
2. Every language defined by a regular expression is also defined by a
   $\epilon$-NFA, which we have already shown is equivalent to a DFA.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-28 12:46:13
[[file:images/theory_of_computation/Regular
Expressions/screenshot_2018-08-28_12-46-13.png]]

*** From DFA to Regular Expression
We can number the finite states in a DFA $A$ with $1, 2, \dots, n$.

Let $R_{ij}^{(k)}$ be the name of a regular expression whose language is the
set of strings $w$ such that $w$ is the label of a path from state $i$
to state $j$ in a DFA $A$, and the path has no intermediate node whose
number is greater than $k$.
To construct the expression $R_{ij}^{(k)}$, we use the following inductive
definition, starting at $k= 0$, and finally reaching $k=n$.

BASIS: $k=0$.
Since the states are numbered $1$ or above, the restriction on paths
is that the paths have no intermediate states at all. There are only 2
kinds of paths that meet such a condition:

1. An arc from node (state) $i$ to node $j$.
2. A path of length $0$ that consists only of some node $i$.

If $i \ne j$, then only case $1$ is possible. We must examine DFA $A$
and find input symbols $a$ such that there is a transition from state
$i$ to state $j$ on symbol $a$.

1. If there is no such symbol $a$, then $R_{ij}^{(0)} = \emptyset$.
2. If there is exactly one such symbol $a$, then $R_{ij}^{(0)} = \mathbb{a}$
3. If there are symbols $a_1, a_2, \dots, a_k$ that label arcs from
   state $i$ to state $j$, then $R_{ij}^{(0)} = \mathbb{a_1} + \mathbb{a_2} +
   \dots + \mathbb{a_k}$

In case (a), the expression becomes $\epsilon$, in case (c), the expression
becomes $\epsilon + \mathbb{a_1} + \mathbb{a_2} + \dots + \mathbb{a_k}$.

INDUCTION: Suppose there is a path from state $i$ to state $j$ that
goes through no state higher than $k$. Then either:

1. The path does not go through state $k$ at all. In this case, the
   label of the path is $R_{ij}^{(k-1)}$.
2. The path goes through state $k$ at least once. We can break the
   path into several pieces:


#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-28 12:58:35
[[file:images/theory_of_computation/Regular
Expressions/screenshot_2018-08-28_12-58-35.png]]

Then the set of labels for all paths of this type is represented by
the regular expression $R_{ik}^{(k-1)}(R_{kk}^{(k-1)})^*R_{kj}^{(k-1)}$. Then, we can
combine the expressions for the paths of the two above:

\begin{equation}
R_{ij}^{(k)} = R_{ij}^{(k-1)} + R_{ik}^{(k-1)}(R_{kk}^{(k-1)})^*R_{kj}^{(k-1)}
\end{equation}

We can compute $R_{ij}^{(n)}$ for all $i$ and $j$, and the language of the
automaton is then the sum of all expressions $R_{ij}^{(n)}$ such that state
$j$ is an accepting state.

*** Converting DFAs to regular expressions by eliminating states
The above method of conversation always works, but is expensive. $n^3$
expressions have to be constructed for an n-state automaton, but the
length of the expression can grow by a factor of 4 on the average,
with each of the $n$ inductive steps, and the expressions themselves
could reach on the order of $4^n$ symbols.

The approach introduced here avoids duplicating work at some points,
by eliminating states. If we eliminate a state $s$, then all paths
that went through $s$ no longer exist in the automaton. To preserve
the language, we must include on an arc that goes directly from $q$ to
$p$, the labels of paths that went from some state $q$ to $p$ through
$s$. This label now includes strings, but we can use a regular
expression to represent all such strings.

Hence, we can construct a regular expression from a finite automaton
as follows:

1. For each accepting state $q$, apply the reduction process to
   produce an equivalent automaton with regular-expression labels on
   the arcs. Eliminate all states except $q$ and the start state $q_0$.
2. If $q \neq q_0$, then a two-state automaton remains, as depicted. The
   regular expression for the automaton is $(R + SU^*T)^*SU^*$.


#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-28 23:30:02
[[file:images/theory_of_computation/Regular
Expressions/screenshot_2018-08-28_23-30-02.png]]

3. If the start state is also an accepting state, then we must perform
   a state-elimination from the original automaton that gets rid of
   every state but the start state, leaving a one-state automaton,
   which accepts $R^*$.


#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-28 23:30:48
[[file:images/theory_of_computation/Regular
Expressions/screenshot_2018-08-28_23-30-48.png]]

*** Converting regular expressions to automata
We can show every language defined by a regular expression is also
defined by a finite automaton, and we do so by converting any regular
expression $R$ to an $\epsilon$-NFA $E$ with:

1. Exactly one accepting state
2. No arcs into initial state
3. No arcs out of the accepting state

The proof is conducted by structural induction on R, following the
recursive definition of regular expressions.

The basis of the induction involves constructing automatons for
regular expressions (a) $\epsilon$, (b) $\emptyset$ and (c) $\mathbb{a}$. They are displayed
below:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-28 23:36:18
[[file:images/theory_of_computation/Regular Expressions/screenshot_2018-08-28_23-36-18.png]]

The inductive step consists of 4 cases: (a) The expression is $R + S$
for some smaller expressions $R$ and $S$. (b) The expression is $RS$
for smaller expressions $R$ and $S$. (c) The expression is $R*$ for
some smaller expression $R$. (d) The expression is (R) for some
expression R. The automatons for (a), (b), and (c) are shown below:


#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-28 23:39:41
[[file:images/theory_of_computation/Regular
Expressions/screenshot_2018-08-28_23-39-41.png]]

The automaton for $R$ also serves as the automaton for $(R)$.

*** Algebraic law for regular expressions
 - commutativity :: $x + y = y + x$.
 - associativity :: $(x \times y) \times z = x \times (y \times z)$.
 - distributive :: $x \times (y + z) = x \times y + x \times z$

- $L + M = M + L$
- $(L + M) + N = L + (M + N)$
- $(LM)N = L(MN)$
- $\emptyset + L = L + \emptyset = L$. $\emptyset$ is the identity for union.
- $\epsilon L = L \epsilon = L$. $\epsilon$ is the identity for concatenation.
- $\emptyset L = L\emptyset = \emptyset$. $\emptyset$ is the annihilator for concatenation.   
- $L(M + N) = LM + LN$ (left distributive)
- $(M + N)L  = ML + NL$ (right distributive)
- $L + L = L$ (idempotence law)
- $(L^*)^* = L^*$.
- $\emptyset^* = \epsilon$
- $\epsilon^* = \epsilon$
- $L^{+} = LL^* = L^*L$.
- $L^* = L^{+} + \epsilon$
- $L? = \epsilon + L$

*** Discovering laws for regular expressions
The truth of a law reduces to the question of the equality of two
languages. We show set equivalence: a string in one language must be
in another, and vice-versa.

* Properties of Regular Languages

Regular languages exhibit the "closure" property. These properties let
us build recognizers for languages that are constructed from other
languages by certain operations. Regular languages also exhibit
"decision properties", which allow us to make decisions about whether
two automata define the same language. This means that we can always
minimize an automata to have as few states as possible for a
particular language.

** Pumping Lemma
We have established that the class of languages known as regular
languages are accepted by DFAs, NFAs and by $\epsilon$-NFAs.

However, not every language is a regular language. An easy way to see
this is that the number of languages is infinite, but DFAs have finite
number of states, and are finite.

The pumping lemma lets us show that certain languages are not regular.

#+ATTR_LATEX: :options [Pumping Lemma]
#+BEGIN_theorem
<<pumping-lemma>>
Let $L$ be a regular language. Then there exists a constant $n$ (which
depends on $L$) such that for every string $w$ in $L$ such that
$| w |  \ge n$, we can break $w$ into three strings $w = xyz$
such that:

1. $y \ne \epsilon$
2. $| xy | \le n$
3. For all $k \ge 0$, the string $xy^k z$ is also in $L$
#+END_theorem

That is, we can always find a non-empty string $y$ not too far from
the beginning of $w$ that can be "pumped". This means repeating $y$
any number of times, or deleting it, keeps the resulting string in the
language $L$.

** Closure of Regular Languages
If certain languages are regular, then languages formed from them by
certain operations are also regular. These are referred to as the
closure properties of regular languages. Below is a summary:

1. Union of 2 regular languages
2. Intersection of 2 regular languages
3. Complement of 2 regular languages
4. Difference of 2 regular languages
5. Reversal of a regular language
6. Closure (star) of a regular language
7. The concatenation of regular languages
8. A homomorphism (substitution of strings for symbols) of a regular language
9. The inverse homomorphism of a regular language

The above are all regular.

* Context-free Grammars and Languages
Context-free languages are a larger class of languages, that have
context-free grammars. We show how these grammars are defined, and how
they define languages.

Context-free grammars are recursive definitions. For example, the
context-free grammar for palindromes can be defined as:

1. $P \rightarrow \epsilon$
2. $P \rightarrow 0$
3. $P \rightarrow 1$
4. $P \rightarrow 0P0$
5. $P \rightarrow 1P1$

There are four important components in a grammatical description of a
language:

1. There is a finite set of symbols that form the strings of the
   language. These alphabets are called the /terminals/, or /terminal
   symbols/.
2. There is a finite set of variables, or /nonterminals/ or /syntactic
   categories/. Each variable represents a language. In the language
   above, the only variable is $P$.
3. One of the variables represents the language being defined, called
   the /start symbol/.
4. There is a finite set of productions or rules that represent the
   recursive definition of a language. Each production rule consists:
   1. A variable that is being defined by the production (called the /head/).
   2. The production symbol $\rightarrow$.
   3. A string of zero or more terminals and variables.

We can represent any CFG as these 4 components, we denote CFG $G = (V,
T, P, S)$.

** Derivations using a Grammar
We can apply the productions of a CFG to infer that certain strings
are in the language of a certain variable.

The process of deriving strings by applying productions from head to
body requires the definition of a new relation symbol, $\Rightarrow$. Suppose $G
= (V, T, P, S)$ is a CFG> Let $\alpha A \beta$ be a string of terminals and
variables, with $A$ a variable. That is $\alpha$ and $\beta$ are strings in $(V
\cup T)^*$, and $A$ is in $V$. Let $A \rightarrow \gamma$ be a production of $G$. We say
that $\alpha A \beta \Rightarrow_{G} \alpha \gamma B$. If $G$ is understood, we can omit the
subscript.

We may extend the $\Rightarrow$ relationship to represent zero, one or many
derivation steps, similar to the extended transition function.

** Leftmost and Rightmost Derivations
In order to restrict the number of choices we have in deriving a
string, it is often useful to require that at each step, we replace
the leftmost variable by one of its production bodies. Such a
derivation is called a /leftmost derivation/. Leftmost derivations are
indicated with $\Rightarrow_{lm}$ and $\Rightarrow_{lm}^*$.

Similarly, it is possible to require that at each step the rightmost
variable is replaced by one of its bodies. These are called /rightmost
derivations/. These are similarly denoted $\Rightarrow_{rm}$ and $\Rightarrow_{rm}^*$.

** The language of a Grammar
If $G = (V, T, P, S)$ is a CFG, then the language of $G$, denoted
$L(G)$ is the set of terminal strings that have derivations from the
start symbol:

\begin{equation}
  L(G) = \right\{w in T^* | S \Rightarrow_{G}^* w \left\}
\end{equation}

** Sentential Forms
Derivations from the start symbol produce strings that have a special
role. These are called /sentential forms/. We also denote
left-sentential and right-sentential forms for leftmost derivations
and rightmost derivations respectively.

** Parse Trees
There is a tree representation of derivations, that clearly show how
symbols of a terminal string are grouped into substrings, each of
which belongs to the language of one of the variables of the grammar.
This tree is the data structure of choice when representing the source
of a program.


*** Construction
The parse trees for a CFG $G$ are trees with the following conditions:

1. Each interior node is labeled by variable in $V$.
2. Each leaf is labeled by either a variable, a terminal, or $\epsilon$.
   However, if the leaf is labeled $\epsilon$, then it must be the only child
   of its parent.
3. If an interior node is labeled $A$, and its children are labeled
   $X_1, X_2, \dots, X_k$, respectively from the left, then $A \rightarrow X_1 X_2 \dots
   X_k$ is a production in $P$.

*** The yield
The yield of the tree is the concatenation of the leaves of the parse
tree from the left. This yield is a terminal string (all leaves are
labeled either with a terminal or with $\epsilon$). The root is labeled by
the start symbol.

*** Inferences and derivations
The following statements are equivalent:

1. The recursive inference procedure determines that terminal string
   $w$ is in the language of variable $A$.
2. $A \Rightarrow^* w$
3. $A \Rightarrow_{lm}^* w$
4. $A \Rightarrow_{rm}^* w$
5. There is a parse tree with root $A$ and yield $w$.

We can prove these equivalences using the following arcs:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-09-16 16:21:01
[[file:images/theory_of_computation/Context-free Grammars and Languages/screenshot_2018-09-16_16-21-01.png]]

** Linear Grammars
Right linear grammars have all the productions of form:

1. $A \rightarrow wB$ for $B \in V$ and $w \in T^*$
2. $A \rightarrow  w$, for $w \in T^*$

Every regular language can be generated by some right-linear grammar.
Suppose $L$ is accepted by DFA $A = (Q, \Sigma, \delta, q_0, F)$, Then, let $G =
(Q, \Sigma, P, q_0)$ where,

1. For $q, p \in Q$, $a \in \Sigma$, if $\delta(q, a) = p$, then we have a
   production in $P$ of the form $q \rightarrow ap$.
2. We also have productions $q \rightarrow \epsilon$ for each $q \in F$.

We can prove by induction on $|w|$ that $\hat{\delta}(q_0, w) = p$  iff $q_0
\Rightarrow^* wp$. This would give $\hat{\delta}(q_0, w) \in F$  iff $q_0 \Rightarrow^* w$.

** TODO Ambiguous Grammars

** Chomsky Normal Form
Chomsky normal form is useful in giving algorithms for working with
context-free grammars. A context-free grammar is in Chomsky normal
form if every rule is of the form:

1. $A \rightarrow BC$
2. $A \rightarrow a$

weher $a$ is any terminal and $A$, $B$, $C$ are any variables, except
$B$ and $C$ cannot be the start variable. $S \rightarrow \epsilon$ is also allowed.

Any context-free language is generated by a context-free grammar in
Chomsky normal form. This is because we can convert any grammar into
Chomsky normal form.

* Pushdown Automata
Pushdown automata are equivalent in power to context-free grammars
This equivalence is useful because it gives us two options for proving
that a language is context-free. Certain languages are more easily
described in terms of recognizers, while others aremore easily
described in terms of generators.

** Definition
It is in essence a nondeterministic finite automaton with
\epsilon-transitions permitted, with one additional capability: a stack on
which it can store a string of "stack symbols".

We can view the pushdown automaton informally as the device suggested
below:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-09-18 12:36:26
[[file:images/Pushdown Automata/screenshot_2018-09-18_12-36-26.png]]

A "finite-state-control" reads inputs, one symbol at a time. The
automaton is allowed to observe the symbol at the top of the stack,
and to base its transition on its current state. It :

1. Consumes from the input the symbol it uses in the transition. If \epsilon
   is used for the input, then no input symbol is consumed.
2. Goes to a new state
3. Replaces the symbol at the top of the stack by any string. This
   corresponds to \epsilon, which corresponds to a pop of the stack. It could
   also replace the top symbol by one other symbol. Finally the top
   stack symbol could be replaced by 2 or more symbols, which has the
   effect of changing the top stack symbol, and pushing one or more
   new symbols onto the stack.

** Formal Definition
We can specify a PDA $P$ as follows:

\begin{equation}
  P = (Q,\Sigma, \Gamma, \delta, q_0, Z_0, F)
\end{equation}

- $Q$ is the finite set of states
- $\Sigma$ is the finite set of input symbols
- $\Gamma$ is the finite stack alphabet
- $\delta$ is the transition function, taking a triple $\delta(q,a,X), where:
  - $q$ is a state in $Q$
  - $a$ is either an input symbol in $\Sigma$ or $\epsilon$.
  - $X$ is a stack symbol, that is a member of $\Gamma$
- $q_0$ the start state
- $Z_0$ the start symbol. Initially, the PDA's stack consists of this
  symbol, nothing else
- $F$ is the set of accepting states

The formal definition of a PDA contains no explicit mechanism to allow
the PDA to test for an empty stack. The PDA is able to get the same
effect by initially placing a special symbol $ on the stack. If it
sees the $ again, it knows that the stack effectively is empty.
** TODO Instantaneous Descriptions

** Equivalence of CFG and PDA
Let $A$ be a CFL. From the definition we know that $A$ has a CFG, $G$,
generating it. We show how to convert $G$ into an equivalent PDA.

The PDA $P$ we now describe will work by accepting its input $w$, if
$G$ generates that input, by determining whether there is a derivation
for $w$. Each step of the derivation yields an intermediate string of
variables and terminals. We design $P$ to determine whether some
series of substitutions of $G$ can lead from the start variable to
$w$.

The PDA $P$ begins by writing the start variable on its stack. It goes
through a series of intermediate strings, making one substitution
after another. Eventually it may arrive at a string that contains only
terminal symbols, meaning that it has used the grammar to derive a
string. Then $P$ accepts if this string is identical to the string it
has received as input.

1. Place the marker symbol $ and the start variable on the stack
2. Repeat:
   - If the top of stack is a variable symbol $A$, nondeterministically
     select one of the rules for $A$ and substitute $A$ by the string
     on the right-hand side of the rule
   - If the top of stack is a terminal symbol $a$, read the next
     symbol from the input and compare it to $a$. If they match,
     continue. Else, reject the branch of nondeterminism.
   - If the top of stack is the symbol $, enter the accept state.

Now we prove the reverse direction. We have a PDA $P$ and want to make
a CFG $G$ that generates all the strings that $P$ accepts.

For each pair of states $p$ and $q$, the grammar will have a variable
$A_{pq}$.

First, we simplify the task by modifying P slightly to give it the
following three features.

1. It has a single accept state, $q_{accept}$.
2. It empties its stack before accepting.
3. Each transition either pushes a symbol onto the stack (a push move)
   or pops one off the stack (a pop move), but it does not do both at
   the same time.

Giving $P$ features 1 and 2 is easy. To give it feature 3, we replace
each transition that simultaneously pops and pushes with a two
transition sequence that goes through a new state, and we replace each
transition that simultaneously pops and pushes with a two transition
sequence that goes through a new state, and we replace each transition
that neither pops nor pushes with a two transition sequence that
pushes then pops an arbitrary stack symbol.


To design $G$ so that $A_{pq}$ generates all strings that take $P$ from
$p$ to $q$, regardless of the stack contents at $p$, leaving the stack
at $q$ in the same condition as it was at $p$.

First, we simplify our task by modifying $P$ slightly to give it the
following three features.
