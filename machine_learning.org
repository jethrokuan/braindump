#+SETUPFILE: ./export_template.org
#+TITLE: Machine Learning

* Reference Book                                                   :noexport:
Learning From Data
[[file:~/Dropbox/Books/Programming/Machine%20Learning/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf][Pattern Recognition and Machine Learning]]
[[file:~/Dropbox/Books/Programming/Machine%20Learning/understanding-machine-learning-theory-algorithms.pdf][Understanding Machine Learning]]
* What is Learning?
An agent is said to be /learning/ if it improves its performance P on
task T based on experience/observations/data E. T must be fixed, P
must be measurable, E must exist. See [[file:artificial_intelligence.org::*Learning%20agents][Learning Agents]].
** Inductive Bias
The incorporation of prior knowledge biases the learning mechanism.
This is also called /inductive bias/. The incorporation of prior
knowledge is inevitable for the success of learning algorithms (the
no-free-lunch theorem). The stronger the prior knowledge that one
starts the learning process with, the easier it is to learn from
further examples.
* Types of Learning
** Active vs Passive Learning
An active learner interacts with the environment at training time,
(e.g. by posing queries or performing experiments), while a passive
learner only observes the information provided by the environment.
** Online vs Batch Learning
In online learning, the hypothesis has to be updated each time a new
label is received.
** Supervised vs Unsupervised Learning
In unsupervised learning, given a training set $S = \left(x_1, \hdots,
x_m\right)$, without a labeled output, one must construct a "good"
model/description of the data.

Example use cases include:
- clustering
- dimension reduction to ind essential parts of the data and reduce
  noise (e.g. PCA)
- minimises description length of data

** Reinforcement Learning
Each action in a staet has an associated cost and a probability
distribution of the next state.

Goal is to learn a policy (mapping from state to action) that
minimizes the sum of expected current and future costs.
** Supervised Learning
*** Measure of success
 A loss function helps measure our success. Given a set $H$ of
 hypothesis of models, and a domain $Z$, let $l$ be a function from $H
 \times Z$ to non-negative real numbers $l: $H \times Z \rightarrow
 \mathbb{R}_{+}$.

 The /risk function/ is the expected loss of the hypothesis,

 \begin{equation*}
   L_D(h) = E_{z \sim D}[l(h,z)]
 \end{equation*}

 We are interested in finding a hypothesis $h$ that has a small risk,
 or expected loss.
*** Empirical Risk Minimisation (ERM)
 - The training set error is often called the /empirical error/ or
   /empirical risk/.
 - Given a hypothesis class $H$, finding the hypothesis $h \in H$ that
   minimizes the empirical risk is a simple learning strategy.
*** Assumptions Made âš 
1. One common assumption is that the data in the data generation
   process is independently and identically distributed (IID),
   according to the distribution $D$.

Q: Given a large enough training set, do you expect the long term test
error to be similar to the training error?

- If IID, then yes
- If not, there is likely dependencies, but under certain conditions,
  yes.
  - If sampling mixes well, it will not take long for D' to look
    like a steady set distribution.
- If dependencies are exploited, there is a possibility of attaining
  lower training and test error.
* Concept Learning
A concept is a boolean-valued function over a set of input instances
(each comprising input attributes). Concept learning is a form of
supervised learning. Infer an unknown boolean-valued function from
training-examples.
** Hypothesis
There is a trade-off between /expressive power/ and smaller
/hypothesis space/. Large hypothesis spaces are bad, because search is
going to take a long time, and also requires more data. Humans exploit
structure in the hypothesis space to guide search and learn faster.

A hypothesis $h$ is consistent with a set of training examples $D$ iff
$h(x) = c(x)$ for all $<x,c(x)> \in D$.
** Inductive Learning
Any hypothesis found to approximate the target function well over a
sufficient large set of *training examples* will also approximate the
target function well over other *unobserved examples*.
** Concept Learning is Search
The goal is to search for a hypothesis $h \in H$ that is consistent
with $D$.
** Exploit Structure in Concept Learning
$h_j$ is more general than or equal to $h_k$ (denoted $h_j \ge_{g}
h_k$) iff any input instance $x$ that satisfies $h_j$ also satisfies
$h_k$.

This is relation is a *partial order*.

** Find-S Algorithm
Intuition: Start with the most specific hypothesis $h$. Whenever it
wrongly classifies a positive training example, we "minimally"
generalize it to satisfy its input instance.
*** Limitations
1. Can't tell whether Find-S has learnt the target concept
2. Can't tell when training examples are /inconsistent/
3. Picks a maximally specific $h$
4. Depending on $H$, there may be several solutions
** Version Space
\begin{equation*}
  VS_{H,D} = {h \in H | h \text{ is consistent with }D}
\end{equation*}
 
- If $c \in H$, then D can reduce $VS_{H,D}$ to ${c}$.
- If D is insufficient, then $VS_{H,D}$ represents the /uncertainty/
  of what the target concept is
- $VS_{H,D}$ contains all consistent hypotheses, including maximally
  specific hypotheses

The *general boundary* G of $VS_{H,D}$ is the set of maximally general
members of $H$ consistent with $D$.

The *specific boundary* S of $VS_{H,D}$ is the set of maximally general
members of $H$ consistent with $D$.

\begin{equation*}
  VS_{H,D} = {h \in H | \exists s \in S \exists g \in G g \ge_g h
    \ge_g s }
\end{equation*}

** List-Then-Eliminate Algorithm
Iterate through all hypotheses in $H$, and eliminate any hypothesis
found inconsistent with any training example. This algorithm is often
prohibitively expensive.

** Candidate-Elimination Algorithm
Start with most general and specific hypotheses. Each training example
"minimally" generalizes S and specializes G to remove inconsistent
hypotheses from version space.
* Decision Tree Learning
/Decision Tree Learning/ is a method of learning which approximates
discrete-valued functions that is robust to noisy data, and is capable
of learning disjunctive expressions

It is most appropriate when:
1. instances are represented as attribute pairs
2. the target function has discrete output values
3. Disjunctive descriptions may be required
4. The training data may contain errors
5. The training data may contain missing attribute values
** ID3 algorithm
ID3 learns decision trees by constructing them top down. Each instance
attribute is evaluated using a statistical test to determine how well
it alone classifies the examples. The best attribute is selected and
used as the test at the root node of the tree.
*** Which is the best attribute?
A statistical property called /information gain/ measures how well a
given attribute separates the training examples according to their
target classification.

Information gain is the expected reduction in entropy caused by
partitioning the examples according to this attribute:

\begin{align}
  Gain(S,A) = Entropy(S) - \sum_{v\in Values(A)}\frac{|S_v|}{|S|}Entropy(S_v)
\end{align}

For example:

\begin{align}
  Values(Wind) &= Weak, Strong \\
  S &= [9+, 5-] \\
  S_{Weak} &\leftarrow [6+, 2-] \\
  S_{Strong} &\leftarrow [3+, 3-] \\
  Gain(S, Wind) &= Entropy(S) - \frac{8}{14}Entropy(S_{Weak}) -
                  \frac{6}{14}Entropy(S_{Strong}) \\
               &=0.048
\end{align}
*** Hypothesis Space Search
ID3 can be characterised as searching a space of hypotheses for one
that fits the training examples. The hypothesis space searched is the
set of possible decision trees. ID3 performs a simple-to-complex,
hill-climbing search. The evaluation measure that guides the search is
the information gain measure.

Because ID3's hypothesis space of all decision trees is a complete
space of finite discrete-valued functions, it avoids the risk that the
hypothesis space might not contain the target function.

ID3 maintains only a single hypothesis as it searches through the
space of decision trees. ID3 loses the capabilities that follow from
explicitly representing all consistent hypothesis.

ID3 in its pure form performs no backtracking in its search, and can
result in locally but not globally optimal target functions.

ID3 uses all training examples at each step to make statistically
based decisions, unlike other algorithms that make decisions incrementally.
*** Inductive bias
The inductive bias of decision tree learning is that shorter trees are
preferred over larger trees (Occam's razor). Trees that place high
information gain attributes close to the root are preferred over those
that do not. ID3 can be viewed as a greedy heuristic search for the
shortest tree without conducting the entire breadth-first search
through the hypothesis space.

Notice that ID3 searches a complete hypothesis space incompletely, and
candidate-elimination searches an incomplete hypothesis space
completely. The inductive bias of ID3 follows from its search strategy
(/preference bias/), while that of candidate elimination follows from
the definition of its search space. (/restriction bias/).
*** Why Prefer Shorter Hypotheses?
1. fewer shorter hypothesis than larger ones, means it's less likely
   to over-generalise
* Density Estimation
/Density Estimation/ refers to the problem of modeling the probability
distribution $p(x)$ of a random variable $x$, given a finite set $x_1,
x_2, \hdots, x_n$ of observations.

We first look at parametric distributions, which are governed by a
small number of adaptive parameters. In a frequentist treatment, we
choose specific values for the parameters optimizing some criterion,
such as the likelihood function. In a Bayesian treatment, we
introduce prior distributions and then use Bayes' theorem to compute
the corresponding posterior distribution given the observed data.

An important role is played by /conjugate priors/, which yield
posterior distributions of the same functional form.

The maximum likelihood setting for parameters can give severely
over-fitted results for small data sets. To develop a Bayesian
treatment to this problem, we consider a form of prior distribution
with similar form as the maximum likelihood function. this property is
called /conjugacy/. For a binomial distribution, we can choose the
beta distribution as the prior.
* Refile
** Data Compression
In /lossy compression/, we seek to trade off code length with
reconstruction error.

In /vector quantization/, we seek a small set of vectors ${z_i}$ to
describe a large dataset of vectors ${x_i}$, such that we can
represent each $x-i$ with its closest approximation in ${z_i}$ with
small error. (Clustering problem)

In /transform coding/, we transform the data, usually using a linear
tranformation. The data in the transformed domain is quantized,
usually discarding the small coefficients, corresponding to removing
some of the dimensions.
