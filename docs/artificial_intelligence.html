<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-04-09 Mon 12:24 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Artificial Intelligence</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Jethro Kuan" />
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Artificial Intelligence</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org533ec48">1. What is Artificial Intelligence?</a>
<ul>
<li><a href="#orgff545a6">1.1. Acting Humanly: Turing Test</a></li>
<li><a href="#orgb881d00">1.2. Thinking Humanly</a></li>
<li><a href="#orgeda76f3">1.3. Thinking rationally</a></li>
<li><a href="#org98014e8">1.4. Acting Rationally</a></li>
</ul>
</li>
<li><a href="#orgf0d683d">2. Intelligent Agents</a>
<ul>
<li><a href="#orge8126a0">2.1. Rational Agents</a></li>
<li><a href="#orgedfdac7">2.2. Exploration vs Exploitation</a></li>
<li><a href="#org46d5020">2.3. Specifying Task Environment (PEAS)</a></li>
<li><a href="#orgc42c669">2.4. Properties of Task Environments</a></li>
<li><a href="#org1312789">2.5. Table-Driven Agent</a></li>
<li><a href="#org105b037">2.6. Reflex agents</a></li>
<li><a href="#org0808ddb">2.7. Model-based Reflex Agents</a></li>
<li><a href="#orgd47e249">2.8. Goal-based agents</a></li>
<li><a href="#orgda9b055">2.9. Utility-based agents</a></li>
<li><a href="#orgf2d2103">2.10. Learning agents</a></li>
<li><a href="#org76ab81e">2.11. State representations</a>
<ul>
<li><a href="#org2d9803d">2.11.1. Atomic Representation</a></li>
<li><a href="#org68d347c">2.11.2. Factored Representation</a></li>
<li><a href="#org1ee377e">2.11.3. Structured Representations</a></li>
<li><a href="#orga6daf98">2.11.4. Implications</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org9c2be81">3. Problem-Solving</a></li>
<li><a href="#orgf9c8135">4. Classical Search</a>
<ul>
<li><a href="#orgf79408e">4.1. How Search Algorithms Work</a></li>
<li><a href="#org5669856">4.2. Measuring Performance</a></li>
<li><a href="#orgef5a053">4.3. Uninformed Search Strategies</a>
<ul>
<li><a href="#org98adbfe">4.3.1. Breadth-first Search</a></li>
<li><a href="#org57e65cc">4.3.2. Uniform-cost Search</a></li>
<li><a href="#org39629c8">4.3.3. Depth-first Search</a></li>
<li><a href="#org8cbb4d2">4.3.4. Depth-limited Search</a></li>
<li><a href="#org2366c86">4.3.5. Iterative Deepening Depth-first Search</a></li>
<li><a href="#orgfd81acf">4.3.6. Bidirectional Search</a></li>
</ul>
</li>
<li><a href="#org000f0e3">4.4. Informed Search Strategies</a>
<ul>
<li><a href="#org809b0e3">4.4.1. Greedy best-first search</a></li>
<li><a href="#org4cad8a8">4.4.2. A* search</a></li>
</ul>
</li>
<li><a href="#org3efb2fa">4.5. Learning to Search Better</a></li>
</ul>
</li>
<li><a href="#org3074d03">5. Heuristic Functions</a>
<ul>
<li><a href="#orgea81dad">5.1. Generating Admissible Heuristics</a>
<ul>
<li><a href="#org44d7057">5.1.1. From Relaxed Problems</a></li>
<li><a href="#org36c4337">5.1.2. From Subproblems: Pattern Databases</a></li>
<li><a href="#orgd53c5a8">5.1.3. From Experience</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orga4fa459">6. Beyond Classical Search</a>
<ul>
<li><a href="#org1a4049c">6.1. Hill-climbing Search</a>
<ul>
<li><a href="#org74643a9">6.1.1. Variants</a></li>
</ul>
</li>
<li><a href="#org5f3ba02">6.2. Simulated Annealing</a></li>
<li><a href="#orga595642">6.3. Local Beam Search</a></li>
<li><a href="#orgee1a02f">6.4. Genetic Algorithms</a></li>
<li><a href="#orgd42836c">6.5. Local Search in Continuous Spaces</a></li>
<li><a href="#orgc7c1540">6.6. Searching with Non-deterministic Actions</a>
<ul>
<li><a href="#org11dd4f8">6.6.1. AND-OR search trees</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org3602928">7. Adversarial Search</a>
<ul>
<li><a href="#org2b86968">7.1. Optimal Strategy</a></li>
<li><a href="#org5fe5fac">7.2. Alpha-Beta Pruning</a></li>
</ul>
</li>
<li><a href="#orge07f974">8. RANDOM</a>
<ul>
<li><a href="#orgfd9eff2">8.1. Simon's Ant</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
-<b>- mode: Org; org-download-image-dir: "./images/ai/"; -</b>-
</p>
<div id="outline-container-org533ec48" class="outline-2">
<h2 id="org533ec48"><span class="section-number-2">1</span> What is Artificial Intelligence?</h2>
<div class="outline-text-2" id="text-1">
<p>
Designing agents that act rationally (e.g. through maximising a reward
function).
</p>

<p>
Humans often act in ways that do not maximise their own benefit
(irrational).
</p>
</div>
<div id="outline-container-orgff545a6" class="outline-3">
<h3 id="orgff545a6"><span class="section-number-3">1.1</span> Acting Humanly: Turing Test</h3>
<div class="outline-text-3" id="text-1-1">
<p>
A computer would require:
</p>

<ul class="org-ul">
<li>natural language processing</li>
<li>knowledge representation</li>
<li>automated reasoning</li>
<li>machine learning</li>
</ul>
</div>
</div>
<div id="outline-container-orgb881d00" class="outline-3">
<h3 id="orgb881d00"><span class="section-number-3">1.2</span> Thinking Humanly</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Cognitive science brings together computer models and experimental
techniques in psychology to construct testable and provable theories
of the human mind.
</p>
</div>
</div>
<div id="outline-container-orgeda76f3" class="outline-3">
<h3 id="orgeda76f3"><span class="section-number-3">1.3</span> Thinking rationally</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Taking informal knowledge and expressing it in logical terms.
</p>
</div>
</div>
<div id="outline-container-org98014e8" class="outline-3">
<h3 id="org98014e8"><span class="section-number-3">1.4</span> Acting Rationally</h3>
<div class="outline-text-3" id="text-1-4">
<p>
A rational agent is one that acts so as to achieve the best outcome
or,when there is uncertainty, the best expected outcome.
</p>

<p>
An agent is a function from percept histories to actions, i.e. \(f: P^*
\rightarrow A\). We seek the best-performing agent for a certain task;
must consider computation limits.
</p>
</div>
</div>
</div>
<div id="outline-container-orgf0d683d" class="outline-2">
<h2 id="orgf0d683d"><span class="section-number-2">2</span> Intelligent Agents</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li>Agents perceive the environment through sensors</li>
<li>Agents act upon the environment through actuators</li>
</ul>
</div>
<div id="outline-container-orge8126a0" class="outline-3">
<h3 id="orge8126a0"><span class="section-number-3">2.1</span> Rational Agents</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>For each possible percept sequence, select an action that is
expected to maximise its performance measure. The performance
measure is a function of a given sequence of environment states.</li>
<li>Given the evidence provided by the percept sequence and whatever
built-in knowledge the agent has.</li>
<li>Agents can perform actions that help them gather useful information
(exploration)</li>
<li>An agent is <i>autonomous</i> if its behaviour is determined by its own
experience (with ability to learn and adapt)</li>
</ul>
</div>
</div>
<div id="outline-container-orgedfdac7" class="outline-3">
<h3 id="orgedfdac7"><span class="section-number-3">2.2</span> Exploration vs Exploitation</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Doing actions that modify future percepts (information gathering) is
an important part of rationality. In most scenarios, agents don't know
the entire environment <i>a priori</i>.
</p>
</div>
</div>
<div id="outline-container-org46d5020" class="outline-3">
<h3 id="org46d5020"><span class="section-number-3">2.3</span> Specifying Task Environment (PEAS)</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>Performance measure</li>
<li>Environment</li>
<li>Actuators</li>
<li>Sensors</li>
</ul>
</div>
</div>
<div id="outline-container-orgc42c669" class="outline-3">
<h3 id="orgc42c669"><span class="section-number-3">2.4</span> Properties of Task Environments</h3>
<div class="outline-text-3" id="text-2-4">
<dl class="org-dl">
<dt>Fully observable</dt><dd>whether an agent's sensors gives it access to
the complete state of the environment at any given point in time</dd>
<dt>Deterministic</dt><dd>if the next state is completely determined by the
current environment. Otherwise it is <b>stochastic</b>.</dd>
<dt>Episodic</dt><dd>whether an agents experience is divided into atomic
episodes. In each episode, an agent receives a percept
and performs a single action. In <b>sequential</b>
environments short-term actions can have long-term
consequences. For this reason, episodic environments are
generally simpler.</dd>
<dt>Static</dt><dd>whether the environment can change while the agent is
deliberating.</dd>
<dt>Discrete</dt><dd>whether the state of the environment, how time is
handled, and the percepts and actions of the agent
discretely quantized.</dd>
<dt>Single agent</dt><dd>in some environments, for example chess, there are
multiple agents acting in the same environment.
<dl class="org-dl">
<dt>cooperative</dt><dd>if the two agents need to work together.</dd>
<dt>competitive</dt><dd>if the two agents are working against each other.</dd>
</dl></dd>
<dt>Known</dt><dd>whether the agent knows the outcome of its actions.</dd>
</dl>
</div>
</div>
<div id="outline-container-org1312789" class="outline-3">
<h3 id="org1312789"><span class="section-number-3">2.5</span> Table-Driven Agent</h3>
<div class="outline-text-3" id="text-2-5">
<p>
Simple to implement, and works. However, the number of table entries
is exponential in time: \(\text{#percepts}^\text{time}\). Hence it is
doomed to failure. The key challenge to AI is to produce rational
behaviour from a small program rather than a vast table.
</p>
</div>
</div>
<div id="outline-container-org105b037" class="outline-3">
<h3 id="org105b037"><span class="section-number-3">2.6</span> Reflex agents</h3>
<div class="outline-text-3" id="text-2-6">
<p>
A simple reflex agent is one that selects actions on the basis of the
<i>current</i> percept, ignoring the rest of the percept history. A
<i>condition-action</i> rule is triggered upon processing the current
percept. E.g. <b>if</b> the car in front is braking, <b>then</b> brake too.
</p>

<p>
Basing actions on only the current percept can be highly limiting, and
can also lead to infinite loops. Randomized actions of the right kind
can help escape these infinite loops.
</p>
</div>
</div>
<div id="outline-container-org0808ddb" class="outline-3">
<h3 id="org0808ddb"><span class="section-number-3">2.7</span> Model-based Reflex Agents</h3>
<div class="outline-text-3" id="text-2-7">
<p>
The agent maintains some <b>internal state</b> that depends on percept
history and reflects at least some of the unobserved aspects of the
current state. Information about how the world evolves independently
from the agent is encoded into the agent. This knowledge is called a
<b>model</b> of the world, and this agent is hence a <b>model-based</b> agent.
</p>
</div>
</div>
<div id="outline-container-orgd47e249" class="outline-3">
<h3 id="orgd47e249"><span class="section-number-3">2.8</span> Goal-based agents</h3>
<div class="outline-text-3" id="text-2-8">
<p>
Knowing about the current state of the environment may not be enough
to decide on what to do. Agents may need <b>goal</b> information that
describes situations that are desirable. Sometimes goal-based action
selection is straightforward, but in others <b>searching</b> and <b>planning</b>
are required to achieve the goal. Goal-based agents are flexible
because the knowledge that supports its decisions is represented
explicitly and can be modified, although it is less efficient.
</p>
</div>
</div>
<div id="outline-container-orgda9b055" class="outline-3">
<h3 id="orgda9b055"><span class="section-number-3">2.9</span> Utility-based agents</h3>
<div class="outline-text-3" id="text-2-9">
<p>
Goals provide a binary distinction between good and bad states. A more
general performance measure should allow a comparison between world
states according to exactly how good it is to the agent. An agent's
<b>utility function</b> is an internalisation of the performance measure.
An agent chooses actions to maximise its expected utility. A
utility-based agents has to model and keep track of its environment.
</p>
</div>
</div>
<div id="outline-container-orgf2d2103" class="outline-3">
<h3 id="orgf2d2103"><span class="section-number-3">2.10</span> Learning agents</h3>
<div class="outline-text-3" id="text-2-10">
<p>
A learning agent can be divided into four conceptual components.
</p>
<dl class="org-dl">
<dt>learning element</dt><dd>responsible for making improvements</dd>
<dt>performance element</dt><dd>responsible for selecting extrenal actions</dd>
<dt>problem generator</dt><dd>suggests actions that will lead to new and
informative experiences</dd>
</dl>

<p>
the learning element takes in feedback from the <b>critic</b> on how the
agent is doing and determines show the performance element should be
modified to do better in the future.
</p>
</div>
</div>
<div id="outline-container-org76ab81e" class="outline-3">
<h3 id="org76ab81e"><span class="section-number-3">2.11</span> State representations</h3>
<div class="outline-text-3" id="text-2-11">
</div>
<div id="outline-container-org2d9803d" class="outline-4">
<h4 id="org2d9803d"><span class="section-number-4">2.11.1</span> Atomic Representation</h4>
<div class="outline-text-4" id="text-2-11-1">
<p>
In an atomic representation each state of the world is indivisible,
and has no internal structure. Search, game-playing, hidden Markov
models and Markov decision processes all work with atomic
representations.
</p>
</div>
</div>
<div id="outline-container-org68d347c" class="outline-4">
<h4 id="org68d347c"><span class="section-number-4">2.11.2</span> Factored Representation</h4>
<div class="outline-text-4" id="text-2-11-2">
<p>
A factored representation splits up each state into a fixed set of
<b>variables</b> or <b>attributes</b>, each of which can have a <b>value</b>.
</p>

<p>
Constraint satisfaction algorithms, propositional logic, planning,
Bayesian networks and machine learning algorithms work with factored
representations.
</p>
</div>
</div>

<div id="outline-container-org1ee377e" class="outline-4">
<h4 id="org1ee377e"><span class="section-number-4">2.11.3</span> Structured Representations</h4>
<div class="outline-text-4" id="text-2-11-3">
<p>
Structured representations underlie relational databases and
first-order logic, first-order probability models, knowledge-based
learning and much of natural language understanding.
</p>
</div>
</div>
<div id="outline-container-orga6daf98" class="outline-4">
<h4 id="orga6daf98"><span class="section-number-4">2.11.4</span> Implications</h4>
<div class="outline-text-4" id="text-2-11-4">
<p>
A more expressive representation can capture, at least as concisely, a
everything a more expressive one can capture, plus more. On the other
hand, reasoning and learning become more complex as the expressive
power of the representation increases.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org9c2be81" class="outline-2">
<h2 id="org9c2be81"><span class="section-number-2">3</span> Problem-Solving</h2>
<div class="outline-text-2" id="text-3">
<p>
Problem-solving agents use <i>atomic</i> representations, as compared to
goal-based agents, which use more advanced factored or structured
representations.
</p>

<p>
The process of looking for a sequence of actions that reaches the goal
is called <i>search</i>. A search algorithm takes a problem as input and
returns a <i>solution</i> in the form of an action sequence.
</p>
</div>
</div>
<div id="outline-container-orgf9c8135" class="outline-2">
<h2 id="orgf9c8135"><span class="section-number-2">4</span> Classical Search</h2>
<div class="outline-text-2" id="text-4">
<p>
This addresses observable, deterministic, and known environments where
the solution is a sequence of actions.
</p>
</div>
<div id="outline-container-orgf79408e" class="outline-3">
<h3 id="orgf79408e"><span class="section-number-3">4.1</span> How Search Algorithms Work</h3>
<div class="outline-text-3" id="text-4-1">
<p>
Search algorithms consider various possible action sequences. The
possible action sequences start at the initial state form a <i>search
tree</i>.
</p>

<p>
Search algorithms require a data structure to keep track of the search
tree that is being constructed.
</p>

<dl class="org-dl">
<dt>state</dt><dd>state in the state space to which the node corresponds</dd>
<dt>parent</dt><dd>the node in the search tree that generated this node</dd>
<dt>action</dt><dd>the action that was applied to the parent to generate this node</dd>
<dt>path-cost</dt><dd>the cost, traditionally denoted by \(g(n)\), of the path
from the initial state to the node, as indicated by the
parent pointers</dd>
</dl>
</div>
</div>
<div id="outline-container-org5669856" class="outline-3">
<h3 id="org5669856"><span class="section-number-3">4.2</span> Measuring Performance</h3>
<div class="outline-text-3" id="text-4-2">
<dl class="org-dl">
<dt>completeness</dt><dd>is the algorithm guaranteed to find a solution if
it exists?</dd>
<dt>optimality</dt><dd>does the strategy find the optimal solution?</dd>
<dt>time complexity</dt><dd>how long does it take to find a solution?</dd>
<dt>space complexity</dt><dd>how much memory is required to do the search?</dd>
</dl>
</div>
</div>
<div id="outline-container-orgef5a053" class="outline-3">
<h3 id="orgef5a053"><span class="section-number-3">4.3</span> Uninformed Search Strategies</h3>
<div class="outline-text-3" id="text-4-3">
</div>
<div id="outline-container-org98adbfe" class="outline-4">
<h4 id="org98adbfe"><span class="section-number-4">4.3.1</span> Breadth-first Search</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
The root node is expanded first, then all the successors of the root
node are expanded next, then their successors, and so on.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">performance</th>
<th scope="col" class="org-left">rating</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">completeness</td>
<td class="org-left">YES</td>
</tr>

<tr>
<td class="org-left">optimal</td>
<td class="org-left">NO</td>
</tr>

<tr>
<td class="org-left">time complexity</td>
<td class="org-left">\(O(b^d)\)</td>
</tr>

<tr>
<td class="org-left">space complexity</td>
<td class="org-left">\(O(b^d)\)</td>
</tr>
</tbody>
</table>

<p>
The shallowest node may not be the most optimal node.
</p>

<p>
The space used in the <i>explored set</i> is \(O(b^{d-1})\) and the space
used in the <i>frontier</i> is \(O(b^d)\).
</p>

<p>
In general, exponential-complexity search problems cannot be solved by
uninformed methods for any but the smallest instances.
</p>
</div>
</div>
<div id="outline-container-org57e65cc" class="outline-4">
<h4 id="org57e65cc"><span class="section-number-4">4.3.2</span> Uniform-cost Search</h4>
<div class="outline-text-4" id="text-4-3-2">
<p>
Uniform-cost search expands the node \(n\) with the lowest path
cost \(g(n)\). The goal test is applied to a node when it is selected
for expansion rather than when it is first generated.
</p>

<p>
This is equivalent to BFS if all step costs are qual.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">performance</th>
<th scope="col" class="org-left">rating</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">completeness</td>
<td class="org-left">MAYBE</td>
</tr>

<tr>
<td class="org-left">optimal</td>
<td class="org-left">YES</td>
</tr>

<tr>
<td class="org-left">time</td>
<td class="org-left">\(O(b^{1+\lfloor{\frac{C^*}{\epsilon}}\rfloor})\), where \(C^*\) is the optimal cost.</td>
</tr>

<tr>
<td class="org-left">space</td>
<td class="org-left">\(O(b^{1+\lfloor{\frac{C^*}{\epsilon}}\rfloor})\)</td>
</tr>
</tbody>
</table>

<p>
Completeness is guaranteed only if the cost of every step exceeds some
small positive constant \(\epsilon\). an infinite loop may occur if
there is a path with an infinite sequence of zero-cost actions.
</p>
</div>
</div>
<div id="outline-container-org39629c8" class="outline-4">
<h4 id="org39629c8"><span class="section-number-4">4.3.3</span> Depth-first Search</h4>
<div class="outline-text-4" id="text-4-3-3">
<p>
Always expands the <i>deepest</i> node in the current frontier of the
search tree.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">performance</th>
<th scope="col" class="org-left">rating</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">completeness</td>
<td class="org-left">YES</td>
</tr>

<tr>
<td class="org-left">optimal</td>
<td class="org-left">NO</td>
</tr>

<tr>
<td class="org-left">time complexity</td>
<td class="org-left">\(O(b^m)\)</td>
</tr>

<tr>
<td class="org-left">space complexity</td>
<td class="org-left">\(O(b^m)\), \(O(m)\) if backtrack</td>
</tr>
</tbody>
</table>

<p>
The time complexity of DFS may be worse than BFS: \(O(b^m)\) might be
larger than \(O(b^d)\).
</p>

<p>
DFS only requires storage of \(O(bm)\) nodes, where \(m\) is the maximum
depth of any node. <b>backtracking search</b> only generates one successor
at a time, modifying the current state description rather than copying
it. Memory requirements reduce to one state description and \(O(m)\)
actions.
</p>
</div>
</div>
<div id="outline-container-org8cbb4d2" class="outline-4">
<h4 id="org8cbb4d2"><span class="section-number-4">4.3.4</span> Depth-limited Search</h4>
<div class="outline-text-4" id="text-4-3-4">
<p>
In depth-limited search, nodes at depth of pre-determined limit \(l\)
are treated as if they had no successors. This limit solves the
infinite-path problem.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">performance</th>
<th scope="col" class="org-left">rating</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">completeness</td>
<td class="org-left">YES</td>
</tr>

<tr>
<td class="org-left">optimal</td>
<td class="org-left">NO</td>
</tr>

<tr>
<td class="org-left">time complexity</td>
<td class="org-left">\(O(b^l)\)</td>
</tr>

<tr>
<td class="org-left">space complexity</td>
<td class="org-left">\(O(b^l)\), \(O(l)\) if backtrack</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org2366c86" class="outline-4">
<h4 id="org2366c86"><span class="section-number-4">4.3.5</span> Iterative Deepening Depth-first Search</h4>
<div class="outline-text-4" id="text-4-3-5">
<p>
Key idea is to gradually increase the depth limit: first 0, then
1, then 2&#x2026; until a goal is found.
</p>



<div class="figure">
<p><img src="images/ai/Problem-Solving/screenshot_2018-01-22_15-26-50.png" alt="screenshot_2018-01-22_15-26-50.png" />
</p>
</div>


<p>
\(N(IDS) = (d)b + (d-1)b^2 + \hdots + (1)b^d\), which gives a time
complexity of \(O(b^d)\)
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">performance</th>
<th scope="col" class="org-left">rating</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">completeness</td>
<td class="org-left">YES</td>
</tr>

<tr>
<td class="org-left">optimal</td>
<td class="org-left">NO (unless step cost is 1)</td>
</tr>

<tr>
<td class="org-left">time complexity</td>
<td class="org-left">\(O(b^d)\)</td>
</tr>

<tr>
<td class="org-left">space complexity</td>
<td class="org-left">\(O(b^d)\), \(O(m)\) if backtrack</td>
</tr>
</tbody>
</table>


<ol class="org-ol">
<li>BFS and IDS are complete if \(b\) is finite.</li>
<li>UCS is complete if \(b\) is finite and step cost is \(\ge \epsilon\).</li>
<li>BFS and IDS are optimal if all step costs are identical.</li>
</ol>
</div>
</div>
<div id="outline-container-orgfd81acf" class="outline-4">
<h4 id="orgfd81acf"><span class="section-number-4">4.3.6</span> Bidirectional Search</h4>
<div class="outline-text-4" id="text-4-3-6">
<p>
Conduct two simultaneous searches &#x2013; one forward from the initial
state, and the other backward from the goal. This is implemented by
replacing the goal test with a check to see whether the frontiers of
two searches intersect. This reduces the time ad space complexity to \(O(b^{d/2})\).
</p>
</div>
</div>
</div>
<div id="outline-container-org000f0e3" class="outline-3">
<h3 id="org000f0e3"><span class="section-number-3">4.4</span> Informed Search Strategies</h3>
<div class="outline-text-3" id="text-4-4">
</div>
<div id="outline-container-org809b0e3" class="outline-4">
<h4 id="org809b0e3"><span class="section-number-4">4.4.1</span> Greedy best-first search</h4>
<div class="outline-text-4" id="text-4-4-1">
<p>
<i>Greedy best-first search</i> tries to expand the node that is closest to
the goal, on the grounds that this is likely to lead to a solution
quickly. It evaluates nodes by using just the heuristic function:
\(f(n) = h(n)\).
</p>

<p>
Greedy best-first tree search is incomplete even in a finite state
space. The graph search version is complete in finite spaces, but not
in infinite ones. The worst case time and space complexity is
\(O(b^m)\). However, with a good heuristic function, the complexity can
be reduced substantially.
</p>
</div>
</div>
<div id="outline-container-org4cad8a8" class="outline-4">
<h4 id="org4cad8a8"><span class="section-number-4">4.4.2</span> A* search</h4>
<div class="outline-text-4" id="text-4-4-2">
<p>
It evaluates nodes by combining \(g(n)\) the cost to reach the node, and
\(h(n)\) the cost to get to the goal: \(f(n) = g(n) + h(n)\). Since \(g(n)\)
gives the path cost from the start node to node \(n\), and \(h(n)\) is the
estimated cost of the cheapest path from \(n\) to the goal,$f(n) = $
estimated cost of the cheapest solution through \(n\).
</p>

<p>
\(h(n)\) is an <i>admissible heuristic</i> iff it never overestimates the
cost to reach the goal. For A*, this means that \(f(n)\) would never
overestimate the cost of a solution along the current path.
</p>

<p>
Admissible heuristics are by nature optimistic because they think the
cost of solving the problem is less than it actually is.
</p>

<p>
A second, slightly stronger condition is called <i>consistency</i>, and is
required only for applications of A* to graph search. A heuristic
\(h(n)\) is <i>consistent</i> iff for every node \(n\) and every successor \(n'\)
of \(n\) generated by any action \(a\), the estimated cost of reaching the
goal from \(n\) is no greater than the step cost of getting to \(n'\) plus
the estimated cost of reaching the goal from \(n'\): \(h(n) \le
c(n,a,n') + h(n')\). This is a form of the general triangle inequality.
</p>

<p>
A* search is complete, optimal and optimally efficient with a
consistent heuristic. The latter means that no other optimal algorithm
is guaranteed to expand fewer nodes than A*.
</p>

<p>
However, for most problems, the number of states within the goal
contour search space is still exponential in the length of the
solution. 
</p>

<p>
The <i>absolute error</i> of a heuristic is defined as \(\Delta = h^*-h\),
and the <i>relative error</i> is defined as \(\epsilon = \frac{h^*-h}{h*}\).
The complexity results depend strongly on the assumptions made about
the state space. For constant step costs, it is \(O(b^{\epsilon d})\),
and the effective branching factor is \(b^\epsilon\).
</p>

<p>
A* keeps all generated nodes in memory, and hence it usually runs out
of space  long before it runs of time. Hence, it is not practical for
large-scale problems.
</p>

<p>
Other memory-bounded heuristic searches include:
</p>
<ul class="org-ul">
<li>iterative-deepening A* (IDA*)</li>
<li>Recursive best-first search (RBFS)</li>
<li>Memory-bounded A* (MA*)</li>
<li>simplified MA* (SMA*)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org3efb2fa" class="outline-3">
<h3 id="org3efb2fa"><span class="section-number-3">4.5</span> Learning to Search Better</h3>
<div class="outline-text-3" id="text-4-5">
<p>
Each state in a <i>metalevel state space</i> captures the internal
computational state of a program that is searching in an <i>object-level
state space</i>. A <i>metalevel learning</i> algorithm can learn from
experiences to avoid exploring unpromising subtrees. The goal of the
learning is to minimise the total cost of problem solving, trading off
computational expense and path cost.
</p>
</div>
</div>
</div>

<div id="outline-container-org3074d03" class="outline-2">
<h2 id="org3074d03"><span class="section-number-2">5</span> Heuristic Functions</h2>
<div class="outline-text-2" id="text-5">
<p>
If for any node n \(h_2(n) \ge h_1(n)\), we say that \(h_2\) <i>dominates</i>
\(h_1\). Domination translates directly into efficiency: A* using \(h_2\)
will never expand more nodes than \(h_1\). Hence it is generally better
to use a heuristic function with higher value, while making sure it is
consistent, and computing the heuristic function is computationally
feasible.
</p>
</div>

<div id="outline-container-orgea81dad" class="outline-3">
<h3 id="orgea81dad"><span class="section-number-3">5.1</span> Generating Admissible Heuristics</h3>
<div class="outline-text-3" id="text-5-1">
</div>
<div id="outline-container-org44d7057" class="outline-4">
<h4 id="org44d7057"><span class="section-number-4">5.1.1</span> From Relaxed Problems</h4>
<div class="outline-text-4" id="text-5-1-1">
<p>
Because the relaxed problem adds edges to the state space, any optimal
solution in the original problem is, by definition, also a solution in
the relaxed problem. Hence, the cost of an optimal solution to a
relaxed problem is an admissible heuristic for the original problem.
Because the derived heuristic is an exact cost for the relaxed
problem, it must obey the triangle inequality and is therefore
consistent.
</p>
</div>
</div>

<div id="outline-container-org36c4337" class="outline-4">
<h4 id="org36c4337"><span class="section-number-4">5.1.2</span> From Subproblems: Pattern Databases</h4>
<div class="outline-text-4" id="text-5-1-2">
<p>
<i>Pattern Databases</i> store exact solution costs for every possible
subproblem instance. In the case of the 8-puzzle, every possible
configuration of the four tiles and the blank. Each pattern database
yields an admissible heuristic, and these heuristics can be combined
by taking the maximum value. Solutions to subproblems can overlap:
<i>disjoint pattern databases</i> account for this. These work by dividing
the problem in a way that each move affects only one subproblem.
</p>
</div>
</div>

<div id="outline-container-orgd53c5a8" class="outline-4">
<h4 id="orgd53c5a8"><span class="section-number-4">5.1.3</span> From Experience</h4>
<div class="outline-text-4" id="text-5-1-3">
<p>
Inductive learning methods work best when supplied with <i>features</i> of
a state that are relevant to predicting the state's value. A common
approach to combining features would be through a linear combination:
\(h(n) = c_1x_1(n) + c_2x_2(n)\).
</p>

<p>
These heuristics satisfy the requirement that \(h(n) = 0\) for goal
states, but are not necessarily admissible or consistent.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orga4fa459" class="outline-2">
<h2 id="orga4fa459"><span class="section-number-2">6</span> Beyond Classical Search</h2>
<div class="outline-text-2" id="text-6">
<p>
Here, we cover algorithms that perform purely <i>local search</i> in the
state space, evaluating and modifying one or more current states
rather than systematically exploring paths from an initial state.
These include methods inspired by statistical physics (simulated
annealing) and evolutionary biology (genetic algorithms).
</p>

<p>
If an agent cannot predict exactly what percept it will receive, then
it will need to consider what to do under each <i>contingency</i> that its
percepts may reveal.
</p>

<p>
If the path to the goal doesn't matter, we giht consider a different
class of algorithms, ones that do not worry about the paths at all.
<i>Local search</i> algorithms operate using a single <i>current node</i> and
generally move only to neighbours of that node. Its advantages
include:
</p>

<ol class="org-ol">
<li>They generally use a <span class="underline">constant amount of memory</span></li>
<li>They can often find <span class="underline">reasonable solutions in large or infinite
state spaces</span> where systematic algorithms are not suitable.</li>
</ol>
</div>

<div id="outline-container-org1a4049c" class="outline-3">
<h3 id="org1a4049c"><span class="section-number-3">6.1</span> Hill-climbing Search</h3>
<div class="outline-text-3" id="text-6-1">
<p>
The hill-climbing search is a loop that continually moves in the
direction of increasing value. 
</p>

<p>
Consider the 8-queens problem.
</p>

<p>
Local search algorithms typically use a <i>complete-state formation</i>. The
successors of a state are all possible states generated by moving a
single queen to another square in the same column.
</p>

<p>
We could use a heuristic cost function \(h\) equal to the number of
queens that are attacking each other, either directly or indirectly.
</p>

<p>
The global minimum of this function is zero, which only occurs for
perfect solutions. Hill-climbing algorithms typically choose randomly
among the set of best successors having the lowest \(h\).
</p>

<p>
Hill-climbing algorithms can get stuck for the following reasons:
</p>

<ul class="org-ul">
<li>local maxima</li>
<li id="ridges">sequence of local maxima</li>
<li id="plateaux">flat local maximum, or <i>shoulder</i>, from which progress
is possible.</li>
</ul>
</div>


<div id="outline-container-org74643a9" class="outline-4">
<h4 id="org74643a9"><span class="section-number-4">6.1.1</span> Variants</h4>
<div class="outline-text-4" id="text-6-1-1">
<dl class="org-dl">
<dt>stochastic hill-climbing</dt><dd>chooses at random from among the uphill
moves; the probability of selection can vary with the steepness
of the uphill move. Usually converges more slowly, but finds
better solutions.</dd>
<dt>first-choice hill-climbing</dt><dd>stochastic hill-climbing with randomly
generated successors until one is generated that is better than
the current state. Good when state has many successors.</dd>
<dt>random-restart hill-climbing</dt><dd>conducts hill-climbing searches from
randomly generated initial states, until a goal is found.
Trivially complete with probability approaching 1.</dd>
</dl>
</div>
</div>
</div>

<div id="outline-container-org5f3ba02" class="outline-3">
<h3 id="org5f3ba02"><span class="section-number-3">6.2</span> Simulated Annealing</h3>
<div class="outline-text-3" id="text-6-2">
<p>
A hill-climbing algorithm that never makes 'downhill' moves towards
states with lower-value is guaranteed to be incomplete, because it can
be stuck on a local maximum.
</p>

<div class="org-src-container">
<pre class="src src-text">function SIMULATED-ANNEALING(problem, schedule)
  inputs: problem, a problem
          schedule, a mapping from time to 'temperature'

  current ← MAKE-NODE(problem, INITIAL-STATE)
  for t = 1 to ∞ do
    T ← schedule(t)
    if T = 0 then return current
    next ← a randomly selected successor of current
    𝞓E ← next.VALUE - current.VALUE
    if 𝞓E &gt; 0 then current ← next
    else current ← next only with probability e^(𝞓E/T)
</pre>
</div>
</div>
</div>

<div id="outline-container-orga595642" class="outline-3">
<h3 id="orga595642"><span class="section-number-3">6.3</span> Local Beam Search</h3>
<div class="outline-text-3" id="text-6-3">
<p>
Local beam search keeps track of \(n\) states rather than just one. It
begins with \(n\) randomly generated states, at each step all the
successors of all states are generated. If any one is a goal, the
algorithm halts. 
</p>

<p>
Local-beam search passes useful information between the parallel
search threads (compared to running random-restart \(n\) times), quickly
abandoning unfruitful searches and moves its resources to where the
most progress is being made.
</p>

<p>
/Stochastic local beam search chooses \(n\) successors at random, with
the probability of choosing a given successor being an increasing
function of its value/.
</p>
</div>
</div>


<div id="outline-container-orgee1a02f" class="outline-3">
<h3 id="orgee1a02f"><span class="section-number-3">6.4</span> Genetic Algorithms</h3>
<div class="outline-text-3" id="text-6-4">
<p>
A <i>genetic algorithm</i> is a variant of stochastic beam search in which
successor states are generated by combining two parent states rather
than by modifying a single state.
</p>

<p>
GA begins with a set of \(n\) randomly generated states, called the
<i>population</i>. Each state is also called an <i>individual</i>.
</p>

<p>
The production of the next generation of states is rated by the
objective function, or <i>fitness function</i>. A fitness function returns
higher values for better states.
</p>

<p>
Like stochastic beam search, genetic algorithms combine an uphill
tendency with random exploration and exchange of information among
parallel search threads. <i>crossover</i> in genetic algorithms raises the
level of granularity at which the search operates.
</p>

<div class="org-src-container">
<pre class="src src-text">function GENETIC-ALGORITHM(population, FITNESS-FN) returns an individual
  inputs: population, a set of individuals
          FITNESS-FN, a function that measures the fitness of an
  individual

  repeat
    new_population ← empty set
    for i = 1 to SIZE(population) do
      x ← RANDOM-SELECTION(population, FITNESS-FN)
      y ← RANDOM-SELECTION(population, FITNESS-FN)
      child ← REPRODUCE(x,y)
      if (small random probability) then child ← MUTATE(child)
      add child to new_population
    population ← new_population
  until some individual is fit enough, or enough has elapsed
  return the best individual in population, according to FITNESS-FN

function REPRODUCE(x,y) returns an individual
  inputs: x,y, parent individuals

  n ← LENGTH(x); c ← random(1,n)
  return APPEND(SUBSTRING(x,1,c), SUBSTRING(y, c+1, n))
</pre>
</div>
</div>
</div>

<div id="outline-container-orgd42836c" class="outline-3">
<h3 id="orgd42836c"><span class="section-number-3">6.5</span> Local Search in Continuous Spaces</h3>
<div class="outline-text-3" id="text-6-5">
<p>
One way to avoid continuous problems is simply to <i>discretize</i> the
neighbourhood of each state. Many methods attempt to use the
<i>gradient</i> of the landscape to find a maximum: \(x \leftarrow x +
\delta \nabla (x)\), where $&delta; is a small constant called the <i>step
size</i>. For many problems, the <i>Newton-Raphson</i> method is effective. It
solves the roots for equations \(g(x) = 0\), by computing a new
estimate: \(x \leftarrow x - g'(x)/g(x)\). To find a maximum or minimum
of \(f\), we need to find \(x\) such that the gradient is zero. In this
case \(g(\mathbf{x})\) in Newton's formula becomes \(\nabla
f(\mathbf{x})\) and the update equation can be written in matrix-vector
form as:
</p>

\begin{align*}
\mathbf{x} \leftarrow \mathbf{x} - H_f^{-1}(\mathbf{x})\nabla f(\mathbf{x})
\end{align*}

<p>
where \(H_f\) is the <i>Hessian</i> matrix of second derivatives. For
high-dimensional problems, computing the \(n^2\) entries of the Hessian
and inverting it may be expensive, and approximate versions have been
developed.
</p>

<p>
Local search methods suffer from local maxima, ridges and plateaux
in continuous spaces just as much as in discrete spaces.
</p>
</div>
</div>

<div id="outline-container-orgc7c1540" class="outline-3">
<h3 id="orgc7c1540"><span class="section-number-3">6.6</span> Searching with Non-deterministic Actions</h3>
<div class="outline-text-3" id="text-6-6">
<p>
When the environment is either partially observable or
non-deterministic, percepts become useful. In a partially observable
environment, every percept helps narrow down the set of possible
states the agent might be in. In a non-deterministic environment,
percepts tell the agent which of the possible outcomes of its actions
has actually occurred. Future percepts cannot be determined in
advance, and the agent's future actions will depend on those future
percepts. The solution to a problem is not a sequence but a
<i>contingency plan</i>
</p>

<p>
The solutions for no-deterministic problems can contain nested
if-then-else statements, meaning they are trees and not sequences.
</p>
</div>

<div id="outline-container-org11dd4f8" class="outline-4">
<h4 id="org11dd4f8"><span class="section-number-4">6.6.1</span> AND-OR search trees</h4>
<div class="outline-text-4" id="text-6-6-1">
<p>
A solution for an AND-OR search problem is a subtree that:
</p>

<ol class="org-ol">
<li>includes every outcome branch leaf</li>
<li>specifies one action at each of its OR nodes</li>
<li>includes every outcome branch at each of its AND nodes</li>
</ol>

<div class="org-src-container">
<pre class="src src-text">function AND-OR-GRAPH-SEARCH(problem) returns a conditional plan, or failure
  OR-SEARCH(problem, INITIAL-STATE, problem, [])

function OR-SEARCH(state,problem,path) returns a conditional plan, or failure
  if problem, GOAL-TEST(state) then return the empty plan
  if state is on path then return failure
  for each action in problem, ACTIONS(state) do
    plan ← AND-SEARCH(RESULTS(state,action), problem, [state | path])
    if plan ≠ failure then return [action | plan]
  return failure

function AND-SEARCH(states,problem,path) returns a conditional plan, or failure
  for each s_i in states do
    plan_i ← OR-SEARCH(s_i,problem,path)
    if plan_i = failure then return failure
  return [if s_1 then plan_1 else if s_2 then plan_2 ...]
</pre>
</div>

<p>
(stop at AIMA 4.4)
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org3602928" class="outline-2">
<h2 id="org3602928"><span class="section-number-2">7</span> Adversarial Search</h2>
<div class="outline-text-2" id="text-7">
<p>
Competitive environments, in which the agent's goals are in conflict,
give rise to <i>adversarial search</i> problems.
</p>

<p>
Game theory views any multi-agent environment as a game, provied that
the impact of each agent on the others is significant.
</p>

<p>
Games often have large branching factors, and require making some
decision even before computing the optimal decision.
</p>

<p>
<i>Pruning</i> allows us to ignore portions of the search tree that make no
difference to the final choice, and heuristic evaluation functions
allow us to approximate the true utility of a state without doing a
complete search.
</p>

<p>
A game can be formally defined as a search problem with the following
elements:
</p>

<dl class="org-dl">
<dt>\(S_0\)</dt><dd>the <i>initial state</i>, which specifies how the game is set up
at the start</dd>
<dt>\(Player(s)\)</dt><dd>Defines which player has the move in a state</dd>
<dt>\(Actions(s)\)</dt><dd>Returns the set of legal moves in a state</dd>
<dt>\(Result(s,a)\)</dt><dd>The transition model, which defines the result of a move</dd>
<dt>\(TerminalTest(s)\)</dt><dd>Terminal test, which is true when the game is
over, and false otherwise.</dd>
<dt>\(Utility(s,p)\)</dt><dd>A utility function defines the numeric value for a
game that ends in terminal state \(s\) for a player
\(p\).</dd>
</dl>


<p>
The initial state, \(Actions\) function and \(Result\) function define the
game tree for the game.
</p>
</div>


<div id="outline-container-org2b86968" class="outline-3">
<h3 id="org2b86968"><span class="section-number-3">7.1</span> Optimal Strategy</h3>
<div class="outline-text-3" id="text-7-1">
<p>
The optimal strategy can be determined from the <i>minimax value</i> of
each node (\(Minimax(n)\)). The minimax value of a node is the utility
of being in the corresponding state, assuming that players play
optimally from there to the nd of the game. The minimax value of a
terminal state is its utility.
</p>

\begin{align}
  Minimax(s) =
  \begin{cases}
    Utility(s), \text{ if } TerminalTest(s) \\
    max_{a \in Actions(s)}MINIMAX(Result(s,a)), \text{if Player(s) =
      Max} \\
    min_{a \in Actions(s)}MINIMAX(Result(s,a)), \text{if Player(s) = Min}
  \end{cases}
\end{align}

<p>
Minimax uses utility function on leaf nodes, backing up through the
tree, setting the node value to be the minimum of the children.
</p>
</div>
</div>


<div id="outline-container-org5fe5fac" class="outline-3">
<h3 id="org5fe5fac"><span class="section-number-3">7.2</span> Alpha-Beta Pruning</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Eliminate parts of the search tree that do not affect decision. 
</p>
</div>
</div>
</div>

<div id="outline-container-orge07f974" class="outline-2">
<h2 id="orge07f974"><span class="section-number-2">8</span> RANDOM</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-orgfd9eff2" class="outline-3">
<h3 id="orgfd9eff2"><span class="section-number-3">8.1</span> Simon's Ant</h3>
<div class="outline-text-3" id="text-8-1">
<p>
Simon, noble prize in economics
</p>

<blockquote>
<p>
The complexity of the behaviour is the manifestation of the complexity
of the environment and not the complexity of the program.
</p>
</blockquote>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Jethro Kuan</p>
<p class="date">Created: 2018-04-09 Mon 12:24</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
