<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-09-01 Sat 17:07 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Artificial Intelligence</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Jethro Kuan" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Artificial Intelligence</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org10f1572">1. Decision Networks</a></li>
<li><a href="#orgb473de0">2. </a></li>
<li><a href="#orgd7c4b89">3. Information Value Theory</a>
<ul>
<li><a href="#org93ada95">3.1. Properties of the value of information</a></li>
<li><a href="#org1145a92">3.2. Information Gathering Agents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org10f1572" class="outline-2">
<h2 id="org10f1572"><span class="section-number-2">1</span> Decision Networks</h2>
<div class="outline-text-2" id="text-1">
<p>
Decision networks extend Bayesian networks with nodes for actions and
utilities. An example of a decision network is below:
</p>


<div class="figure">
<p><img src="images/artificial_intelligence/Decision Networks/diagnosisDN_2018-09-01_16-50-40.png" alt="diagnosisDN_2018-09-01_16-50-40.png" />
</p>
</div>

<p>
The notation is as such:
</p>

<dl class="org-dl">
<dt>chance nodes (oval)</dt><dd>these represent random variables</dd>
<dt>decision nodes (rectangles)</dt><dd>these represent points where the
decision maker has a choice of actions.</dd>
<dt>utility nodes (diamonds)</dt><dd>represent the agent's utility function</dd>
</dl>

<p>
A simplified form is used in many cases. The notation remains
identical, but the chance nodes describing the outcome state are
omitted. Rather than representing a utility function on outcome states,
the utility node represents the expected utility associated with each
action. This node is associated with an action-utility function (also
known as Q-function). 
</p>
</div>
</div>
<div id="outline-container-orgb473de0" class="outline-2">
<h2 id="orgb473de0"><span class="section-number-2">2</span> </h2>
</div>
<div id="outline-container-orgd7c4b89" class="outline-2">
<h2 id="orgd7c4b89"><span class="section-number-2">3</span> Information Value Theory</h2>
<div class="outline-text-2" id="text-3">
<p>
The value of a given piece of information is defined to be the
difference in expected value between the best action before and after
information is obtained.  This is one of the most important parts of
decision-making: knowing what information to obtain.  Information has
value to the extent that it is likely to cause a change of plan and to
the extent that the new plan is significantly better than the old
plan.
</p>

<p>
Mathematically, the value of perfect information (VPI) is defined as:
</p>

\begin{equation}
VPI_e(E_j) = \left( \sum_k P(E_j = e_{jk} | \mathbb{e})
  MEU(\alpha_{e_{jk}} | \mathbb{e}, E_j = e_{jk}) \right) - MEU(\alpha |
\mathbb{e})
\end{equation}

<p>
This is obtained by considering the best action (maximum expected
utility) before and after obtaining the information, and averaging it
across all possible values for the new information, using our current
beliefs of its value.
</p>
</div>

<div id="outline-container-org93ada95" class="outline-3">
<h3 id="org93ada95"><span class="section-number-3">3.1</span> Properties of the value of information</h3>
<div class="outline-text-3" id="text-3-1">
<p>
First, the expected value of information is non-negative. 
</p>

\begin{equation}
  \forall \mathbb{e}, E_j VPI_{\mathbb{e}} (E_j) \ge 0
\end{equation}

<p>
The theorem is about the expected value, and not the real value. This
means that information can sometimes lead to a plan that is harmful.
</p>

<p>
It is important to note that VPI is dependent on the current state of
information. VPI is not additive in general:
</p>

\begin{equation}
  VPI_{\mathbb{e}}(E_j, E_k) \ne VPI_{\mathbb{e}}(E_j) + VPI_{\mathbb{e}}(E_k)
\end{equation}

<p>
VPI is order independent. That is:
</p>

\begin{equation}
  VPI_{\mathbb{e}}(E_j, E_k) = VPI_{\mathbb{e}}(E_j) +
  VPI_{\mathbb{e}, e_j}(E_k) = VPI_{\mathbb{e}}(E_k) +
  VPI_{\mathbb{e}, e_k}(E_j)
\end{equation}
</div>
</div>

<div id="outline-container-org1145a92" class="outline-3">
<h3 id="org1145a92"><span class="section-number-3">3.2</span> Information Gathering Agents</h3>
<div class="outline-text-3" id="text-3-2">
<p>
we can implement a myopic information-gathering agent, by using the
VPI formula shortsightedly.
</p>

<div class="org-src-container">
<pre class="src src-text">function INFORMATION-GATHERING-AGENT(percept) returns an action
  persistent: D, a decision network

integrate percept into D
j &lt;- the value that maximises VPI(E_j) / Cost(E_j)
if VPI(E_j) &gt; Cost(E_j)
  return REQUEST(E_j)
else return the best action from D
</pre>
</div>

<p>
If we know the associated cost of observing evidence, it simply
retrieves the evidence if the cost of observing it is less than the
value it provides.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Jethro Kuan</p>
<p class="date">Created: 2018-09-01 Sat 17:07</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
