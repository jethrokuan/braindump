<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-30 Thu 15:33 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Linear Algebra</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Jethro Kuan" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Linear Algebra</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0803eb7">1. Span</a></li>
<li><a href="#org1a62296">2. Basis</a></li>
<li><a href="#orgbf69508">3. Matrices as Linear Transformations</a></li>
<li><a href="#org66ad9ea">4. Matrix Multiplication as Composition</a></li>
<li><a href="#orgb8da893">5. Determinant</a></li>
<li><a href="#orgced56f0">6. Matrices for solving linear equations</a></li>
<li><a href="#org9520461">7. Rank</a></li>
<li><a href="#org7832b20">8. Column Space</a></li>
<li><a href="#org849a524">9. Dot Product</a></li>
<li><a href="#orgeab7221">10. Cross Product</a></li>
<li><a href="#orgb0c2ff6">11. How to translate a matrix</a></li>
<li><a href="#org3a850c8">12. Eigenvectors and eigenvalues</a></li>
<li><a href="#orga4dd25c">13. Singular Value Decomposition</a></li>
<li><a href="#org86e3b23">14. References</a></li>
</ul>
</div>
</div>
<div id="outline-container-org0803eb7" class="outline-2">
<h2 id="org0803eb7"><span class="section-number-2">1</span> Span</h2>
<div class="outline-text-2" id="text-1">
<p>
The "span" of \(\hat{v}\) and \(\hat{w}\) is the set of all their linear
combinations:
</p>

\begin{equation}
 a \hat{v} + b \hat{w}
\end{equation}
</div>
</div>

<div id="outline-container-org1a62296" class="outline-2">
<h2 id="org1a62296"><span class="section-number-2">2</span> Basis</h2>
<div class="outline-text-2" id="text-2">
<p>
The basis of a vector space is a set of linearly independent vectors
that span the full space.
</p>
</div>
</div>

<div id="outline-container-orgbf69508" class="outline-2">
<h2 id="orgbf69508"><span class="section-number-2">3</span> Matrices as Linear Transformations</h2>
<div class="outline-text-2" id="text-3">
<p>
A transformation is linear if it fulfills two properties:
</p>

<ol class="org-ol">
<li>All lines remain lines (they don't get curved)</li>
<li>The origin is fixed.</li>
</ol>

<p>
Under a linear transformation, grid lines remain parallel and evenly
spaced. This property allows us to compute the transformed vector,
only by recording how the basis vectors are transformed.
</p>


<div class="figure">
<p><img src="images/linear_algebra/Matrices%20as%20Linear%20Transformations/screenshot_2018-08-25_14-32-57.png" alt="screenshot_2018-08-25_14-32-57.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org66ad9ea" class="outline-2">
<h2 id="org66ad9ea"><span class="section-number-2">4</span> Matrix Multiplication as Composition</h2>
<div class="outline-text-2" id="text-4">
<p>
Often, we want to describe the effects of multiple linear
transformations composed together, for example, a rotation, followed
by a shear. The composition of linear transformations is also a linear
transformation, and can be described with a single matrix (see above).
</p>


<div class="figure">
<p><img src="images/linear_algebra/Matrix%20Multiplication%20as%20Composition/screenshot_2018-08-25_14-36-56.png" alt="screenshot_2018-08-25_14-36-56.png" />
</p>
</div>


<div class="figure">
<p><img src="images/linear_algebra/Matrix Multiplication as Composition/screenshot_2018-08-25_14-41-53.png" alt="screenshot_2018-08-25_14-41-53.png" />
</p>
</div>

<p>
Hence, we can think about matrix multiplication as computing where the
final basis vectors land.
</p>
</div>
</div>

<div id="outline-container-orgb8da893" class="outline-2">
<h2 id="orgb8da893"><span class="section-number-2">5</span> Determinant</h2>
<div class="outline-text-2" id="text-5">
<p>
The fact that linear transformations leave grid lines parallel and
evenly spaced, means that the area of each unit square is scaled by the
same amount.
</p>

<p>
The determinant of a transformation is the amount of scaling of area
of a unit square. If the determinant is negative, then the orientation
of the resulting grid space is reversed.
</p>



<div class="figure">
<p><img src="images/linear_algebra/Determinant/screenshot_2018-08-25_15-28-49.png" alt="screenshot_2018-08-25_15-28-49.png" />
</p>
</div>

<p>
In 3D space, the determinant is the volume of the parallelpiped. 
</p>

\begin{equation}
det \left( \begin{bmatrix}
  a & b & c \\
  d & e & f \\
  g & h & i \\
\end{bmatrix} \right)  = a \cdot det \left( \begin{bmatrix}
  e & f \\
  h & i
\end{bmatrix} \right)
- b \cdot det \left( \begin{bmatrix}
  d & f \\
  g & i
\end{bmatrix}  \right)
+ c \cdot det \left( \begin{bmatrix}
  d & e \\
  g & h
\end{bmatrix} \right)
\end{equation}
</div>
</div>

<div id="outline-container-orgced56f0" class="outline-2">
<h2 id="orgced56f0"><span class="section-number-2">6</span> Matrices for solving linear equations</h2>
<div class="outline-text-2" id="text-6">

<div class="figure">
<p><img src="images/linear_algebra/Matrices for solving linear equations/screenshot_2018-08-25_15-40-10.png" alt="screenshot_2018-08-25_15-40-10.png" />
</p>
</div>

<p>
Suppose we want to compute \(\vec{x}\) such that \(A\vec{x} = \vec{v}\).
Then we can compute the inverse of the matrix \(A\), which corresponds
to the inverse transformation. For example if \(A\) were to rotate the
grid space clockwise 90 degrees, then the inverse of \(A\) would be to
rotate the grid space anti-clockwise 90 degrees: \(\vec{x} = A^{^{-1}}
\vec{v}\).
</p>

<p>
Suppose the determinant of the transformation is 0. Then we know that
it does not have an inverse. However, solutions can still exist. 
</p>
</div>
</div>

<div id="outline-container-org9520461" class="outline-2">
<h2 id="org9520461"><span class="section-number-2">7</span> Rank</h2>
<div class="outline-text-2" id="text-7">
<p>
The rank is the number of dimensions of the output of the
transformation. It is easy to see that the maximum rank of the
transformation is the original dimensions of the matrix. Rank
corresponds to the maximal number of linearly independent columns of
\(A\).
</p>
</div>
</div>

<div id="outline-container-org7832b20" class="outline-2">
<h2 id="org7832b20"><span class="section-number-2">8</span> Column Space</h2>
<div class="outline-text-2" id="text-8">
<p>
The column space of the matrix \(A\) is the set of all possible outputs of
\(A \vec{v}\). It is also the span of all the columns.
</p>
</div>
</div>

<div id="outline-container-org849a524" class="outline-2">
<h2 id="org849a524"><span class="section-number-2">9</span> Dot Product</h2>
<div class="outline-text-2" id="text-9">
<p>
The dot product \(\vec{w} \cdot \vec{v}\) can be viewed as the
\((\text{length of projected vector }\vec{x}) \cdot (\text{length of
}\vec{x})\).
</p>

<p>
We can think of \(1 \times 2\) matrices as projection matrices, where the
first column indicates where \(\hat{i}\) lands, and the second column
indicates where \(\hat{j}\) lands. Suppose we have a vector \(\hat{i}\),
and we want to project it onto \(\hat{\mu}\). By symmetry, it's the same
value as when \(\hat{\mu}\) is projected onto \(\hat{i}\). However, this is
just the \(x\) coordinate value of \(\hat{\mu}\).
</p>


<div class="figure">
<p><img src="images/linear_algebra/Dot Product/screenshot_2018-08-25_16-03-15.png" alt="screenshot_2018-08-25_16-03-15.png" />
</p>
</div>

<p>
Hence, \(\hat{i}\) and \(\hat{j}\) land at \(\mu_x\) and \(\mu_y\) respectively. We
can easily see the duality between matrix-vector product and dot
product here.
</p>



<div class="figure">
<p><img src="images/linear_algebra/Dot Product/screenshot_2018-08-25_16-05-08.png" alt="screenshot_2018-08-25_16-05-08.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgeab7221" class="outline-2">
<h2 id="orgeab7221"><span class="section-number-2">10</span> Cross Product</h2>
<div class="outline-text-2" id="text-10">
<p>
The cross product of \(\vec{v}\) and \(\vec{w}\), denoted \(\vec{v} \times
\vec{w}\) is a vector. The vector has length equal to the area of a
parallelogram obtained by duplicating and shifting the two vectors.
The sign of the cross product is determined using the right-hand rule.
This vector is perpendicular to the parallelogram.
</p>


<div class="figure">
<p><img src="images/linear_algebra/Cross Product/screenshot_2018-08-25_16-29-46.png" alt="screenshot_2018-08-25_16-29-46.png" />
</p>
</div>



<div class="figure">
<p><img src="images/linear_algebra/Cross Product/screenshot_2018-08-25_16-33-08.png" alt="screenshot_2018-08-25_16-33-08.png" />
</p>
</div>

<p>
We want to find the dual vector \(\hat{p}\) that corresponds to the
cross product.
</p>


<div class="figure">
<p><img src="images/linear_algebra/Cross Product/screenshot_2018-08-25_16-39-00.png" alt="screenshot_2018-08-25_16-39-00.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgb0c2ff6" class="outline-2">
<h2 id="orgb0c2ff6"><span class="section-number-2">11</span> How to translate a matrix</h2>
<div class="outline-text-2" id="text-11">
<p>
Suppose someone uses a different coordinate system (i.e. different
basis vectors), which we can represent with a matrix:
</p>

\begin{equation}
  \begin{bmatrix}
    2 & -1 \\
    1 & 1
  \end{bmatrix}
\end{equation}

<p>
Suppose then that we want to apply a linear transformation to a vector
in her coordinate system. In the case of a rotation 90 degrees
anti-clockwise, it would be represented in a matrix as:
</p>

\begin{equation}
  \begin{bmatrix}
    0 & -1 \\
    1 & 0
  \end{bmatrix}
\end{equation}

<p>
In the "default" basis vector coordinate system. What does this
transformation look like in the new coordinate system? Given some
vector \(\hat{v}\) in the other language. First, we translate the
vector into one in the default language:
</p>

\begin{equation}
  \begin{bmatrix}
    2 & -1 \\
    1 & 1
  \end{bmatrix}
  \hat{v}
\end{equation}

<p>
Then, we apply the transformation to the vector in the default
language:
</p>

\begin{equation}
  \begin{bmatrix}
    0 & -1 \\
    1 & 0
  \end{bmatrix}
  \begin{bmatrix}
    2 & -1 \\
    1 & 1
  \end{bmatrix}
  \hat{v}
\end{equation}

<p>
Then, we apply to the inverse of the change in basis matrix, to return
the vector to the other language:
</p>

\begin{equation}
  \begin{bmatrix}
    2 & -1 \\
    1 & 1
  \end{bmatrix}^{-1}
  \begin{bmatrix}
    0 & -1 \\
    1 & 0
  \end{bmatrix}
  \begin{bmatrix}
    2 & -1 \\
    1 & 1
  \end{bmatrix}
  \hat{v}
\end{equation}

<p>
This form \(A^{{-1}}MA\) is frequently encountered when dealing with
eigenvectors and eigenvalues.
</p>
</div>
</div>

<div id="outline-container-org3a850c8" class="outline-2">
<h2 id="org3a850c8"><span class="section-number-2">12</span> Eigenvectors and eigenvalues</h2>
<div class="outline-text-2" id="text-12">
<p>
Consider the span of a particular vector, that is, the set of vectors
obtainable by applying a scaling constant to it. Some vectors remain
on their own span, even with linear transformations.
</p>

<p>
These vectors are called <i>eigenvectors</i>, and the value of the scaling
constant is called the <i>eigenvalue</i>. Mathematically, this is expressed
as:
</p>

\begin{equation}
  A \hat{v} = \lambda \hat{v}
\end{equation}

<p>
Consider a 3D rotation. If we can find an eigenvector for this 3D
transformation, then we have found the axis of rotation.
</p>
</div>
</div>

<div id="outline-container-orga4dd25c" class="outline-2">
<h2 id="orga4dd25c"><span class="section-number-2">13</span> Singular Value Decomposition</h2>
<div class="outline-text-2" id="text-13">
<p>
Given an input data matrix \(A\) consisting of \(m\) documents and \(n\)
terms, we can decompose it into 3 matrices.
</p>

\begin{equation}
  A_{[m \times n]} = U_{[m \times r]} \Sigma_{[r \times r]} V_{[n
    \times r]}^T
\end{equation}

<p>
\(U\) are left singular vectors of size \(m \times r\), which we can think of
as \(m\) documents and \(r\) concepts. \(\Sigma\) is a \(r \times r\) diagonal matrix,
representing the strength of each concept, where \(r\) is the rank of
the matrix \(A\). \(V\) stores the right singular vectors, consisting of
\(n\) terms and \(r\) concepts.
</p>


<div class="figure">
<p><img src="images/linear_algebra/Singular Value Decomposition/screenshot_2018-08-25_17-26-42.png" alt="screenshot_2018-08-25_17-26-42.png" />
</p>
</div>

<p>
It is always possible to decompose a real matrix \(A\) into \(A = U \Sigma
V^T\), where:
</p>

<ol class="org-ol">
<li>\(U\), \(\Sigma\), and \(V\) are unique</li>
<li>\(U\) and \(V\) are column orthonormal: \(U^T U = I\), \(V^T V = I\)</li>
<li>\(\Sigma\) is diagonal, and entries are positive, sorted in decreasing
order.</li>
</ol>
</div>
</div>


<div id="outline-container-org86e3b23" class="outline-2">
<h2 id="org86e3b23"><span class="section-number-2">14</span> References</h2>
<div class="outline-text-2" id="text-14">
<ol class="org-ol">
<li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra</a></li>
<li><a href="https://www.youtube.com/watch?v=P5mlg91as1c">Lecture 47 - Singular Value Decomposition | Stanford University</a></li>
</ol>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Jethro Kuan</p>
<p class="date">Created: 2018-08-30 Thu 15:33</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
