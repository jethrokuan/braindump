<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-30 Thu 15:33 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Deep Learning</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Jethro Kuan" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Deep Learning</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orga5472d8">1. Definition</a></li>
<li><a href="#orgeaec2eb">2. Introduction</a></li>
<li><a href="#org3d2c4ce">3. Model/task categorisation</a></li>
<li><a href="#orgd91ea15">4. Bayesian probability</a></li>
<li><a href="#org812b8d2">5. Universal Approximation Theorem</a></li>
<li><a href="#orge2f7d1c">6. Measure Theory</a>
<ul>
<li><a href="#orgf8e6c4e">6.1. Kullback-Leiber (KL) divergence</a></li>
<li><a href="#org6fc4fbb">6.2. Cross Entropy</a></li>
</ul>
</li>
<li><a href="#orgb90b55e">7. Object Detection</a>
<ul>
<li><a href="#org1c7e8e8">7.1. Detection Without Proposals: YOLO / SSD</a></li>
</ul>
</li>
<li><a href="#org31f6f62">8. Learning Rates</a>
<ul>
<li><a href="#org1c470e3">8.1. Learning Rate Annealing</a></li>
</ul>
</li>
<li><a href="#org240f5e6">9. Reinforcement Learning</a></li>
<li><a href="#orgb55de45">10. How Companies use Deep Learning</a>
<ul>
<li><a href="#org5c9ae50">10.1. ViSenze</a>
<ul>
<li><a href="#org34e63bf">10.1.1. Things they focused on as a company</a></li>
<li><a href="#orgbb1209c">10.1.2. Visual Embeddings Used</a></li>
<li><a href="#org94ff0a7">10.1.3. Lessons Learnt</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orga632c0e">11. Key Open Questions about Deep Learning Systems</a>
<ul>
<li><a href="#org3eeeaee">11.1. Concrete Theoretical Questions</a></li>
</ul>
</li>
<li><a href="#org4a9bf23">12. ConvNets from First Principles</a>
<ul>
<li><a href="#orgd1edff9">12.1. Finding a generative model</a></li>
</ul>
</li>
<li><a href="#org67a3e87">13. Variational Inference and Deep Learning</a>
<ul>
<li><a href="#org9245748">13.1. Derivation of the Variational Bound</a></li>
<li><a href="#org7752580">13.2. Variational Autoencoder</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orga5472d8" class="outline-2">
<h2 id="orga5472d8"><span class="section-number-2">1</span> Definition</h2>
<div class="outline-text-2" id="text-1">
<p>
A class of machine learning algorithm that uses a cascade of multiple
layeprs of non-linear processing units for feature extraction and
transformation
</p>
</div>
</div>
<div id="outline-container-orgeaec2eb" class="outline-2">
<h2 id="orgeaec2eb"><span class="section-number-2">2</span> Introduction</h2>
<div class="outline-text-2" id="text-2">
<p>
Sensory perception is plagued with large amounts of nuisance
variation. Nuisance variables are task-dependent and can be implicit.
How do we deal with nuisance variation in the input?
</p>

<p>
The holy grail of machine learning is to learn a <i>disentangled
representation</i> that factors out variation in the sensory input into
meaningful intrinsic degrees of freedom.
</p>

<dl class="org-dl">
<dt>selective</dt><dd>sensitive to task-relevant (target) features</dd>
<dt>invariant</dt><dd>robust to task-irrelevant (nuisance) features</dd>
<dt>multi-task</dt><dd>useful for many different tasks</dd>
</dl>

<p>
Deep learning solves this central problem in representation learning
by introducing representations that are expressed in terms of other,
simpler representations. Deep learning enables the computer to build
complex concepts out of simpler concepts.
</p>

<p>
The key inspiration from neuroscience is to build up feature
selectivity and tolerance over multiple layers in a hierarchy.
</p>
</div>
</div>

<div id="outline-container-org3d2c4ce" class="outline-2">
<h2 id="org3d2c4ce"><span class="section-number-2">3</span> Model/task categorisation</h2>
<div class="outline-text-2" id="text-3">
<dl class="org-dl">
<dt>supervised learning</dt><dd>input -&gt; otuput, learning the pattern from
labeled training data</dd>
<dt>unsupervised learning</dt><dd>learning to differentiate/cluster
unlabeled data</dd>
<dt>reinforcement learning</dt><dd>learning from trial-and-error through rewards</dd>
</dl>
</div>
</div>
<div id="outline-container-orgd91ea15" class="outline-2">
<h2 id="orgd91ea15"><span class="section-number-2">4</span> Bayesian probability</h2>
<div class="outline-text-2" id="text-4">
<p>
<a href="https://www.quora.com/For-a-non-expert-what-is-the-difference-between-Bayesian-and-frequentist-approaches/answer/Jason-Eisner">https://www.quora.com/For-a-non-expert-what-is-the-difference-between-Bayesian-and-frequentist-approaches/answer/Jason-Eisner</a>
The quantification of the expression of uncertainty. We can make
precise revisions of uncertainty in light of new evidence.
</p>

<p>
In the example of polynomial curve fitting, it seems reasonable to
apply the frequentist notion of probability to the random values of
the observed variables. However, we can use probability theory to
describe the uncertainty in model parameters, and the choice of model
itself.
</p>

<p>
We can evaluate the uncertainty in model parameters <code>w</code> after we have
observed some data <code>D</code>. \(p(D|w)\) is called the <i>likelihood function</i>.
</p>

<p>
The posterior probability is proportional to <code>likelihood x prior</code>.
Model parameters <code>w</code> are chosen to maximise the likelihood function
\(p(D|w)\). Because the negative log-likelihood is a monotonically
decreasing function, minimising it is equivalent to maximising
likelihood.
</p>
</div>
</div>
<div id="outline-container-org812b8d2" class="outline-2">
<h2 id="org812b8d2"><span class="section-number-2">5</span> Universal Approximation Theorem</h2>
<div class="outline-text-2" id="text-5">
<p>
<a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">https://en.wikipedia.org/wiki/Universal_approximation_theorem</a>
A feed-forward network with a single hidden layer containing a finite
number of neurons can approximate continuous functions on compact
subsets of the real space.
</p>
</div>
</div>

<div id="outline-container-orge2f7d1c" class="outline-2">
<h2 id="orge2f7d1c"><span class="section-number-2">6</span> Measure Theory</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-orgf8e6c4e" class="outline-3">
<h3 id="orgf8e6c4e"><span class="section-number-3">6.1</span> Kullback-Leiber (KL) divergence</h3>
<div class="outline-text-3" id="text-6-1">
<p>
The KL divergence is 0 iff P and Q are the same distribution in
discrete variables, and equal almost everywhere in continuous
variables. Because the KL divergence is non-negative and measures the
difference between the two distributions, it is soften conceptualised
as some sort of distance, but it is not a true measure, because it is
not symmetric.
</p>
</div>
</div>
<div id="outline-container-org6fc4fbb" class="outline-3">
<h3 id="org6fc4fbb"><span class="section-number-3">6.2</span> Cross Entropy</h3>
<div class="outline-text-3" id="text-6-2">
<p>
Minimising the cross entropy with respect to Q is equivalent to
minimising the KL divergence.
</p>
</div>
</div>
</div>
<div id="outline-container-orgb90b55e" class="outline-2">
<h2 id="orgb90b55e"><span class="section-number-2">7</span> Object Detection</h2>
<div class="outline-text-2" id="text-7">
<ul class="org-ul">
<li>Region Proposal Networks find blobby regions that are likely to
contain objects, and are relatively fast to run
<ul class="org-ul">
<li>e.g. Selective Search, gives about 2000 region proposals in a few
seconds on CPU.</li>
<li><a href="https://ivi.fnwi.uva.nl/isis/publications/bibtexbrowser.php?key=UijlingsIJCV2013&amp;bib=all.bib">https://ivi.fnwi.uva.nl/isis/publications/bibtexbrowser.php?key=UijlingsIJCV2013&amp;bib=all.bib</a></li>
</ul></li>
<li>R-CNN uses a RPN like selective search to generate region proposals,
followed by a convnet and an SVM on top to predict image labels.
There is also a bounding box regression loss, which predicts
correction to the proposals generated.</li>
<li>Fast R-CNN passes the image through a convnet, and run the RPN to
generate image proposals on a particular feature map of the image.
These crops go through a RoI pooling layer.</li>
<li>Faster R-CNN trains the region proposal network, and is jointly
trained with 4 losses:
<ul class="org-ul">
<li>RPN classify object/not object</li>
<li>RPN regress box coordinates</li>
<li>Final classification score (object classes)</li>
<li>Final object coordinates</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org1c7e8e8" class="outline-3">
<h3 id="org1c7e8e8"><span class="section-number-3">7.1</span> Detection Without Proposals: YOLO / SSD</h3>
<div class="outline-text-3" id="text-7-1">
<p>
Within each grid cell:
</p>
<ul class="org-ul">
<li>Regress from each of the B base boxes to a final box with 5 numbers:
dx, dy, dh, dw, confidence</li>
<li>Predict scores for each of C classes (including background as a
class)</li>
</ul>

<p>
Output: \(7 * 7 *(5*B + C)\)
</p>
</div>
</div>
</div>

<div id="outline-container-org31f6f62" class="outline-2">
<h2 id="org31f6f62"><span class="section-number-2">8</span> Learning Rates</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-org1c470e3" class="outline-3">
<h3 id="org1c470e3"><span class="section-number-3">8.1</span> Learning Rate Annealing</h3>
<div class="outline-text-3" id="text-8-1">
<p>
Decay learning rate after several iterations&#x2026;
</p>

<p>
Also: SGDR with cyclic learning rate, restores high learning rate
after several iterations to try to find local minima that is largely
flat, and doesn't change so much in any direction. (Generalises
better)
</p>
</div>
</div>
</div>
<div id="outline-container-org240f5e6" class="outline-2">
<h2 id="org240f5e6"><span class="section-number-2">9</span> Reinforcement Learning</h2>
<div class="outline-text-2" id="text-9">
<p>
<a href="https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287">https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287</a>
</p>
</div>
</div>
<div id="outline-container-orgb55de45" class="outline-2">
<h2 id="orgb55de45"><span class="section-number-2">10</span> How Companies use Deep Learning</h2>
<div class="outline-text-2" id="text-10">
</div>
<div id="outline-container-org5c9ae50" class="outline-3">
<h3 id="org5c9ae50"><span class="section-number-3">10.1</span> ViSenze</h3>
<div class="outline-text-3" id="text-10-1">
<p>
Visenze's primary product is their Visual Search (reverse image
search).
</p>

<p>
Initially, they started out with a similar model to our approach:
Train a CNN, read encodings before the FC layer, use it to perform NN
search.
</p>

<p>
Now their pipeline is as follows:
Query time -&gt; object detection -&gt; Extract Features -&gt; Nearest Neighbours -&gt; Ranked Results
Offline training -&gt; Detection Model -&gt; Embedding models -&gt; Nearest Neighbours
Index time -&gt; Objects -&gt; Extract Features -&gt; Search Index
(Compression/Hash Model)
</p>

<p>
Object detection followed the trends in research papers:
</p>
<ol class="org-ol">
<li>R-CNN</li>
<li>Fast-CNN</li>
<li>Faster-CNN</li>
<li>YOLO/SSD</li>
<li>DSOD (Current)</li>
</ol>

<p>
Model performance is based of standard IR metrics: They are using DCG
score for evaluating their reverse image search. This requires a lot
of manual annotation.
</p>

<p>
Models are trained offline using physical purchased GPUs.
</p>

<p>
Deployment: Kubernetes on AWS, with generally small CPU servers.
Overall latency is less than 200ms.
</p>
</div>

<div id="outline-container-org34e63bf" class="outline-4">
<h4 id="org34e63bf"><span class="section-number-4">10.1.1</span> Things they focused on as a company</h4>
<div class="outline-text-4" id="text-10-1-1">
<ol class="org-ol">
<li>Tooling:
<ol class="org-ol">
<li>Annotation System
<ol class="org-ol">
<li>Complete annotation is crucial to detection training</li>
</ol></li>
<li>Training System
<ol class="org-ol">
<li>Make it easy to change hyperparameters and retrain models</li>
<li>Abstract away need for knowing deep learning</li>
<li>Platform for tracking metrics</li>
</ol></li>
<li>Querylog pipeline
<ol class="org-ol">
<li>Take user input as training data</li>
</ol></li>
<li>Evaluation System
<ol class="org-ol">
<li>Both automatic evaluation via metrics and manual (AB testing)
is done before release</li>
<li>Visualisations via T-SNE to see if clusters remain the
same/make sense, when new learnings are added: <b>learning
without forgetting</b></li>
</ol></li>
</ol></li>
<li>Business:
<ol class="org-ol">
<li>Attend to customer requirements: e.g. if a company wants to sell
hats, model needs to be trained to detect hats, and these
learnings need to be added to the existing model without
affecting data earlier</li>
</ol></li>
</ol>
</div>
</div>
<div id="outline-container-orgbb1209c" class="outline-4">
<h4 id="orgbb1209c"><span class="section-number-4">10.1.2</span> Visual Embeddings Used</h4>
<div class="outline-text-4" id="text-10-1-2">
<p>
At Visenze, they use multiple embeddings in different feature spaces
to measure similarity. The results are then combined before returned.
The 4 main embeddings are:
</p>

<ol class="org-ol">
<li>Exact matches (same item)
<ol class="org-ol">
<li>Trained with siamese network with batched triplet loss
(typically used in face recognition, but seems to work well with
product classification)</li>
</ol></li>
<li>Same Category
<ol class="org-ol">
<li>Domain specific labels have been most helpful in achieving
state-of-the-art accuracy
<ol class="org-ol">
<li>e.g for fashion, sleeve length, jeans length etc.</li>
</ol></li>
</ol></li>
<li>Similar Categories</li>
</ol>
</div>
</div>
<div id="outline-container-org94ff0a7" class="outline-4">
<h4 id="org94ff0a7"><span class="section-number-4">10.1.3</span> Lessons Learnt</h4>
<div class="outline-text-4" id="text-10-1-3">
<ol class="org-ol">
<li>Taxonomy Coverage</li>
<li>Training Data Coverage
<ol class="org-ol">
<li>Obtaining training data from one source only can lead to severe
bias (e.g. detecting watermarks and using it as feature)</li>
</ol></li>
<li>Overfitting</li>
<li>Continuous Improvement (Learning Without Forgetting)</li>
<li>Bad-case driven development
<ol class="org-ol">
<li>Be quick to identify hard negatives, and add in similar
negative samples into training data to improve accuracy</li>
</ol></li>
<li>Image quality, rotation</li>
<li>Re-ranking based on customer</li>
</ol>
</div>
</div>
</div>
</div>
<div id="outline-container-orga632c0e" class="outline-2">
<h2 id="orga632c0e"><span class="section-number-2">11</span> Key Open Questions about Deep Learning Systems</h2>
<div class="outline-text-2" id="text-11">
<ol class="org-ol">
<li>How and why do they work? Can we derive their structure from first
principles? Can we compress/explain the myriad empirical
observations/best practices about deep nets?</li>
<li>Can we shed new light on the hidden representations of objects? Can
we generate new theories and testable predictions for both
artificial/real neuroscience?</li>
<li>Why do they fail? How to improve them? How to alleviate their
intrinsic limitations?</li>
<li>Can we guide the search for better
architectures/algorithms/performance in applied DL?</li>
</ol>
</div>
<div id="outline-container-org3eeeaee" class="outline-3">
<h3 id="org3eeeaee"><span class="section-number-3">11.1</span> Concrete Theoretical Questions</h3>
<div class="outline-text-3" id="text-11-1">
<ol class="org-ol">
<li>What are the implicit modelling assumptions?</li>
<li>What is the inference task and algorithm?</li>
<li>What is the learning algorithm?</li>
<li>Can we generate new testable predictions for artificial/real nets?</li>
<li>What modeling assumptions are being violated in failures? How can
we improve the models, tasks and algorithms?</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org4a9bf23" class="outline-2">
<h2 id="org4a9bf23"><span class="section-number-2">12</span> ConvNets from First Principles</h2>
<div class="outline-text-2" id="text-12">
<p>
There are many architectures, but just a few key operations and
objectives:
</p>
<ul class="org-ul">
<li>2D (De)Convolution, Spatial max-(un)pooling, ReLu, Skip-connections</li>
<li>Batch Normalization</li>
<li>DropOut, Noise Corruption</li>
<li>Data Augmentation</li>
<li>Objectives: XEnt, NLL, Reconstruction Error, Mutual Information</li>
</ul>

<p>
We focus on the properties conserved across all species of Convnets.
</p>
</div>
<div id="outline-container-orgd1edff9" class="outline-3">
<h3 id="orgd1edff9"><span class="section-number-3">12.1</span> Finding a generative model</h3>
<div class="outline-text-3" id="text-12-1">
<p>
We reverse engineer a Convnet by building a generative model
</p>

<ol class="org-ol">
<li>Define a generative model that captures nuisance variation</li>
<li>Recast feed-forward propogation in a DCN as MAP inference of the
full latent configuration -&gt; generative classifier</li>
<li>Apply a <i>discriminative relaxation</i> -&gt; discriminative classifier</li>
<li>Learn the parameters via Batch Hard EG Algorithm -&gt; SGD-Backprop
Training of a DCN</li>
<li>Use new generative model to address limitations of DCNs: top-down
inference, learning from unlabeled data, hyperparameter
optimization</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org67a3e87" class="outline-2">
<h2 id="org67a3e87"><span class="section-number-2">13</span> Variational Inference and Deep Learning</h2>
<div class="outline-text-2" id="text-13">
<p>
<a href="https://www.youtube.com/watch?v=h0UE8FzdE8U">https://www.youtube.com/watch?v=h0UE8FzdE8U</a>
</p>

<p>
Optimizers work well with sums, not products. We want to be able to
calculate gradients on a subset of examples and have reasonable
confidence for convergence.
</p>

<p>
Latent variables give our model structure and can make training more
tractable. With latent variables \(\bar{z}\), we have:
</p>

\begin{equation}
  p(x) = \sum_z p(x,z) =  \sum_z p(x | z)p(z)
\end{equation}

<p>
Putting log-likelihood and latent variables together, we get:
</p>

\begin{equation}
L = \sum_i \log \left( \sum_z p(x_i, z)\right)
\end{equation}

<p>
Usually, we want to sample one example and one value for the latent
variable at a time, for tractable optimization. This only works for a sum.
</p>

<p>
We compare the above structure with this equation:
</p>

\begin{equation}
L = \sum_i \left( \sum_z \log p(x_i, z)\right)
\end{equation}

<p>
Intuitively the new equation says that every latent variable value
needs to do a good job of explaining the data
</p>
</div>

<div id="outline-container-org9245748" class="outline-3">
<h3 id="org9245748"><span class="section-number-3">13.1</span> Derivation of the Variational Bound</h3>
<div class="outline-text-3" id="text-13-1">
\begin{equation}
  p(x) = \sum_i \log (\sum_z p(x_i, z))
\end{equation}

\begin{equation}
p(x) = \sum_i \log(\sum_z \frac{q(z|x_i) p(x_i, z)}{q(z|x_i)})
\end{equation}

<p>
By Jensen's Inequality (concavity of the log function),
</p>

\begin{equation}
p(x) \ge \sum_i \sum_z q(z|x_i) \log \frac{p(x_i, z)}{q(z|x_i)}
\end{equation}

\begin{equation}
  p(x) \ge \sum_i q(z|x_i) \sum_z \log (p(x_i | z)) + \log \frac{p(z)}{q(z|x_i)}
\end{equation}

\begin{equation}
  p(x) \ge \mathbb{E}_{z \sim q(z|x_i)} \log (p(x_i | z)) - KL(q(z|x_i) || p(z))
\end{equation}

<p>
The first term is the conditional likelihood of our observation if we
used a sampled z value from the approximated posterior \(q(z | x)\). The
second term is a divergence between the approximate posterior and the
prior. This is often well defined.
</p>
</div>
</div>

<div id="outline-container-org7752580" class="outline-3">
<h3 id="org7752580"><span class="section-number-3">13.2</span> Variational Autoencoder</h3>
<div class="outline-text-3" id="text-13-2">
<p>
We can show that a special type of autoencoder corresponds to this
lower bound, where \(q(z | x)\) is the encoder and \(p(x|z)\) is the
decoder. The first term is the usual reconstruction objective for
autoencoders. The second term is a special KL term which pulls the
bottleneck closer to a prior.
</p>

<p>
In the simplest case, we make both \(q(z|x)\) and \(p(z)\) independent
Gaussians:
</p>

\begin{equation}
  KL(q(z|x_i) || p(z)) = \sum_j \mu_j^2 + \sigma_j^2 - \log(\sigma_j^2)
\end{equation}

<p>
\(q(z|x)\) has parameters \(\mu\) and \(\sigma\) which are just outputs from our
encoder. \(p(z)\) is a prior distribution with \(\mu = 0\) and \(\sigma = 1\).
</p>

<p>
The bottleneck can't remember spatial information, and optimizing for
\(p(x|z)\) puts heavy emphasis on exact recovery of spatial information.
Solutions involve removing the bottleneck, or replacing the prior
\(p(z)\) with one that has more structure.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Jethro Kuan</p>
<p class="date">Created: 2018-08-30 Thu 15:33</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
