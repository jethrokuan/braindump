<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-02-09 Fri 11:58 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Jethro Kuan" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org1f2c30b">1. Connecting to SoC VPN</a></li>
<li><a href="#orgbb3d07d">2. Setting up GPU cluster</a></li>
<li><a href="#orgf499664">3. Current Methodology</a>
<ul>
<li><a href="#orgf0aebfb">3.1. TextRank</a></li>
<li><a href="#org86033ae">3.2. RNN</a></li>
<li><a href="#org6fd9e85">3.3. Definition of Trend</a></li>
</ul>
</li>
<li><a href="#org256661d">4. Keyphrase Extraction</a>
<ul>
<li><a href="#org36a30ff">4.1. SurfKE</a>
<ul>
<li><a href="#orgdf07e78">4.1.1. Graph Construction</a></li>
<li><a href="#org45cf9d6">4.1.2. Learning Feature Representations</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgca74f15">5. Word Embeddings</a></li>
<li><a href="#org6daaea3">6. Topical Hierarchy</a></li>
<li><a href="#org9dea2c1">7. Topic Modeling</a>
<ul>
<li><a href="#orgde4005e">7.1. LDA</a>
<ul>
<li><a href="#org9c3f777">7.1.1. Dirichlet Distribution</a></li>
<li><a href="#orgbd98ac5">7.1.2. Exploring a Corpus with the posterior distribution</a></li>
<li><a href="#org6689543">7.1.3. Posterior Inference</a>
<ul>
<li><a href="#org1d8964b">7.1.3.1. Mean Field Variational Inference</a></li>
</ul>
</li>
<li><a href="#orga4dd8b0">7.1.4. Shortcomings</a></li>
</ul>
</li>
<li><a href="#org51c57e4">7.2. TopicRNN</a></li>
</ul>
</li>
</ul>
</div>
</div>
<ol class="org-ol">
<li>LDA, DTM - don't work as well as deep learning model</li>
</ol>

<p>
LDA: list of important words for n topics
DTM: make LDA time variant
</p>

<p>
Problems: common wworks like "computer"&#x2026; 
</p>

<p>
HITS algo, similar to PageRank
</p>

<p>
2 concepts, authors, keywords -&gt; important authors say important
keywords
</p>

<p>
modified textrank
</p>

<p>
&lt;NN&gt;*&lt;NN|NNS&gt;
</p>

<ol class="org-ol">
<li>Author -&gt; keyphrases</li>
<li>Keyphrase needs to be clustered LDA, Embedding</li>
<li>What is a trend? (coarser grain than keyphrase), hierachical
agglomerate clustering</li>
</ol>

<p>
Couperin
</p>
<div id="outline-container-org1f2c30b" class="outline-2">
<h2 id="org1f2c30b"><span class="section-number-2">1</span> Connecting to SoC VPN</h2>
<div class="outline-text-2" id="text-1">
<p>
use openfortivpn.
</p>

<div class="org-src-container">
<pre class="src src-sh">sudo openfortivpn webvpn.comp.nus.edu.sg:443 -u e0036913 -p pass --trusted-cert cert
</pre>
</div>
</div>
</div>

<div id="outline-container-orgbb3d07d" class="outline-2">
<h2 id="orgbb3d07d"><span class="section-number-2">2</span> Setting up GPU cluster</h2>
<div class="outline-text-2" id="text-2">
<ol class="org-ol">
<li>Install Anaconda</li>
<li>Setup conda environment with required packages</li>
<li>conda env update</li>
<li>source activate fastai</li>
</ol>
</div>
</div>

<div id="outline-container-orgf499664" class="outline-2">
<h2 id="orgf499664"><span class="section-number-2">3</span> Current Methodology</h2>
<div class="outline-text-2" id="text-3">
<ol class="org-ol">
<li>Use TextRank algorithm</li>
</ol>
</div>
<div id="outline-container-orgf0aebfb" class="outline-3">
<h3 id="orgf0aebfb"><span class="section-number-3">3.1</span> TextRank</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Prior to TextRank, use NLTK to extract noun phrases via Regex. Each
noun phrase is split further.
</p>
</div>
</div>
<div id="outline-container-org86033ae" class="outline-3">
<h3 id="org86033ae"><span class="section-number-3">3.2</span> RNN</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Basic RNN is employed to use the first k sequential years to predict
the (k+1)th years score. The model takes the top 20 phrases for each
year, and the top 20 based on predictions from the RNN. It is expected
that the predicted phrases closely match the actual top 20 phrases of
the year.
</p>

<p>
A linear regression model is used as the baseline comparison.
</p>
</div>
</div>
<div id="outline-container-org6fd9e85" class="outline-3">
<h3 id="org6fd9e85"><span class="section-number-3">3.3</span> Definition of Trend</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Considerations:
</p>

<ol class="org-ol">
<li>Research areas should have a strong foundation, but not be the
most-discussed issue at the same time</li>
<li>Research areas should be gradually studied</li>
</ol>

\begin{equation*}
trend = a \times (x\textsubscript{k+1} - max(x\textsubscript{1}, ... , x\textsubscript{k})) + (1 - a) \times max(x\textsubscript{1}, ... , x\textsubscript{k})
\end{equation*}
</div>
</div>
</div>

<div id="outline-container-org256661d" class="outline-2">
<h2 id="org256661d"><span class="section-number-2">4</span> Keyphrase Extraction</h2>
<div class="outline-text-2" id="text-4">
<p>
Keyphrase extraction (KE) is the task of automatically extracting
descriptive phrases or concepts that represent the main topics of a
document.
</p>

<p>
In supervised approaches, a candidate phrase is encoded with a set of
features, such as <i>tf-idf</i>. One problem is that many of these manual
features are determined based on statistical information from
documents outside the target domain. Hence, it is essential to
automatically discover such features without relying on feature
engineering.
</p>
</div>
<div id="outline-container-org36a30ff" class="outline-3">
<h3 id="org36a30ff"><span class="section-number-3">4.1</span> SurfKE</h3>
<div class="outline-text-3" id="text-4-1">
</div>
<div id="outline-container-orgdf07e78" class="outline-4">
<h4 id="orgdf07e78"><span class="section-number-4">4.1.1</span> Graph Construction</h4>
<div class="outline-text-4" id="text-4-1-1">
<p>
Build an undirected word graph, with words as vertices. There is an
edge between two words if the words co-occur within <code>w</code> contiguous
word tokens. The weight of the edge is computed based on the
co-occurence within a window of w successive tokens in the
document.
</p>
</div>
</div>
<div id="outline-container-org45cf9d6" class="outline-4">
<h4 id="org45cf9d6"><span class="section-number-4">4.1.2</span> Learning Feature Representations</h4>
<div class="outline-text-4" id="text-4-1-2">
<p>
Learn a mapping function \(f : V \rightarrow \mathbb{R}^{|V| \times d},
d \le |V|\) where d is a parameter specifying the number of dimensions
for the embeddings. The network feature learning model requires a
corpus and a vocabulary in order to learn a mapping of nodes to a
low-dimensional space of features. A biased sampled random walk
strategy is employed.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgca74f15" class="outline-2">
<h2 id="orgca74f15"><span class="section-number-2">5</span> Word Embeddings</h2>
<div class="outline-text-2" id="text-5">
<p>
<a class='org-ref-reference' href="#smalheiser-2018-unsup-low">smalheiser-2018-unsup-low</a>
</p>

<p>
<a class='org-ref-reference' href="#shalaby-2018-beyon-word-embed">shalaby-2018-beyon-word-embed</a>
</p>
</div>
</div>
<div id="outline-container-org6daaea3" class="outline-2">
<h2 id="org6daaea3"><span class="section-number-2">6</span> Topical Hierarchy</h2>
<div class="outline-text-2" id="text-6">
<p>
<a href="file:///home/jethro/Dropbox/NUS/UROP/CATHY_-_Construction_of_Topical_Hierarch.pdf">CATHY</a>
</p>

<p>
CATHY - co-occurrence network clustering, recursively apply
CATHY to discover sub-topics
</p>

<p>
<a href="http://chbrown.github.io/kdd-2013-usb/kdd/p437.pdf">Phrase Mining Framework for Recursive Construction of a Topical Hierachy</a>
</p>
</div>
</div>
<div id="outline-container-org9dea2c1" class="outline-2">
<h2 id="org9dea2c1"><span class="section-number-2">7</span> Topic Modeling</h2>
<div class="outline-text-2" id="text-7">
<p>
<a href="http://www.cs.columbia.edu/~blei/topicmodeling.html">http://www.cs.columbia.edu/~blei/topicmodeling.html</a>
</p>
</div>
<div id="outline-container-orgde4005e" class="outline-3">
<h3 id="orgde4005e"><span class="section-number-3">7.1</span> LDA</h3>
<div class="outline-text-3" id="text-7-1">
<p>
<a href="https://www.youtube.com/watch?v=FkckgwMHP2s">https://www.youtube.com/watch?v=FkckgwMHP2s</a>
<a href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf">http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf</a>
<a class='org-ref-reference' href="#blei2009topic">blei2009topic</a>
</p>
</div>
<div id="outline-container-org9c3f777" class="outline-4">
<h4 id="org9c3f777"><span class="section-number-4">7.1.1</span> Dirichlet Distribution</h4>
<div class="outline-text-4" id="text-7-1-1">
<p>
<a href="https://www2.ee.washington.edu/techsite/papers/documents/UWEETR-2010-0006.pdf">https://www2.ee.washington.edu/techsite/papers/documents/UWEETR-2010-0006.pdf</a>
</p>

<p>
Dirchilet distribution is a family of continuous multivariate
probability distributions parameterized by a vector Î± of positive
reals.
</p>

<p>
The infinite-dimensional generalization of the Dirichlet distribution
is the Dirichlet process.
</p>

<p>
The Dirichlet distribution is the conjugate prior distribution of the
categorical distribution (a generic discrete probability distribution
with a given number of possible outcomes) and multinomial distribution
(the distribution over observed counts of each possible category in a
set of categorically distributed observations). This means that if a
data point has either a categorical or multinomial distribution, and
the prior distribution of the distribution's parameter (the vector of
probabilities that generates the data point) is distributed as a
Dirichlet, then the posterior distribution of the parameter is also a
Dirichlet.
</p>
</div>
</div>
<div id="outline-container-orgbd98ac5" class="outline-4">
<h4 id="orgbd98ac5"><span class="section-number-4">7.1.2</span> Exploring a Corpus with the posterior distribution</h4>
<div class="outline-text-4" id="text-7-1-2">
<p>
Quantities needed for exploring a corpus are the posterior
expectations of hidden variables. Each of these quantities are
conditioned on the observed corpus.
</p>

<p>
Visualizing a topic is done by visualizing the posterior topics
through their per-topic probabilities \(\hat{\beta}\).
</p>

<p>
Visualizing a document uses the posterior topic proportions
\(\hat{\theta}_{d,k}\) and the posterior topic assignments
\(\hat{z}_{d,k}\).
</p>

<p>
Finding similar documents can be done through the <i>Hellinger
distance</i>:
</p>

\begin{align*}
  D_{d,k} = \sum_{k=1}^K \left( \sqrt{\hat{\theta}_{d,k}} - \sqrt{\hat{\theta}_{f,k}}\right)^2
\end{align*}
</div>
</div>
<div id="outline-container-org6689543" class="outline-4">
<h4 id="org6689543"><span class="section-number-4">7.1.3</span> Posterior Inference</h4>
<div class="outline-text-4" id="text-7-1-3">
</div>
<div id="outline-container-org1d8964b" class="outline-5">
<h5 id="org1d8964b"><span class="section-number-5">7.1.3.1</span> Mean Field Variational Inference</h5>
<div class="outline-text-5" id="text-7-1-3-1">
<p>
Approximate intractable posterior distribution with a simpler
distribution containing free variational parameters. These parameters
are fit to approximate the true posterior.
</p>

<p>
In contrast to the true posterior, the mean field variational
distribution for LDA is one where the variables are independent of
each other, with and each governed by a different variational
parameter.
</p>

<p>
We fit the variational parameters to minimise the KL-divergence to the
true posterior.
</p>

<p>
The general approach to mean-field variational methods - update each
variational parameter with the parameter given by the expectation of
the true posterior under the variational distribution - is applicable
when the conditional distribution of each variable is the exponential
family.
</p>
</div>
</div>
</div>
<div id="outline-container-orga4dd8b0" class="outline-4">
<h4 id="orga4dd8b0"><span class="section-number-4">7.1.4</span> Shortcomings</h4>
<div class="outline-text-4" id="text-7-1-4">
<ul class="org-ul">
<li>strong, potentially invalid statistical assumptions:
<ul class="org-ul">
<li>topics have no correlation to one another (dirichlet assumes
nearly independent)
<ul class="org-ul">
<li>solution: CTM: use a logistic normal distribution</li>
</ul></li>
<li>assumes order of documents don't matter
<ul class="org-ul">
<li>solution: DTM: use logistic normal distribution to model topics
evolving over time</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org51c57e4" class="outline-3">
<h3 id="org51c57e4"><span class="section-number-3">7.2</span> TopicRNN</h3>
<div class="outline-text-3" id="text-7-2">
<p>
<a href="http://www.columbia.edu/~jwp2128/Papers/DiengWangetal2017.pdf">http://www.columbia.edu/~jwp2128/Papers/DiengWangetal2017.pdf</a> 
</p>

<p>
In TopicRNN, latent topic models are used to capture global semantic
dependencies so that the RNN can focus its modeling capacity on the
local dynamics of the sequences
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Jethro Kuan</p>
<p class="date">Created: 2018-02-09 Fri 11:58</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
