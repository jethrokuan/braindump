<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-02-06 Tue 22:29 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Natural Language Processing</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Jethro Kuan" />
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Natural Language Processing</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org086a52a">1. How to Represent Words</a>
<ul>
<li><a href="#org8d8efa0">1.1. Window-based Co-occurence Matrix</a></li>
<li><a href="#org165b4d0">1.2. Issues with SVD-based methods</a></li>
<li><a href="#org1818b9a">1.3. Iteration-based Methods (e.g. word2vec)</a>
<ul>
<li><a href="#org0962faf">1.3.1. Language Models</a>
<ul>
<li><a href="#orgec57a90">1.3.1.1. Continuous bag-of-words (CBOW)</a></li>
<li><a href="#orga6c1707">1.3.1.2. <span class="todo TODO">TODO</span> Skipgram</a></li>
</ul>
</li>
<li><a href="#orga28af88">1.3.2. Training Methods</a>
<ul>
<li><a href="#orgab08ab6">1.3.2.1. <span class="todo TODO">TODO</span> Negative Sampling</a></li>
<li><a href="#orgaab5bb9">1.3.2.2. <span class="todo TODO">TODO</span> Hierachical Softmax</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org39ce4e3">1.4. Global Vectors for Word Representation (GloVe)</a>
<ul>
<li><a href="#org4b27b0e">1.4.1. Co-occurrence Matrix</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org8a07ef5">2. Word Vector Evaluation</a>
<ul>
<li><a href="#org2bd7f88">2.1. Intrinsic</a></li>
<li><a href="#orgb28118f">2.2. Extrinsic</a></li>
</ul>
</li>
<li><a href="#orgdebaa55">3. Neural Networks</a>
<ul>
<li><a href="#orgad47f0a">3.1. A neuron</a></li>
<li><a href="#org5ea0e89">3.2. A single layer of neurons</a></li>
<li><a href="#org1cdadf0">3.3. Feed-forward computation</a></li>
<li><a href="#orgbf907d3">3.4. Maximum Margin Objective Function</a></li>
<li><a href="#org3b37f75">3.5. Gradient Checks</a>
<ul>
<li><a href="#orgac7db18">3.5.1. </a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org0396d3f">4. Dependency Parsing</a>
<ul>
<li><a href="#org83190a2">4.1. Dependency Grammar and Dependency Structure</a></li>
<li><a href="#orgb04a710">4.2. Dependency Parsing</a></li>
<li><a href="#org67f5cc6">4.3. Transition-based Parsing</a>
<ul>
<li><a href="#orgde8c8cf">4.3.1. Greedy Deterministic Transition-based Parsing</a></li>
<li><a href="#org96c8931">4.3.2. Neural Dependency Parsing</a>
<ul>
<li><a href="#org0229907">4.3.2.1. Feature Selection</a></li>
<li><a href="#org6983928">4.3.2.2. The network</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<p>
-<b>- mode: Org; org-download-image-dir: "./images/nlp/"; -</b>-
</p>

<div id="outline-container-org086a52a" class="outline-2">
<h2 id="org086a52a"><span class="section-number-2">1</span> How to Represent Words</h2>
<div class="outline-text-2" id="text-1">
<p>
<a href="http://ruder.io/word-embeddings-2017/index.html">http://ruder.io/word-embeddings-2017/index.html</a>
</p>

<p>
How do we represent words as input to any of our models? There are an
estimated <b>13 million</b> tokens for the English language, some of them
have similar meanings.
</p>

<p>
The simplest representation is <b>one-hot vector</b>: we can represent
every word as a \(\mathbb{R}^{|V|x1}\) vector with all 0s and one 1 at
the index of that word in the sorted english language.
</p>

<p>
To reduce the size of this space, we can use SVD-based methods. For
instance, accumulating co-occurrence count into a matrix <code>X</code>, and
perform SVD on <code>X</code> to get a \(USV^T\) decomposition. We can then use the
rows of <code>U</code> as the word embeddings for all words in the dictionary.
</p>
</div>
<div id="outline-container-org8d8efa0" class="outline-3">
<h3 id="org8d8efa0"><span class="section-number-3">1.1</span> Window-based Co-occurence Matrix</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Count the number of times each word appears inside a window of a
particular size around the word of interest. We calculate this count
for all words in the corpus.
</p>
</div>
</div>
<div id="outline-container-org165b4d0" class="outline-3">
<h3 id="org165b4d0"><span class="section-number-3">1.2</span> Issues with SVD-based methods</h3>
<div class="outline-text-3" id="text-1-2">
<ol class="org-ol">
<li>Dimensions of the matrix can change very often (new words added to corpus)</li>
<li>Matrix is extremely sparse as most words don't co-occur</li>
<li>Matrix is very high-dimensional</li>
<li>Quadratic cost to perform SVD</li>
</ol>

<p>
Some solutions include:
</p>
<ol class="org-ol">
<li>Ignore function words (blacklist)</li>
<li>Apply a ramp window (distance between words)</li>
<li>Use <a href="https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php">Pearson correlation</a> and set negative counts to 0 instead of raw count.</li>
</ol>
</div>
</div>
<div id="outline-container-org1818b9a" class="outline-3">
<h3 id="org1818b9a"><span class="section-number-3">1.3</span> Iteration-based Methods (e.g. word2vec)</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Design a model whose parameters are the word vectors, and train the
model on a certain objective. Discard the model, and use the vectors
learnt.
Efficient tree structure to compute probabiltiies for all the vocabulary
</p>
</div>
<div id="outline-container-org0962faf" class="outline-4">
<h4 id="org0962faf"><span class="section-number-4">1.3.1</span> Language Models</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
The model will need to assign a probability to a sequence of tokens.
We want a high probability for a sentence that makes sense, and a low
probability for a sentence that doesn't.
</p>

<p>
Unigrams completely ignore this, and a nonsensical sentence might
actually be assigned a high probability. Bigrams are a naive way of
evaluating a whole sentence.
</p>
</div>
<div id="outline-container-orgec57a90" class="outline-5">
<h5 id="orgec57a90"><span class="section-number-5">1.3.1.1</span> Continuous bag-of-words (CBOW)</h5>
<div class="outline-text-5" id="text-1-3-1-1">
<p>
Predicts a center word from the surrounding context in terms of word
vectors.
</p>

<p>
For each word, we want to learn 2 vectors:
</p>
<ol class="org-ol">
<li>v: (input vector) when the word is in the context</li>
<li>u: (output vector) when the word is in the center</li>
</ol>


<ul class="org-ul">
<li>known parameters
<dl class="org-dl">
<dt>input</dt><dd>sentence represented by one-hot vectors</dd>
<dt>output</dt><dd>one hot vector of the known center word</dd>
</dl></li>
</ul>

<p>
Create 2 matrices \(\mathbb{V} \in \mathbb{R}^{n \times |V|}\) and
\(\mathbb{U} \in \mathbb{R}^{|V| \times n}\), where n is the arbitrary
size which defines the size of the embedding space.
</p>

<p>
\(V\) is the input word matrix such that the <i>i</i>-th column of \(V\) is the
$n$-dimensional embedded vector for word \(w_i\) when it is an input to
this model. \(U\) is the output word matrix.
</p>

<ol class="org-ol">
<li>Generate one-hot word vectors for the input context of size m.</li>
<li>Get our embedded word vectors for the context: (\(v_{c-m} =
     Vx^{c-m}\), &#x2026;)</li>
<li>Average these vectors to get \(\hat{v} = \frac{\sum v}{2m} \in \mathbb{R^n}\)</li>
<li>Generate a score vector \(z = U\hat{v} \in \mathbb{R}^{|V|}\) As the
dot product of similar vectors is higher, it will push simila words
close to each other in order to achieve a high score.</li>
<li>Turn the scores into probabilities \(\hat{y} = softmax(z) \in \mathbb{R}^{|V|}\)</li>
</ol>

<p>
Minimise loss function (cross entropy), and use stochastic gradient
descent to update all relevant word vectors.
</p>
</div>
</div>
<div id="outline-container-orga6c1707" class="outline-5">
<h5 id="orga6c1707"><span class="section-number-5">1.3.1.2</span> <span class="todo TODO">TODO</span> Skipgram</h5>
<div class="outline-text-5" id="text-1-3-1-2">
<p>
<a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a>
</p>

<p>
Predicts the distribution of context words from a center word. This is
very similar to the CBOW approach, wind the input and output vectors
reversed. Here, a naive Bayes assumption is invoked: that given the
center word, all output words are completely independent.
</p>

<p>
Input vector: one-hot vectors corresponding to the vocabulary
</p>

<p>
The neural network is consists of one hidden layer of \(n\) units, where
\(n\) is the desired dimension of the word vector. The output layer is a
softmax regression layer, outputting a value between 0 and 1. We want
the value for the context words to be high, and the non-context words
to be low.
</p>

<p>
After training, the output layer is discarded, and the weights at the
hidden layer are the word vectors we want.
</p>
</div>
</div>
</div>

<div id="outline-container-orga28af88" class="outline-4">
<h4 id="orga28af88"><span class="section-number-4">1.3.2</span> Training Methods</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
In practice, negative sampling works better for frequently occurring
words and lower-dimensional vectors, and hierachical softmax works
better for infrequent words.
</p>
</div>
<div id="outline-container-orgab08ab6" class="outline-5">
<h5 id="orgab08ab6"><span class="section-number-5">1.3.2.1</span> <span class="todo TODO">TODO</span> Negative Sampling</h5>
<div class="outline-text-5" id="text-1-3-2-1">
<p>
Take \(k\) negative samples, and minimise the probability of the two
words co-occurring while also maximising the probability of the two
words in the same window co-occur.
</p>
</div>
</div>
<div id="outline-container-orgaab5bb9" class="outline-5">
<h5 id="orgaab5bb9"><span class="section-number-5">1.3.2.2</span> <span class="todo TODO">TODO</span> Hierachical Softmax</h5>
<div class="outline-text-5" id="text-1-3-2-2">
<p>
Hierachical Softmax uses a binary tree to represent all words in the
vocabulary. Each leaf of the tree is a word, and there is a unique
path from root to leaf. The probability of a word \(w\) given a vector
\(w_i\), \(P(w|w_i)\), is equal to the probability of a random walk
starting from the root and ending in the leaf node corresponding to
\(w\).
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org39ce4e3" class="outline-3">
<h3 id="org39ce4e3"><span class="section-number-3">1.4</span> Global Vectors for Word Representation (GloVe)</h3>
<div class="outline-text-3" id="text-1-4">
<p>
Count-based methods of generating word embeddings rely on global
statistical information, and do poorly on tasks such as word analogy,
indicating a sub-optimal vector space structure.
</p>

<p>
word2vec presents a window-based method of generating word-embeddings
by making predictions in context-based windows, demonstrating the
capacity to capture complex linguistic patterns beyond word
similarity.
</p>

<p>
GloVe consists of a weighted least-squares model that combines the
benefits of the word2vec skip-gram model when it comes to word analogy
tasks, but also trains on global word-word co-occurrence counts, and
produces a word vector space with meaningful sub-structure.
</p>

<p>
The appropriate starting point for word-vector learning should be with
ratios of co-occurrence probabilities rather than the probabilities
themselves. Since vector spaces are inherently linear structures, the
most natural way to encode the information present in a ratio in the
word vector space is with vector differences.
</p>

<p>
The training objective of GloVe is to learn word vectors such that
their dot product equals the logarithm of the words’ probability of
co-occurrence. Owing to the fact that the logarithm of a ratio equals
the difference of logarithms, this objective associates (the logarithm
of) ratios of co-occurrence probabilities with vector differences in
the word vector space.
</p>
</div>

<div id="outline-container-org4b27b0e" class="outline-4">
<h4 id="org4b27b0e"><span class="section-number-4">1.4.1</span> Co-occurrence Matrix</h4>
<div class="outline-text-4" id="text-1-4-1">
<p>
Let \(X\) denote the word-word co-occurrence matrix, where \(X_{ij}\)
indicate the number of times word \(j\) occur in the context of word
\(i\). Let \(X_i = \sum_k X_{ik}\) be the number of times any word \(k\)
appears in the context of word \(i\). Let \(P_{ij} = P(w_j|w_i) =
\frac{X_{ij}}{X_i}\) be the probability of j appearing in the context
of word \(i\).
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org8a07ef5" class="outline-2">
<h2 id="org8a07ef5"><span class="section-number-2">2</span> Word Vector Evaluation</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org2bd7f88" class="outline-3">
<h3 id="org2bd7f88"><span class="section-number-3">2.1</span> Intrinsic</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>Evaluation on a specific/intermediate subtask</li>
<li>Fast to compute</li>
<li>Helps understand the system</li>
<li>Not clear if really helpful unless correlation to real task is established</li>
</ul>
</div>
</div>

<div id="outline-container-orgb28118f" class="outline-3">
<h3 id="orgb28118f"><span class="section-number-3">2.2</span> Extrinsic</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>Evaluation on a real task</li>
<li>Can take a long time to compute accuracy</li>
<li>Unclear if the subsystem is the problem or its interaction or other subsystems</li>
<li>If replacing exactly one subsystem with another improves accuracy ➡ Winning!</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgdebaa55" class="outline-2">
<h2 id="orgdebaa55"><span class="section-number-2">3</span> Neural Networks</h2>
<div class="outline-text-2" id="text-3">
<p>
Most data are not linearly separable, hence the need for non-linear
classifiers. Neural networks are a family of classifiers with
non-linear decision boundaries.
</p>
</div>

<div id="outline-container-orgad47f0a" class="outline-3">
<h3 id="orgad47f0a"><span class="section-number-3">3.1</span> A neuron</h3>
<div class="outline-text-3" id="text-3-1">
<p>
A neuron is a generic computational unit that takes \(n\) inputs and
produces a single output. The sigmoid unit takes a $n$-dimensional
vector $x4 and produces a scalar activation (output) \(a\). This neuron
is also associated with an $n$-dimensional weight vector, \(w\), and a
bias scalar, \(b\). for example, the output of this neuron is then:
</p>

\begin{equation*}
a = \frac{1}{1 + exp(-(w^Tx + b))}
\end{equation*}

<p>
This is also frequently formulated as:
</p>

\begin{equation*}
a = \frac{1}{1 + exp(-([w^T\text{ }b] \cdot [x\text{ }1]))}
\end{equation*}
</div>
</div>

<div id="outline-container-org5ea0e89" class="outline-3">
<h3 id="org5ea0e89"><span class="section-number-3">3.2</span> A single layer of neurons</h3>
<div class="outline-text-3" id="text-3-2">
<p>
This idea can be extended to multiple neurons by considering the case
where the input \(x\) is fed as input to multiple such neurons.
</p>

\begin{align*}
  \sigma\left( z \right) =
  \begin{bmatrix}
    \frac{1}{1 + exp(z_1)} \\
    \vdots \\
    \frac{1}{1 + exp(z_m)}
  \end{bmatrix}
\end{align*}

\begin{align*}
  b =
  \begin{bmatrix}
    b_1 \\
    \vdots \\
    b_m
  \end{bmatrix}
  \in \mathbb{R}^m
\end{align*}

\begin{align*}
  W =
  \begin{bmatrix}
    - && w^{(1)T} && - \\
    && \hdots && \\
    - && w^{(m)T} && -
  \end{bmatrix}
  \in \mathbb{R}^{m\times n}
\end{align*}

\begin{equation*}
z = Wx + b
\end{equation*}

<p>
The activations of the sigmoid function can be written as:
</p>

\begin{align*}
  \begin{bmatrix}
    a^{(1)} \\
    \vdots \\
    a^{(m)}
  \end{bmatrix}
  = \sigma\left( Wx + b \right)
\end{align*}

<p>
Activations indicate the presence of some weighted combination of
features. We can use these combinations to perform classification
tasks.
</p>
</div>
</div>

<div id="outline-container-org1cdadf0" class="outline-3">
<h3 id="org1cdadf0"><span class="section-number-3">3.3</span> Feed-forward computation</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Non-linear decisions cannot be classified in by feeding inputs
directly into a softmax function. Instead, we use another matrix \(U
\in \mathbb{R}^{m\times 1}\) to generate an unnormalized score for a
classification task from the activations.
</p>

\begin{equation*}
s = U^Ta = U^Tf\left( Wx + b \right)
\end{equation*}

<p>
where f is the activation function.
</p>
</div>
</div>

<div id="outline-container-orgbf907d3" class="outline-3">
<h3 id="orgbf907d3"><span class="section-number-3">3.4</span> Maximum Margin Objective Function</h3>
<div class="outline-text-3" id="text-3-4">
<p>
Neural networks also need an optimisation objective. the maximum
margin objective ensures that the score computed for "true" labeled
data points is higher than the score computed for "false" labeled data
points, i.e. \(\text{minimize} J = max(s_c - s, 0)\), where \(s_c\) is the
corrupt false window. This optimization can be achieved through
backpropagation and gradient descent.
</p>
</div>
</div>

<div id="outline-container-org3b37f75" class="outline-3">
<h3 id="org3b37f75"><span class="section-number-3">3.5</span> Gradient Checks</h3>
<div class="outline-text-3" id="text-3-5">
<p>
One can numerically approximate these gradients, allowing us to
precisely estimate the derivative with respect to any parameter,
serving as a sanity check on the correctness of our analytic
derivatives.
</p>

\begin{equation*}
  f'(\theta) = \frac{J(\theta^{i+}) - J(\theta^{i-})}{2\epsilon}
\end{equation*}
</div>

<div id="outline-container-orgac7db18" class="outline-4">
<h4 id="orgac7db18"><span class="section-number-4">3.5.1</span> </h4>
</div>
</div>
</div>

<div id="outline-container-org0396d3f" class="outline-2">
<h2 id="org0396d3f"><span class="section-number-2">4</span> Dependency Parsing</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org83190a2" class="outline-3">
<h3 id="org83190a2"><span class="section-number-3">4.1</span> Dependency Grammar and Dependency Structure</h3>
<div class="outline-text-3" id="text-4-1">
<p>
Parse trees in NLP are used to analyse the syntatic structure of
sentences.
</p>

<p>
Consistency Grammar uses phrase structure grammar to organize words
into nested constituents.
</p>

<p>
Dependency structure of sentences shows which words depend on (modify
or are arguments of) other words. These binary assymetric relations
between words are called dependencies and are depicted as arrows
going from the <b>head</b> to the <b>dependent</b>. Usually these dependencies
form a tree structure.
</p>
</div>
</div>
<div id="outline-container-orgb04a710" class="outline-3">
<h3 id="orgb04a710"><span class="section-number-3">4.2</span> Dependency Parsing</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Dependency parsing is the task of analysing the syntactic dependency
structure of a given input sentence S. The output of a dependency
parser is a dependency tree where the words of the input sentence are
connected by typed dependency relations.
</p>

<p>
Formally, the dependency parsing problem asks to create a mapping from
the input sentence with words \(S = w_0w_1\hdots w_n\) (where \(w_0\) is
the <i>ROOT</i>) to its dependency graph \(G\).
</p>

<p>
The two subproblems are:
</p>

<dl class="org-dl">
<dt>Learning</dt><dd>Given a training set \(D\) of sentences annotated with
dependency graphs, induce a parsing model \(M\) that can
be used to parse new sentences.</dd>
<dt>Parsing</dt><dd>Given a parsing model \(M\), and a sentence \(S\), derive the
optimal dependency graph \(D\) for \(S\) according to \(M\).</dd>
</dl>
</div>
</div>
<div id="outline-container-org67f5cc6" class="outline-3">
<h3 id="org67f5cc6"><span class="section-number-3">4.3</span> Transition-based Parsing</h3>
<div class="outline-text-3" id="text-4-3">
<p>
This relies on a state machine which defines the possible transitions
to create the mapping from the input sentence to the dependency tree.
The learning problem involves predicting the next transition in the
state machine. The parsing problem constructs the optimal sequence of
transitions for the input sentence, given the previously induced model.
</p>
</div>
<div id="outline-container-orgde8c8cf" class="outline-4">
<h4 id="orgde8c8cf"><span class="section-number-4">4.3.1</span> Greedy Deterministic Transition-based Parsing</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
The transition system is a state machine, which consists of <i>states</i>
and <i>transitions</i> between those states. The model induces a sequence
of transitions from some initial state to one of several terminal
states. For any sentence \(S = w_0w_1\hdots w_n\) a state can be
described with a triple \(c = \left(\sigma, \beta, A)\):
</p>

<ol class="org-ol">
<li>a stack \(\sigma\) of words \(w_i\) from S,</li>
<li>a buffer \(\beta\) of words \(w_i\) from S,</li>
<li>a set of dependency arcs \(A\) of the form \(\left(w_i, r, w_j\right)\)</li>
</ol>

<p>
For each sentence, there is an initial state, and a terminal state.
</p>

<p>
There are three types of transitions:
</p>

<dl class="org-dl">
<dt>shift</dt><dd>Remove the first word in the buffer, and push it on top of
the stack</dd>
<dt>left-arc</dt><dd>add a dependency arc \((w_j, r, w_i)\) to the arc set A,
where \(w_i\) is the word second to the top of the stack,
and \(w_j\) is the word at the top of the stack. Remove
\(w_i\) from the stack.</dd>
<dt>right-arc</dt><dd>add a dependency arc \((w_i, r, w_j)\) to the arc set A,
where \(w_i\) is the word second to the top of the stack,
and \(w_j\) is the word at the top of the stack. Remove
\(w_i\) from the stack.</dd>
</dl>
</div>
</div>
<div id="outline-container-org96c8931" class="outline-4">
<h4 id="org96c8931"><span class="section-number-4">4.3.2</span> Neural Dependency Parsing</h4>
<div class="outline-text-4" id="text-4-3-2">
</div>
<div id="outline-container-org0229907" class="outline-5">
<h5 id="org0229907"><span class="section-number-5">4.3.2.1</span> Feature Selection</h5>
<div class="outline-text-5" id="text-4-3-2-1">
<dl class="org-dl">
<dt>\(S_{word}\)</dt><dd>vector representation of the words in 4s$</dd>
<dt>\(S_{tag}\)</dt><dd>Part-of-Speech (POS) tags, comprising of a small
discrete set \(P = {NN, NP, \hdots}\)</dd>
<dt>\(S_{label}\)</dt><dd>Arc-labels, comprising of a small discrete set,
describing the dependency relation.</dd>
</dl>

<p>
For each feature type, we will have a corresponding embedding matrix,
mapping from the feature's one-hot encoding, to a d-dimensional dense
vector representation.
</p>

<p>
The full embedding for \(S_{word}\) is \(E^w \in \mathbb{R}^{d\times
N_w}\) where \(N_w\) is the vocabulary size. The POS and label embedding
matrices are \(E^t \in \mathbb{R}^{d\times N_t}\) and \(E^l \in
\mathbb{R}^{d\times N_l}\) where \(N_t\) and \(N_l\) are the number of
distinct POS tags and arc labels respectively.
</p>
</div>
</div>
<div id="outline-container-org6983928" class="outline-5">
<h5 id="org6983928"><span class="section-number-5">4.3.2.2</span> The network</h5>
<div class="outline-text-5" id="text-4-3-2-2">
<p>
The network contains an input layer \([x^w, x^t, x_l]\), a hidden layer,
and a final softmax layer with a cross-entropy loss function.
</p>

<p>
We can define a single weight matrix in the hidden layer, to operate
on a concatenation of \([x^w, x^t, x^l]\), or we can use three weight
matrices \([W^w_1, W^t_1, W^l_1]\), one for each input type. We then
apply a non-linear function and use one more affine (fully-connected)
layer \([W_2]\) so that there are an equivalent number of softmax
probabilities to the number of possible transitions (the output
dimension).
</p>



<div class="figure">
<p><img src="images/nlp/Dependency Parsing/screenshot_2018-01-20_15-36-45.png" alt="screenshot_2018-01-20_15-36-45.png" />
</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Jethro Kuan</p>
<p class="date">Created: 2018-02-06 Tue 22:29</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
