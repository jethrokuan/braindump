<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-15 Wed 12:12 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Information Theory</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Jethro Kuan" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Information Theory</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org79577bf">1. Introduction</a>
<ul>
<li><a href="#org32137d6">1.1. How can we achieve perfect communication over an imperfect noisy communication channel?</a></li>
<li><a href="#orgedda667">1.2. Error-correcting codes for binary symmetric channels</a>
<ul>
<li><a href="#orgdf9ba79">1.2.1. Repetition codes</a></li>
<li><a href="#orgcfe7390">1.2.2. Block codes - the (7,4) Hamming Code</a>
<ul>
<li><a href="#orge9b3980">1.2.2.1. Decoding for linear codes: syndrome decoding</a></li>
</ul>
</li>
<li><a href="#orgadcadfc">1.2.3. What performance can the best codes achieve?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org3b264e8">2. Shannon Information Content</a></li>
<li><a href="#org421a20d">3. Source Coding Theorem</a></li>
</ul>
</div>
</div>
<div id="outline-container-org79577bf" class="outline-2">
<h2 id="org79577bf"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org32137d6" class="outline-3">
<h3 id="org32137d6"><span class="section-number-3">1.1</span> How can we achieve perfect communication over an imperfect noisy communication channel?</h3>
<div class="outline-text-3" id="text-1-1">
<p>
The physical solution is to improve the characteristics of the
communication channel to reduce its error probability. For example, we
can use more reliable components in the communication device's
circuitry.
</p>

<p>
Information theory and coding theory offer an alternative: we accept
the given noisy channel as it is, and add communication systems to it
to detect and correct errors introduced by the channel.
</p>

<p>
An encoder encodes the source message <code>s</code> into a transmitted message
<code>t</code>, adding <i>redundancy</i> to the original message in some way. The
channel adds noise to the transmitted message, yielding a received
message <code>r</code>. The decoder uses the known redundancy introduced by the
encoding system to infer both the original signal <code>s</code> and the added
noise.
</p>

<p>
Information theory is concerned of the theoretical limitations and
potentials of these systems. Coding theory is concerned with the
creation of practical encoding and decoding systems.
</p>
</div>
</div>
<div id="outline-container-orgedda667" class="outline-3">
<h3 id="orgedda667"><span class="section-number-3">1.2</span> Error-correcting codes for binary symmetric channels</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="outline-container-orgdf9ba79" class="outline-4">
<h4 id="orgdf9ba79"><span class="section-number-4">1.2.1</span> Repetition codes</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
Key idea: repeat every bit of the message a prearranged number of
times, and pick the bit with the majority vote.
</p>

<p>
We can describe the channel as adding a sparse noise vector n to the
transmitted vector, adding in a modulo 2 arithmetic.
</p>

<p>
One can show that this algorithm is optimal by considering the maximum
likelihood function of <code>s</code>.
</p>

<p>
The repetition code \(R_3\) (repeat 3 times) has reduced the probability
of error, but has also reduced the <i>rate</i> of information by a factor
of 3.
</p>
</div>
</div>
<div id="outline-container-orgcfe7390" class="outline-4">
<h4 id="orgcfe7390"><span class="section-number-4">1.2.2</span> Block codes - the (7,4) Hamming Code</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
Key idea: add redundancy to blocks of data instead of encoding one bit
at a time.
</p>

<p>
A block code is a rule for converting a sequence of source bits <code>s</code>,
of length <code>K</code>, into  transmitted sequence <code>t</code> of length <code>N &gt; K</code> bits.
The Hamming code transmits <code>N=7</code> bits for every <code>K=4</code> bits.
</p>

<p>
Because the Hamming code is a linear code, it can be written compactly
as a matrix:
</p>

\begin{equation*}
  \text{transmitted} = G^T \text{source}
\end{equation*}

<p>
where \(G\) is the generator matrix of the code.
</p>
</div>
<div id="outline-container-orge9b3980" class="outline-5">
<h5 id="orge9b3980"><span class="section-number-5">1.2.2.1</span> Decoding for linear codes: syndrome decoding</h5>
<div class="outline-text-5" id="text-1-2-2-1">
<p>
The decoding problem for linear codes can also be described in terms
of matrices. We evaluate 3 parity-check bits for the received bits
\(r_1r_2r_3r_4\), and see whether they match the three received bits
\(r_5r_6r_7\). The differences (mod 2) between these 2 triplets are
called the <i>syndrome</i> of the received vector. If the syndrome is 0,
then the received vector is a code word, and the most probable
decoding is given by reading its first four bits.
</p>

\begin{align*}
  G^T &=
  \begin{bmatrix}
    I_4 \\
    P
  \end{bmatrix}, \\
  H &=
      \begin{bmatrix}
        -P & I_3
      \end{bmatrix}
             =
      \begin{bmatrix}
        P & I_3
      \end{bmatrix}
            =
      \begin{bmatrix}
    1 & 1 & 1 & 0 & 1 & 0 & 0 \\
    0 & 1 & 1 & 1 & 0 & 1 & 0 \\
    1 & 0 & 1 & 1 & 0 & 0 & 1
    \end{bmatrix}
\end{align*}
</div>
</div>
</div>
<div id="outline-container-orgadcadfc" class="outline-4">
<h4 id="orgadcadfc"><span class="section-number-4">1.2.3</span> What performance can the best codes achieve?</h4>
<div class="outline-text-4" id="text-1-2-3">
<p>
We consider the <code>(R, p_b)</code> plane, where \(R\) is the rate,
and \(p_b\) is the decoded bit-error probability, Claude Shannon proved that the boundary between achievable and
non-achievable points meets the \(R\) axis at a non-zero value \(R = C\).
For any channel, there exist codes that make it possible to
communicate with arbitrarily small probability of error $p<sub>b</sub>= at
non-zero rates. This theorem is called the <i>noisy-channel coding
theorem</i>.
</p>

<p>
The maximum rate at which communication is possible with arbitrarily
small \(p_b\) is called the capacity of the channel.
</p>

\begin{equation*}
  C(f) = 1 - H_2(f) = 1 - \left[f\log_2\frac{1}{f} + (1-f)\log_2\frac{1}{1-f}\right]
\end{equation*}
<p>
n
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org3b264e8" class="outline-2">
<h2 id="org3b264e8"><span class="section-number-2">2</span> Shannon Information Content</h2>
<div class="outline-text-2" id="text-2">
<p>
A set of size S can be communicated with \(\ceil{\log S}\) bits.
</p>
</div>
</div>



<div id="outline-container-org421a20d" class="outline-2">
<h2 id="org421a20d"><span class="section-number-2">3</span> Source Coding Theorem</h2>
<div class="outline-text-2" id="text-3">
<p>
We can compress N outcomes from a source \(X\) into roughly \(NH(X)\)
bits.
</p>

<p>
This is provable by counting the typical set.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Jethro Kuan</p>
<p class="date">Created: 2018-08-15 Wed 12:12</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
